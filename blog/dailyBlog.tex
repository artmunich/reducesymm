% siminos/blog/dailyBlog.tex
% $Author$ $Date$

\chapter{Daily blog}
\label{c-DailyBlog}

\begin{description}

\item[2007-06-13 Predrag: Learned nothing from Armbruster {\em et. al}]
Armbruster {\em et. al} showed that four complex Fourier
modes suffice to exhibit most
of the qualitative features of the dynamics,
for a wide range of system sizes\rf{AGHks89}.

\medskip\noindent{\bf 2009-08-28 Predrag} Vaggelis in his thesis and
\HREF{http://ChaosBook.org/projects}
     {Kohler in his ChaosBook.org project}
got nothing of interest out of Armbruster \etal\rf{AGHks89}.

\item[2007-11-28 Predrag: Japanese heresy]
We do not want to refer to wrong papers, but here it is, for
the internal record, so we do not forget not to cite it
(from Physical Review Letters, 16 Sep 2004 request to referee,
which I ignored):

Mitsuhiro Kawasaki and Shin-ichi Sasa,
    ``Statistics of unstable periodic orbits of a chaotic dynamical system
    with a large number of degrees of freedom."

\item[2007-11-28 Predrag: Raves]
Amazon.com reviewers rave about \refref{ITO96};
looks nice, get it.
Check out
 \refrefs{ChePiWa02,Jaco05,Kett95,Hall67}.

A free book (but elementary, not useful):
http://www.sst.ph.ic.ac.uk/people/d.vvedensky/courses.html

\end{description}


\section{2007-05-17 $s_1$, $s_2$, $\tau_x$, $\tau_z$ generate 16 irreps?}

\medskip\noindent{\bf Predrag}
 My main problem is - we are currently using only 4 irreducible reps
of $C_2 \times C_2$ = $D_2$ dihedral group generated by $\tau_x, \tau_z$,
but why not 16 irreps of
the $D_2 \times D_2$ generated by $s_1, s_2, \tau_x, \tau_z$?
They all commute, each one splits the space of reps into 2.
Why stop at $U_S$ subspace?
There should be 16 discrete copies of any
general solution, not just 4.
However, there would still be 4 copies of UB, as UB is within the
fully symmetric irrep $A_1$ od $s_1, s_2$ $D_4$.

\section{2008-05-24 ES: Zeghlache Mandel center manifold}

This is just an attempt to understand the primary bifurcation in
Zeghlache and Mandel\rf{ZeMa85} 5-dimensional flow:
\bea
\dot{x}_1 &=&  \sigma (- x_1 -  \delta x_2 + y_1)
               \continue
\dot{x}_2 &=&  \sigma (\delta x_1   - x_2 + y_2)
               \continue
\dot{y}_1 &=& \rho\,  x_1 - y_1 + \delta y_2 - x_1 z
                                                \label{ZMeqs} \\
\dot{y}_2 &=& \rho\,  x_2 - \delta y_1 - y_2 - x_2 z
               \continue
\dot{z}   &=& x_1 y_1 + x_2 y_2  - \gamma\, z
            \,.\nnu
\eea
Here $\sigma>1,\, \rho>0$.

The equation is equivariant under the action of $\Gamma=SO(2)$ defined by
 \beq
 {D}(\theta)
%  =   \left(\barr{ccc}
%     {\bf R}(\theta) &  0              & 0  \\
%     0               & {\bf R}(\theta) & 0  \\
%     0               &  0              & 1
%     \earr\right)
=   \left(\barr{ccccc}
   \cos\theta  &  \sin\theta & 0  &  0 & 0  \\
  -\sin\theta  &  \cos\theta & 0  &  0 & 0 \\
   0  &  0 & \cos\theta  &  \sin\theta & 0  \\
   0  &  0 &-\sin\theta  &  \cos\theta & 0 \\
   0  &  0 & 0  &  0 & 1
   \earr\right)
 \,.
 \label{ZMrotation}
 \eeq

The \stabmat\ is
  \beq
{\Mvar_{ZM}} =
  \left(\barr{ccccc}
    -\sigma    & -\epsilon\sigma & \sigma &  0       &  0 \\
\sigma\epsilon & -\sigma         & 0      & \sigma   &  0 \\
\rho-z         &     0           & -1     & \epsilon & -x_1 \\
0              & \rho-z       & -\epsilon & -1       & -x_2 \\
y_1            & y_2             & x_1    & x_2      & -\gamma
    \earr\right)
\,.
  \ee{ZMstabMat}

The \stabmat\ commutes with ${D}(\theta)$ for points on the
fixed point subspace of $\Gamma$, \ie, the $z$-axis.

The origin is a fixed point of the flow and as it is rotationally invariant (lies on the $z$-axis)
its \stabmat\ has no zero eigenvalue associated with the action of $\Gamma$. For $r<1+\delta^2$
the origin is a stable fixed point, while at $r=1+\delta^2$ the stability
matrix has eigenvalues $ \left(0,0,-\gamma ,-(\sigma+1) \pm i \delta  (\sigma-1) \right)$
and  thus a bifurcation occurs which in the literature\rf{GL-Gil07b} is classified as pitchfork to a
stable circle ($\Gamma$-orbit) of equilibria, while the origin looses stability. This is a surprising
fact because generically \rf{golubII} one would expect an equivariant Hopf bifurcation to a $\Gamma$-invariant periodic
orbit, \ie, a relative equilibrium (traveling wave). Of course, in view of the degenerate zero eigenvalue
of the \stabmat\ at bifurcation we already begin to question the posibility of a Hopf bifurcation.
Nevertheless one should proceed by first reducing the bifurcation problem to one where all of the \stabmat\ eigenvalues
have zero real part, \ie, apply either Lyapunov-Schmidt reduction\rf{golubI} or the Center Manifold Theorem\rf{guckb}.
We follow the latter approach. The procedure is standard but for a five dimensional system I had to use computer algerba
to hope to do it correctly\ES{I only outline the procedure for now, I'll give a better description
and explicit form of transformation matrices, etc, later on if needed.}.

We begin by a linear transformation to new variables $w_i,\, i=1\ldots 5$ such that at bifurcation
the \stabmat\ at the origin is in block-diagonal form. Thus we use the transformation
\beq
	w = T^{-1} x
\eeq
where $w$ and $x$ is shorthand notation for the new and old variables respectively and $T$ is the column matrix of
eigenvectors of \stabmat\ evaluated at the origin, for $\rho=1+\delta^2$. We seek an approximation to the center manifold
as a graph over the center manifold: $w_i = h_i(w_1,w_2,\mu),\, i=3\ldots5$, where $\mu=\rho-1-\delta^2$ is regarded
as the bifurcation parameter but also as an extra variable satisfying $\dot{\mu}=0$. Substituting a Taylor expansion for $h$
up to third order in $w_1,w_2,\mu$ in the transformed equation\ES{actually to a PDE for h which I haven't introduced.} we obtain a local approximation of the center manifold
and we can write the dynamics for $w_1,w_2$ as:
% \begin{eqnarray}
%  \dot{w}_1 & = & \frac{ \sigma}{B^2+F^2}\left[ F\left(\mu +\frac{\left(3 B^2-F^2\right) \mu ^2 \sigma }{\left(B^2+F^2\right)^2}-\frac{w_1^2+w_2^2}{\gamma
%  \left(1+\delta ^2\right)}\right) w_1 + B\left(\mu +\frac{\left(B^2-3 F^2\right) \mu ^2 \sigma }{\left(B^2+F^2\right)^2}-\frac{w_1^2+w_2^2}{\gamma  \left(1+\delta
% ^2\right)}\right) w_2 \right]  \\
%  \dot{w}_2 & = & \frac{ \sigma}{B^2+F^2}\left[ -B\left(\mu +\frac{\left(B^2-3 F^2\right) \mu ^2 \sigma }{\left(B^2+F^2\right)^2}-\frac{w_1^2+w_2^2}{\gamma
%  \left(1+\delta ^2\right)}\right) w_1 + F\left(\mu +\frac{\left(3 B^2- F^2\right) \mu ^2 \sigma }{\left(B^2+F^2\right)^2}-\frac{w_1^2+w_2^2}{\gamma  \left(1+\delta
% ^2\right)}\right) w_2 \right]
% \end{eqnarray}

\begin{eqnarray}
 \left(\begin{array}{c} \dot{w}_1 \\ \dot{w}_2  \end{array}\right) & = & \left[\lambda + g_r(w_1^2+w_2^2) \right]\left(\begin{array}{c} w_1 \\ w_2  \end{array}\right) + \left[\omega + g_\theta(w_1^2+w_2^2) \right] \left(\begin{array}{c} -w_2 \\ w_1 \end{array}\right)
\end{eqnarray}
where
\[\begin{array}{cc}
	\lambda = \frac{ \sigma F}{B^2+F^2} \left(\mu +\frac{\left(3 B^2-F^2\right) \mu ^2 \sigma }{\left(B^2+F^2\right)^2}\right)\,, &
		\omega = -\frac{ \sigma B}{B^2+F^2}\left(\mu +\frac{\left(B^2-3 F^2\right) \mu ^2 \sigma }{\left(B^2+F^2\right)^2}\right) \\
	g_r= -g_\theta= -\frac{ \sigma F}{B^2+F^2} \frac{w_1^2+w_2^2}{\gamma\left(1+\delta ^2\right)}\,,  & B = \delta(\sigma-1)\,,\ F=\sigma+1\,.  \\
\end{array}
\]
In this form it is clear that the reduced system is
$SO(2)$-equivariant and that the eigenvalues of the \stabmat\
vanish at the bifurcation. Thus we can't have a Hopf
bifurcation. On the other hand, for $\mu>0$ and sufficiently
small there is no equilibrium other than the origin, while
there is a $SO(2)$-invariant periodic orbit, \ie, a relative
equilibrium. This is most readily seen if we transform the
system in polar coordinates:
\bea
	\dot{r} &=&\left(\lambda+ g_r(r^2)\right)r \continue
	\dot{\theta} &=& \omega+ g_\theta(r^2)\,.
\eea
This form justifies the use of $g_r,g_\theta$ above. One can see that we cannot have $\dot{r}=\dot{\theta}=0$ for $\mu>0$ and
thus there are no equilibria to the right of the bifurcation point (other than the origin) . On the other hand there exists a Hopf(?) cycle with
\beq
	r^2= \gamma  \left(1+\delta ^2\right) \mu  \left(1+\frac{\left(3 B^2-F^2\right) \mu  \sigma }{\left(B^2+F^2\right)^2}\right)\,,\ \dot{\theta}=\frac{2 B \sigma^2 \mu^2}{\left(B^2+F^2\right)^2}\,,
\eeq
which is a geometrical circle and thus is $SO(2)$-invariant, \ie, it is a relative equilibrium.

This is in direct contradiction with my proof of no existence of relative equilibria in the original system, so something
is wrong. Possibilities are: The application of center manifold theorem was not performed correctly (perharps consider $\delta$
as bifurcation parameter?) One has to go one step further and study the unfolding of the bifurcation (is it codimension-4 bifurcation
or am I counting totally wrong?) I meshed up in proving there aren't relative equilibria in ZM system (did somebody else check at least the polar coordinate representation of the system?) The relative equilibrium in the center manifold is not relative equilibrium in
the original system (sounds crazy but I'll check it.)

\section{2008-04-22 ES: Locating Heteroclinic Connections}

I recently tried to locate heteroclinic connections in Lorenz equations. One
can easily see that the 2-dimensional unstable manifold of $\EQV{1}$ intersects the
$2-$dimensional stable manifold of $\EQV{0}$ and thus there should be such connections.
As I couldn't find Predrag's secret method documented somewhere I followed a simple
shooting approach.

Although a heteroclinic orbit is an infinite-time orbit it is sufficient to
pin down a finite time segment of the orbit originating at the linear unstable subspace $E_u^{(1)}$
of $\EQV{1}$ and ending at the linear stable subspace $E_s^{(0)}$ of $\EQV{0}$. Those
requirements can be expressed as the boundary value problem:
\bea
	x(0) & = & \EQV{1}+ \epsilon Re(\mathbf{e}_1^{(1)}) \, \label{eq:shootHetIC} \\
	P_1^{(0)} (x(T)-\EQV{0}) & = &  0 \,. \label{eq:shootHetBC}
%%	P_2^{(0)} (x(T)-\EQV{0}) & = &  d_2 \,. \label{eq:shootHetFixT} %% Not needed for Lorenz
\eea

Eq. \refeq{eq:shootHetIC} imposes the requirement that we start on the unstable subspace of $\EQV{1}$.
Alternatively we could have used as a search space a circle of radius $\epsilon$ on the plane defined
by orthonormalizing $\left(Re(\mathbf{e}_1^{(1)}),\Im(\mathbf{e}_1^{(1)})\right)$, parametrized by some
angle $\theta$. Eq. \refeq{eq:shootHetBC} imposes the condition that the final point on the trajectory is
on the stable manifold of $\EQV{0}$. Here
\beq
	P_j^{(0)}= \prod_{i\neq j}^d \frac{\mathbf{A}(\EQV{0})-\lambda_i^{(0)} \mathbf{1}}{\lambda_j^{(0)}-\lambda_i^{(0)}}\,,
\eeq
is projection operator on the $j$'th eigendirection of the linear stability matrix $\mathbf{A}$ at $\EQV{0}$.
%Condition \refeq{eq:shootHetFixT} needs to be imposed due to the time-translational invariance of the
%equations. It corresponds to restricting the final point on a \Poincare section $\PoincS$ transverse to
%the least contracting eigendirection of $\mathbf{A}(\EQV{0})$. For Lorenz this would be a plane normal
%to the $z$-axis at distance $d_2$ from $\EQV{0}$.
\ES{According to Predrag's notation the left hand side
of equation \refeq{eq:shootHetBC} %%and \refeq{eq:shootHetFixT}
is a scalar.}

To solve the boundary value problem I have used Newton's method to refine a guess for $\epsilon_n$. %and $T_n$
%based on the linearization:
%\beq
%	f^{T_{n+1}}(x_{n+1}) \simeq f^{T_n}(x_n) + \mathbf{J}^{T_n}(x_n) Re(\mathbf{e}_1^{(1)})\delta\epsilon\,, %+ v(f^{T_n}(x_n)) \delta T
%\eeq
%and imposing condition \refeq{eq:shootHetBC} % and \refeq{eq:shootHetFixT}
%on $f^{T_{n+1}}(x_{n+1})$.
Predrag suggests using the linear approximation of the flow to analytically continue the heteroclinic
orbits after period $T$ and impose the condition that this analytic solution ends up on the equilibrium.
I cannot see why this is necessary here. The condition \refeq{eq:shootHetBC} guarantees that the solution
is on the linear approximation of the stable manifold of $\EQV{0}$ and thus will end up on $\EQV{0}$.
Essentially this is the analytic part of the problem and no more needs to be done. The accuracy of the
method is limited by the approximation of the local stable manifold of $\EQV{0}$ and the local unstable
manifold of $\EQV{1}$ by the corresponding linear unstable subspaces, \ie, by planes in the case of Lorenz
equations. One can estimate the distance of minimum approach to $\EQV{0}$ by using the expressions for
the linearized flow. Disregarding the strongly contracting eigendirection $e_3^{(0)}$ I find:
\beq
	r_{min}^2= d_2^2\left(1-\frac{\lambda_2}{\lambda_1}\right)\left(\-\frac{d_1^2 \lambda_1}{d_2^2\lambda_2}\right)^{-\frac{\lambda_2}{\lambda_1-\lambda_2}}
\eeq
where $d_1=P_1^{(0)} (x(T)-\EQV{0})$ and $d_2=P_2^{(0)} (x(T)-\EQV{0})$ \ES{Predrag might want to compare
this against his secret notes.}. I use this to compare with the actual minimum distance from $\EQV{0}$ and
evaluate the validity of the linear approximation of the stable manifold.

In \refref{FriedmanDoedelConnections91} the authors present a
method for computation and continuation of heteroclinic
connections similar in spirit but in a more general setting.
They suggest that a condition breaking the time-translational
invariance should be used along with \refeq{eq:shootHetBC}.
Here, \refeq{eq:shootHetIC} is formulated in a way that
excludes variation in the initial conditions in the direction
of the flow and we need not impose an extra condition.

It works well for Lorenz equations but fails to converge for ZM.



\section{2008-01-17 How to quotient the SO(2) symmetry}
% \section{Symmetry-Reduced Representation (SRR) for KSE}

\medskip\noindent{\bf Ruslan:}
In order to quotient the SO(2) symmetry we need to be able to define,
for any state of the KS system $u(x)$
($a = (a_1, a_2, \ldots)^\mathsf{T}$ in Fourier space),
a 'shift' parameter, $s(a) \in S^1 $, such that
\[ \tau_{-s(a)} a \in M/\mathrm{SO}(2). \]
This parameter must satisfy the following {\em monotonicity condition} with
respect to the translation of the state $a$:
\[ s(\tau_{\ell/L}a) = s(a) + \phi(\ell/L) \]
where $\phi(x): S^1 \mapsto S^1$ should be a continuous strictly monotonic function.
This condition is necessary to avoid any ambiguity in the definition of
the shift parameter.

This condition is clearly satisfied when $s(a)$ is proportional to
the first Fourier mode (provided that $|a_1| > 0$)
\begin{equation}
  s(a) =  \theta_1/(2\pi) = \arg(\hat{e}_1^\dagger\,a)/(2\pi)
\label{eq:shift1} \end{equation}
where $\hat{e}_1 = (1+0i, 0, 0, \ldots)^\mathsf{T}$ is the basis vector corresponding
to the first Fourier mode and $\dagger$ denotes Hermitian transpose.
In this case
\[ \arg (e_1^\dagger\,\tau_{\ell/L}a)/(2\pi) = \theta_1/(2\pi) + \ell/L\,, \]
and so $\phi(x) = x$.

It is also possible to get a well-defined shift parameter by
using the difference between phases of Fourier modes $k$ and $k+1$ (provided
that $|a_k|, |a_{k+1}| > 0$):
\begin{equation}
  s(a) = \arg(\hat{e}_{k+1}^\dagger\,a)/(2\pi) -
  \arg(\hat{e}_{k}^\dagger\,a)/(2\pi)\,.
  \label{eq:shiftk} \end{equation}
Maybe for $L = 22$, where dominant Fourier modes are 2 and 3, it is better to use
this shift parameter with $k = 2$?  This needs to be explored.

Of course, as suggested by Predrag, we can define in a similar
fashion the shift parameter with respect to any other
state (e.g. an equilibrium state $a_q$):
\[ s(a) = \arg(a_q^\dagger\, a)/(2\pi)\,, \]
but this shift cannot be guaranteed to satisfy the
monotonicity condition for all $a$.

Since equilibria E1, E2, and E3 for $L = 22$ have dominant
1st, 2nd, and 3rd Fourier modes, respectively, fixing the modes by
Eqs.~(\ref{eq:shift1}) or (\ref{eq:shiftk}) also fixes the equilibria.


\medskip\noindent{\bf 2008-01-17 Monotonicity?}
\medskip\noindent{\bf Predrag:}
Not sure about need for monotonicity - one needs it for the 1-dimensional
Lie group of time evolution, parameterized by a continuous parameter $t$
which can be conjugated to any other parameter $u = u(t,\ssp)$ as long
as it is monotone, but rotations can go whichever way they want, modulo
$2\pi$ (or $L$). Once we look at a problem with $SO(3)$ symmetry, what's the
need for monotonicity in Euler angles, \etc.?

\medskip\noindent{\bf 2008-01-17 Ruslan: I think we need monotonicity.}
Let us say $s(a)$ is defined in such a way that it is not monotonic with respect to
shifts of state $a$.  Then there could exist $\ell_1$ and $\ell_2 \neq \ell_1$ such that
\[ s(\tau_{\ell_1/L}\,a) = s(\tau_{\ell_2/L}\,a) = s\,. \]
Then $\tau_{\ell_1/L - s}\,a$ and $\tau_{\ell_2/L - s}\,a$ would be two distinct points
in $M$/SO(2) representing the same state $a$.  This doesn't seem right.

\section{2009-05-21 Talked to Rowley}

\noindent{\bf Vaggelis:}
I am working on quotienting KS in a some sense. I am in
Snowbird so I got the chance to talk to a few people about
it. I've talked to Gilmore who initially said he cannot help
us with symmetry reduction in thousands of dimensions
(apparently you've talked to him last fall about it) but came
to my talk and got interested I think, asked for the paper
and/or thesis when available. The talk went well I believe.

I've also talked to Rowley (as in Rowley and Marsden). What
they do for "transverse" integration is somewhat different to
what we do for CLe. Their transverse integration depends on
the reconstruction equation (while for us it is totally
independent) so that if the reconstruction equation fails
(there is a denominator that can vanish in it) the whole
procedure, not just reconstruction, has to be restarted with
different "template". This is what makes their method local.
We don't have such problems I think.

\begin{description}
\item[Predrag]
Rowley is right, see \refsect{sect:epyc2Fourier}.

\item[Vaggelis]
Also talked a little bit with Kevin Mitchel about the
geometrical origin of singularities. He is not against the
modified invariants and I think he is right about the
impossibility of global reduction with purely geometrical
means.
\end{description}

\section{2009-08-03 Method of slices for \cLf}
\renewcommand{\ssp}{x}

\begin{description}
\item[2009-08-03 Predrag]
here is some summer work that I had originally inserted into
\\
dasbuch/book/chapter/continuous.tex in fit of over enthusiasm
- because Vaggelis praised it: ``It works like charm.''
Unfortunately, it did not - the slice described here suffers
from the usual discontinuities.


\item[\Slice\ for \cLf.]
%from \Chapter{continuous}{26sep2009}{Relativity for cyclists}
Here we can use the fact that
\[ \groupTan(\sspRed) \cdot \sliceTan{}
 = \sspRed \cdot \Lg^T \,\Lg \, \slicep
 =
    \bar{x}_1 x_1'
   +\bar{x}_2 x_2'
   +\bar{y}_1 y_1'
   +\bar{y}_2 y_2'
\]
is the dot-product restricted to the 4-dimensional
representation of $\SOn{2}$.
A generic  $ \slicep $ can be brought to form $ \slicep  =
(0,1,y_1',y_2',z)$ by a rotation and rescaling. Then $\Lg
\cdot \slicep   = (1,0,y_2',-y_1',0)$, and
%                                                        \exerbox{exer:csectionReduced}
\beq
\frac{\vel \cdot \sliceTan{}}{\groupTan(\sspRed) \cdot \sliceTan{}}
= -
\frac{\vel_1 + \vel_3 y'_2 -\vel_4 y'_1}
     {\bar{x}_2 + \bar{y}_1 y'_1 + \bar{y}_2 y'_2}
%\frac{\vel_1 x'_2 -\vel_2 x'_1 + \vel_3 y'_2 -\vel_4 y'_1}
%     {\vel_1 x'_1 + \vel_2 x'_2 + \vel_3 y'_1 + \vel_4 y'_2}
\,.
\label{PCsectSin}
\eeq
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% computed by wilczak/matmematica/PCsection.nb
\SFIG{PCunrot3}
{}{
Method of moving frames, \slice\ fixed by a point on the \cLe\
\reqv\ group orbit, $\slicep  = \ssp_{\REQV{}{}1}$. The strange
attractor of \cLf\
%\reffig{fig:CLEx1x2z}
in the \reducedsp, %\ of \refeq{EqMotionMovFramePC}
, $\{x_1,x_2,z\}$.
}
{fig:PCunrot3}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
A long time trajectory of \cLf\ %\refeq{EqMotionMovFramePC} with
$\slicep$ on the \reqv\ \REQV{}{1} group orbit is shown in
\reffig{fig:PCunrot3}.
As initial condition
we chose an initial point % \refeq{eq:REQB1velocVal}
on the unstable manifold
of \REQV{}{1}, rotated back to the \slice\ by angle $\gSpace$.
% as prescribed by \refeq{PCsectQ1}.
In \reffig{fig:PCunrot3} we
show the part of the trajectory for $t\in\left[70,100\right]$.
The \reqv\ \REQV{}{1}, now an equilibrium of the
\reducedsp\ dynamics, organizes the flow into a R\"ossler type
attractor.
% (see \reffig{RosslerAtractor}).
There appears to be no singularity in this
projection although we can run into trouble % with \refeq{EqMotionMovFramePC}
wherever the denominator in $\dot{\theta}$ % \refeq{MFdtheta}
vanishes, \ie, the direction of group
action on the point $\ssp$ is perpendicular to the direction
of group action on $\slicep$.
%                                                        \exerbox{exer:PCsectionCLe}
    \PC{
Apparent lack of singularities in \reffig{fig:PCunrot3} appears
fortuitous.
Diverging velocities subspace is a 4\dmn\ subspace, and indeed our simulations encounter
this subspace very quickly, with \reducedsp\ velocity going off to infinity.
    }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% computed by wilczak/matmematica/PCsection.nb
\SFIG{PCunrot2}
{}{
Method of moving frames, \slice\ as in \reffig{fig:PCunrot3},
but $\{x_2,y_2,z\}$ projection. The \cLf\ strange attractor
% of \reffig{fig:CLEx1x2z}
now exhibits a discontinuity due to
vanishing denominator in \refeq{PCsectSin}.
}
{fig:PCunrot2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Indeed, the method does encounter singularities in
subsets of \statesp, with phase velocity $\dot{\gSpace}$
divergent whenever the denominator in \refeq{PCsectSin}
changes sign, see \reffig{fig:PCunrot2}.
Hence a single \slice\ does
not in general suffice to cover $\pS/\Group$ globally.

%From dasbuch/book/chapter/continuous.tex \section*{Flotsam}
%
The moving frames method allows the determination of (non-polynomial)
invariants of the group action by a simple and efficient
algorithm that works well in high-dimensional \statesp s.
    \PC{Vaggelis, add references here? {\bf ES}: mmm... SiminosThesis?}
    \PC{Vaggelis, why ``(non-polynomial)'' invariants?
        length$^2$ is polynomial {\bf ES}: It is the only one though.
        {\bf PC}: is this an answer?}

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CLEpcSect.png computed by  CLEfinal.nb (repo: vaggelis)
% CLEpcSect2.png computed by CLEfinal.nb (repo: vaggelis)
% Predrag's program: wilczak/matmematica/PCsection.nb
\begin{figure}[ht]
\begin{center}
(a) \includegraphics[width=0.40\textwidth]{CLEpcSect}
(b) \includegraphics[width=0.43\textwidth]{CLEpcSect2}
\end{center}
\caption{
Method of moving frames, \slice\ fixed by a point on the \cLe\
\reqv\ group orbit, $\slicep  = \ssp_{\REQV{}{}1}$. The strange
attractor of \cLf\ %\reffig{fig:CLEx1x2z}
in the \reducedsp:
% of \refeq{EqMotionMovFramePC}:
(a) $\{x_1,x_2,z\}$ projection,
(b) $\{x_1,y_1,z\}$ projection.
Color-coding indicates $(\hat{\ssp} \cdot \hat{\slicep })_4$
where $\hat{.}$ stands for unit vector, with green indicating values
of the inner product close to $1$ and brown indicating values
close to $0$.
% \authorES
    }
\label{fig:CLEpcSect}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
    \ES{I will add a scale in \reffig{fig:CLEpcSect} but it is too
   late here to do it tonight.}
    \PC{will need quality CLEpcSect.eps, CLEpcSect2.eps}
    \PC{ in \reffig{fig:CLEpcSect}:\\
        * Mark $\ssp_{\REQV{}{}1}$ \\
        * Draw stable eigenvector of $\ssp_{\REQV{}{}1}$\\
        * State value of $\ssp_{\REQV{}{}1}$ somewhere
        }
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% computed by PCunrot.nb
\SFIG{PCunrot1}
{}{
Method of moving frames, continuous time version, for the
polar coordinates motivated $x'=(0,1,0,0,z)$,
$x_1=0,\;x_2>0$, \slice. The \cLf\ strange attractor of \cLf\
% \reffig{fig:CLEx1x2z}
exhibits a discontinuity at
$x_2=0$ in the \reducedsp:
$\{x_2,y_2,z\}$ projection.
}
{fig:PCunrot1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Indeed, the method does encounter singularities in
subsets of \statesp.
For example, the \reducedsp\ equations \refeq{PCsectSin}
for the polar coordinates inspired \slice\
$x'=(0,1,0,0,z)$, $x_1=0,\;x_2>0$,
%this is illustrated by \reffig{fig:PCunrot}.
%$(\rho_1,\gSpace_1)$ are polar coordinates, $\rho_1 =
%\sqrt{\ssp_1^{ 2} + \ssp_2^{2}}$, see \refeq{eq:CartToPol},
are given by
%                                                    \exerbox{exer:csectionCLe}
\beq
\dot{\ssp} = \vel - \frac{\vel_1}{\sspRed_2} \, \sliceTan{}
\,,
\ee{EqMotionMovFrame1}
with phase velocity $\dot{\gSpace}$ divergent whenever $\sspRed_2$
changes sign, see \reffig{fig:PCunrot1}.

\item[Predrag's `method of connections']

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% from Rowley-Marsden paper
\SFIG{connections}
{}{
Method of connections, as illustrated by Rowley and
Marsden\rf{rowley_reduction_2003}.
}
{fig:connections}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%


We decompose $\vel(x)$
in a part $\vel_\shortparallel$ parallel
to the group action and a part $\vel_\perp$ transverse to it,
\beq
	\vel(\ssp)=\vel_\shortparallel(\ssp)+\vel_\perp(\ssp)\,,
\ee{flowSplit}
using the projection operator
\beq
 	?? %\PperpOp_{ij}(\ssp)
 =\delta_{ij}-
    \frac{ \groupTan(\ssp)_i \groupTan(\ssp)_j}{\groupTan(\ssp)^2}
\ee{transvProj}
that projects a $d$-dimensional flow $v(\ssp)$ onto
flow
\beq
	\dot{\ssp}_\perp = \vel_\perp(\ssp) = \vel(\ssp)
    - \frac{\groupTan(\ssp) \cdot \vel(\ssp)}{\groupTan(\ssp)^2}
      \, \groupTan(\ssp)
\ee{transvFlow}
in a $(d\!-\!1)$-dimensional {\csection} transverse to the
direction fixed by the point $\ssp$. By ignoring the flow
component that can be compensated for by an $\SOn{2}$
rotation we quotient the flow by $\SOn{2}$.

For an illustration, Rowley and
Marsden\rf{rowley_reduction_2003} \reffig{fig:connections}.


Note, however, that a choice of $\ssp_0$ fixes only a
direction, so the reduced flow is still equivariant under the
action of discrete cyclic group $\Ztwo = \{e,D(\pi)\}$ on
$\ssp$, $\vel(\ssp)$ and the reference point $\ssp_0$, just
as was the case %\refeq{LorenzR}
for the Lorenz flow. %\refeq{Lorenz}.
    \PC{\emph{Mea culpa}: Here I screwed up. I forgot that rotation
    moves $\vel$ and counter-moves $\ssp$ in $\vel(\ssp)$, \ie,
    acts by the Lie derivative \refeq{inftmInv}. I could never
    understand why we do not see a translational zero eigenvalue
    everywhere (the Lie group acts globally and commutatively
    right?), but only on \eqva, \reqva\ and \rpo s. Presumably
    the projection operator \refeq{transvProj} is OK for the
    \reqv\ calculation of \refeq{sect:StabEq},
    as the action of the group on $\ssp_{\REQV{}{1}}$ is trivial?
    Not sure how to rewrite the decomposition induced by
    \refeq{transvProj} correctly, in
    terms of the full Lie derivative action, and not only the $\Lg$
    action.
    }


\item[Predrag's integral of the equivariance condition]
Here is a suspect attempt to derive a connection formula.
Integrate the equivariance condition \refeq{inftmInv}:
\bea
{const} &=& (\Lg_a)_{kj}
          \int_0^t d\tau \left(
  \delta_{ik}\vel_j(\ssp) - \Mvar_{ik}(\ssp)\, \ssp_j
           \right)
  \continue
  &=& (\Lg_a)_{ij} (\ssp_j(t)-\ssp_j(0))
    - (\Lg_a)_{kj} \int_0^t d\tau \Mvar_{ik}(\ssp(\tau))\, \ssp_j(\tau)
    \continue
  &=& t(\ssp) - t(\ssp_0) - \int_0^t d\tau \Mvar (\ssp(\tau))\, t(\ssp)
  \,.
\label{integralInv}
\eea
If $const=0$, this is a formula for the transport of the group
tangent field,
\[
t(\ssp) = t(\ssp_0) + \int_0^t d\tau \Mvar (\ssp(\tau)) \, t(\ssp(\tau))
\,,
\]
which is not simply the multiplication by \jacobianM\ $\jMps$.
For a linear flow, this looks a bit unfamiliar:
\[
t(\ssp) = t(\ssp_0) + \Mvar\,\Lg_a \int_0^t d\tau \, \ssp(\tau)
\,.
\]
\end{description}

\section{2009-08-26 Method of slices for an $U(1)$-equivariant linear model}
\renewcommand{\ssp}{a}

%{\em \underline{Preamble:} My gut tells me that the method
%of moving frames, as described in the Section 4.2 of the
%thesis will \underline{not} work for KS.  But since my brain
%is dafter than my gut, I cannot explain why I feel that way.
%So, in order to make progress in this direction, I'm going to
%work with a dynamical system similar to KS, but much simpler.
%If I can figure out how to apply the moving frames to this
%system, then I can do it for KS as well.  If not, then I hope
%it will help me understand why my gut is right. \vspace{2ex}}

\medskip\noindent{\bf Ruslan:} Forget \KS, let's consider a simpler dynamical system.
Let us say that, just like KS, we have a dynamical system on the space of real function $u(x,t)$ periodic in $x \in [-\pi, \pi)$, i.e. $u(x+2\pi,t) = u(x,t)$.  In the Fourier space $a = \mathcal{F}[u]$, $a = (a_1, a_2, \ldots)$, $a_k \in \mathbb{C}$ and $a_{-k} = a_k^\ast$.
I will also use polar coordinates, so $a_k = r_k \mathrm{e}^{i \phi_k}$.
The action of $U(1)$ on $u(x,t)$ is $g(\theta) u(x,t) = u(x+\theta,t)$. In the Fourier space
\[ g(\theta) a_k = \mathrm{e}^{ik\theta}a_k\,, \]
%In polar coordinates $\tau_{\ell/L} (r_k, \theta_k) = (r_k, \theta_k + q_k \ell)$.
To define the $U(1)$ group rotation tangent $t(a)$, we consider an infinitesimal rotation
\[ \mathrm{e}^{ik\theta}a_k = (1 + ik\theta)a_k  = a_k + \theta t_k\,, \]
so $t_k = ika_k$.
In matrix notation, $\mathbf{T} = \mathrm{diag}(ik)$,
    so  $t = \mathbf{T}a$.

The equation defining the slice through some point $\slicep$ is
\[ (\bar{a} - \sliceTan{}) \cdot \sliceTan{} = 0 \]
By the dot product we mean
\[ a \cdot b = \sum_{k=-\infty}^\infty a_k b_k^*\,, \]
which in the space of real periodic functions corresponds to
\[ f \cdot g = \int_{-\pi}^\pi f(x) g(x) dx\,, \]
where $f(x) = \mathcal{F}^{-1}[a]$ and $g(x) = \mathcal{F}^{-1}[b]$,
so this makes sense.  But note that this is not
the only way of defining the dot product.

\subsection{2009-08-26 Epicycles 2-Fourier modes linear dynamical system}
\label{sect:epyc2Fourier}

\medskip\noindent{\bf Ruslan:}
Let us define the dynamical system $\dot{a}_k = v_k(a)$ as follows:
$v_k = i \omega_k a_k$, $\omega_{-k} = -\omega_k$,
where $\omega_k \in \mathbb{R}$ are constants.
    \RLD{$\omega_{-k} = -\omega_k$.
    I used this in my derivation to get the sign right.
    {\bf Predrag} Agreed - I used that too in rederiving your model.}
This
\HREF{http://www.c2.com/cgi/wiki?AddingEpicycles}
     {``epicycles'' model}
is a linear dynamical system, so we can solve it analytically:
\[ a_k(t) = a_k(0) \mathrm{e}^{i \omega_k t} \]
Even simpler, I'm going to use only the first two modes, so $a_k(0) = 0$ for all $|k| \neq 1$ or 2.

We can choose constants $\omega_1$ and $\omega_2$ such that this system can have
a traveling wave or a RPOs in the original space of periodic functions $u(x,t)$.\\
{\bf Example 1: \Reqv.} If $\omega_1 = c$ and $\omega_2 = 2c$, then
\[ a_k(t) = a_k(0) \mathrm{e}^{ikct} = g(ct) a_k(0)\,, \]
which is a wave traveling with speed $c$.\\
{\bf Example 2: \Rpo.} For the system to have an RPO, we can choose, for example,
$\omega_1 = \pi/2$ and $\omega_2 = 3\pi$.  This RPO has period $T = 1$ and shift $\pi/2$, since
\[ a_1(1) = a_1(0) \mathrm{e}^{i\pi/2} = g(\pi/2) a_1(0) \quad \mathrm{and} \quad
   a_2(1) = a_2(0) \mathrm{e}^{i3\pi} = a_2(0) \mathrm{e}^{i\pi} = g(\pi/2) a_2(0) \]

Let us now see what happens if we apply the method of slices to these two examples.
The reconstruction equation is
\[ \dot{\theta} = \frac{v(\bar{a}) \cdot \sliceTan{}}{t(\bar{a}) \cdot \sliceTan{}} \]
Let us say the initial condition $a_1(0) = r_1 > 0$, $a_2(0) = r_2 > 0$
(i.e. $\phi_1 = \phi_2 = 0$) is also the point $\slicep$ defining the slice.  Then
$\sliceTan{}_k = ikr_k$ when $|k| = 1,2$ and zero otherwise.  So,
\beq
\dot{\theta} = \frac{\sum_k (i\omega_k \bar{a}_k) (ikr_k)^*}{\sum_k (i k \bar{a}_k)(ikr_k)^*}
                = \frac{\sum_k k \omega_k \bar{a}_k r_k}{\sum_k k^2 \bar{a}_k r_k}
\ee{RLDrec}
The equation for the flow on the slice (aka reduced flow) is
\beq
\dot{\bar{a}}_k = v_k(\bar{a}) - \frac{v(\bar{a}) \cdot \sliceTan{}}{t(\bar{a}) \cdot \sliceTan{}} t(\bar{a})
                   = i\omega_k \bar{a}_k - \frac{\sum_k k \omega_k \bar{a}_k r_k}{\sum_k k^2 \bar{a}_k r_k} ik\bar{a}_k\,.
\ee{RLDred}
\medskip\noindent{\bf Predrag}
I have coded \texttt{wilczak/matematica/PCruslan.nb} also for
$\slicep$ complex, if we need it - no reason to chose it real,
though experimentally it seems not to be the cause of the singularity in
$d\theta/dt$.

\noindent {\bf Example 1: \Reqv.} Here $\omega_k = kc$, so we immediately get
\[ \dot{\theta} = c \quad \mathrm{and} \quad \dot{\bar{a}}_k = 0\,, \]
as expected for the traveling wave.\\
{\bf Example 2:  \Rpo.} In general, for the flow with two non-zero
modes (remembering that $a_{-k} = a_k^*$ and $\omega_{-k} =
-\omega_k$):
\beq
 \dot{\theta} = \frac{\omega_1 r_1 (\bar{a}_1 + \bar{a}_1^*)
                  + 2\omega_2 r_2 (\bar{a}_2 + \bar{a}_2^*)
                  }{r_1(\bar{a}_1 + \bar{a}_1^*) + 4r_2 (\bar{a}_2 + \bar{a}_2^*)}
\ee{Period1}
and
\[ \dot{\bar{a}}_k = i\omega_k \bar{a}_k - \frac{\omega_1 r_1 (\bar{a}_1 + \bar{a}_1^*) + 2\omega_2 r_2 (\bar{a}_2 + \bar{a}_2^*)}{r_1(\bar{a}_1 + \bar{a}_1^*) + 4r_2 (\bar{a}_2 + \bar{a}_2^*)}ik\bar{a}_k \]
When $\omega_1 = \pi/2$ and $\omega_2 = 3\pi$, the second equation should generate a periodic orbit with period $T = 1$, while the solution of the first equation, starting with $\theta(0) = 0$, should give us $\theta(1) = \pi/2$.  Unfortunately, we cannot solve these equations analytically, so I'll do it numerically for different values of $r_{1,2}$.  I'll set $r_1 = 1$ and increase $r_2$ from zero.  Note that when $r_2 = 0$, we just have a traveling wave with speed $\omega_1$.  Also, when $r_2$ is sufficiently smaller than $r_1$, we will have a traveling wave slightly modulated by the 2nd mode.  This case is shown in the figure below:

\vspace{2ex}\noindent\includegraphics[width=\textwidth]{sliceflow1.eps}

So, everything works well: the reduced orbit is periodic (the beginning of the orbit is denoted by the red circle, while the end is denoted by the black asterisk), and the phase shift at $t = 1$ is equal to $\pi/2$.  We can see similar picture when we increase $r_2$:

\vspace{2ex}\noindent\includegraphics[width=\textwidth]{sliceflow2.eps}

Now, when $r_2$ approaches 0.5, it is clear that the orbit approaches a singularity:

\vspace{2ex}\noindent\includegraphics[width=\textwidth]{sliceflow3.eps}

At $r_2 = 0.5$ we get rubbish:

\vspace{2ex}\noindent\includegraphics[width=\textwidth]{sliceflow4.eps}

which persists for awhile

\vspace{2ex}\noindent\includegraphics[width=\textwidth]{sliceflow5.eps}

until we get back the nice smooth solution:

\vspace{2ex}\noindent\includegraphics[width=\textwidth]{sliceflow6.eps}

which, however, is no longer periodic, while the shift is now $3\pi/2$.  This picture persists for all $r_2 > r_1$.

\vspace{2ex}\noindent\includegraphics[width=\textwidth]{sliceflow7.eps}

So, it looks like the problem with the method of slices is not only due to running into the singularities, but also due to the possibility of generating smooth orbits which go around those singularities and generate wrong shifts without any indication that something has gone wrong.  These orbits are no longer periodic in the reduced space.  The only time this method works reliably is when the flow is dominated by the first Fourier mode.  If this is not the case, as in the KSE regime with multiple active modes, then we cannot expect that it will work.~~{\em Q.E.D.}

\vspace{2ex}
An old idea presented as a new one:  Why don't we define the dot product in the above equations as follows:
\beq
 a \cdot b = a_1 b_1^* + a_1^* b_1
 \,.
\ee{RLDfix}
Then, if I'm not mistaken, the whole thing reduces to my original idea for factoring out the $U(1)$ symmetry from the KS flow.  In this case we won't need to worry about running into, or moving around, the singularities while generating spurious phases.  The only singularity will be at $r_1 = 0$, which is trivial to deal with.

With the dot product defined as in \refeq{RLDfix},
\refeq{RLDrec} and \refeq{RLDred} become
\[
  \dot{\theta} = \omega_1
  \quad \mathrm{and} \quad
  \dot{\bar{a}}_k = i(\omega_k - k\omega_1) \bar{a}_k
\,.
\]
So they (obvious to Ruslan) do the job.


\subsection{2009-08-25 Eurosceptics do not like to slice}
\noindent{\bf Ruslan:} (Predrag's translation) one gets a
phase shift by $\pi$ if one crosses the singularity.
\\
{\bf Ruslan:} The shifts by $\pi$ is the least of my worries.
It is the fact that when $r_2 > r_1$ we get a nice smooth
solution of the reconstruction and reduced equations, yet we
get obviously wrong results.  I don't want to think why this
is happening.  I'd rather just stop using the approach where
such a thing can happen.
\\
{\bf Predrag} Your model illustrates why we need
monotonicity in phase (we are integrating 1d equation,
velocity cannot change sign?). But I think we will fix it -
it is like WKB for harmonic oscillator, one gets geometric
$\pi$ phase for each turn, and a neat way to do it is Maslov
trick - change slice at $\pi/4$, then change back at
$3\pi/4$, etc. Looks like something we can figure out. Sure
cute - getting semiclassical Keller-Maslow phase without
doing neither wave nor quantum mechanics. At least, not
consciously.

More interesting will be the phase for the method of
connections. If we are lucky, it is the same for relative
periodic orbits.
\\
{\bf Ruslan:} We had the discussion about monotonicity some
time ago (see above).  I don't have anything more to add at
the moment.
\\
I don't know much about WKB, or Keller-Maslow
phase, but I would like to get away from generating all kinds
of fixes, like using random $\slicep$ or switching between
slices.

\medskip\noindent{\bf Predrag} 1st mode will not work. Actually,
what Ruslan has run into so far is the same problem you
(Vaggelis) and Rebecca run into independently; he is
measuring angle in polar coordinates, where we also run into
$d\theta/dt$ divergence, and shifts by $\pi$.
\\
{\bf Ruslan:} I disagree that 1st mode won't work, I think it
will. And I don't think using polar coordinates is the
problem.  Forget about dynamics.  Just look at functions
defined on a circle and ask yourself how to identify
functions that differ only by a shift.  My answer: The phase
of the 1st Fourier mode will give you the shift.  So just use
it to rotate the functions on top of each other.

\noindent {\bf Vaggelis:} Apart from quantum mechanics there
are classical systems that exhibit such behavior, see for
example
\HREF{http://prl.aps.org/accepted/L/cf079Ya5Q7f19828e08c7183313c1d9b2e9126ce4}
{this PRL paper}. I've only seen it in Hamiltonian systems,
where this behavior is called monodromy since one studies the
system as it goes once around a singularity (is the etymology
clear to non-Greeks too?). It apparently arises when a
function fails to be single-valued, which again should point
out why we need monotonicity in phase.\\
\textit{Question for Ruslan:} Is it really trivial to deal
with the singularity at $r_1=0$?\\
{\bf Ruslan:} Yes, but you should never ever try to
numerically integrate the reconstruction or the reduced flow
equations if you expect to get the correct reduced
representation of the orbits of the original flow.  Instead,
integrate the original flow and then pull the obtained orbit
onto the slice while keeping track of the shift you generate
while doing it.  If the original orbit goes exactly through
$r_1 = 0$ (within the round-off error), then add (or
subtract) $\pi$.  That's all.\\
{\bf Vaggelis:}  I agree that integrating the equation on the
slice is not the safest think to do. I would like to
understand where the spurious shifts that you have discovered
come from, though. It looks like the reduced space has a
branch cut and as one integrates the equations around the
singularity he finds oneself on a different leaf.

There is something I don't understand in the procedure you
describe here. Does the need to add or subtract $\pi$ come
from the fact that most implementations of $arg$ or $arctan$
functions do not distinguish quadrants? I do not understand
how else it is connected to crossing through $r_1 = 0$ or why
it does take care of the singularity. You can approach the
singularity through any direction on the $a_1$ plane, so the
way to overcome the singularity on a point with $r_1=0$ seems
to be to use the angle at which you approach to it (that you
get from the previous point) to correctly rotate it onto the
slice. The difficulty is that then you are not able to tell
where $\EQV{2}$ and $\EQV{3}$ lie on this space as they both
have $r_1=0$: their group orbits do not intersect your slice,
so you do need another ``coordinate chart'' to cover the
space.
\\
{\bf Vaggelis:} You still need to cover the reduced space
with more than one coordinate systems which is essentially
the same as choosing a new slice.\\
{\bf Ruslan:} No. You don't need more than one coordinate
system.  Just use the Fourier modes as reduced coordinates,
with the phase of the 1st mode fixed.  The phase of the 1st
mode will be the reconstruction shift.\\
{\bf Vaggelis:} Even if this doesn't bother us, I am afraid
that we will get projections that are not more informative
than Figure 42 in todays version of my thesis, where the
singularity was dealt with but $\EQV{2}$ and $\EQV{3}$
collapse to the same point.\\
{\bf Ruslan:} I cannot guarantee that the pictures we get
will be nice and simple.  Of course, since the reduced space
is a semi-space (because $r_1 \geq 0$), the reduced orbits
may not always look pretty and smooth: They will have sharp
turns when they come close to, or hit, the $r_1 = 0$
subspace.  But at least when I look at orbits projected onto
Fourier modes, I know how to interpret them.  When I'm
looking at the projections onto some exotic curvilinear
coordinates, the pictures make no sense to me.\\
{\bf Vaggelis:} On the utility of Fourier modes as a
representation I disagree in two levels. I anyway find that
Fourier modes projection tell us very little about dynamics.
Wasn't this the reason to use different coordinate systems in
the KS paper? So as long we can construct dynamically
meaningful projections in the new variables I do not worry
how we got them.

Furthermore, what you describe above is equivalent to using
the invariants of Table 3 in Chapter 8 in my thesis. The new
coordinates are exactly the Fourier modes rotated back to the
slice defined by $\Im(c_1)=0$, this is how they were
obtained. The difference is that in Table 3 the singularity
is explicit. The important point though is that what you like
to see as a linear transformation is in fact a nonlinear
transformation through the dependance of the angle on the
point in space in which it is evaluated. Whereas in a linear
transformation all points in space are rotated by the same
angle. Therefore without knowing it you use exotic
curvilinear coordinates. The connection to the original
Fourier modes is that the magnitude in each Fourier plane
stays invariant. Of course things get even more exotic in my
thesis, but the motivation behind that was to get projections
that provide more information about the dynamics.

As a general comment, it appears that when we try to see the
reduced space as embedded in the original space, no matter
how we go from $N$ to $N-1$ dimensions we impose a
conservation law that did not exist in the original system
(as it was allowed to have motion in the direction of group
action) that restricts the motion to an $N-1$ dimensional
space which cannot in general be given a nice structure.

\subsection{2009-08-26 Eurosceptics carry the day}

\noindent{\bf Predrag} OK, now that Ruslan has gone on
strike I spent a day screwing around with Mathematica,
checked epicyclist formulas and reproduced Ruslan's graphs.
As I am using \texttt{NDsolve[\dots]} as a black box, I do
not get the same screwy details close to $(r_1,r_2)=(1,0.5)$,
but the result is the same - for $r_2$ sufficiently larger
than 1/2 one gets extra $\pi$ shift, with no integration hint
that one is going around a singularity. That is unacceptable.
As Ruslan expects,
random choices of complex (not real) \slice\ fixing
point $\sliceTan{}$ move the singularity around but are essentially no
help.

I will still try to use Maslov trick and switch the slice
whenever $\dot{\theta}$ starts misbehaving, but for that I
need to learn how to use \texttt{Method -> \{"EventLocator"}
within \texttt{NDsolve[\dots]}, but that sure looks like a
bitter pill. I do not mind having several slices as long as
the returns to (dynamical) Poincar\'e sections trace out
smooth unstable manifolds suitable to partitioning the
\reducedsp.

Marsdenites did not note this problem as they only applied
the method to \reqva, and there it is OK.

I agree that if one is to fix the \slice\ by one Fourier
mode, \refeq{RLDfix} is the most natural choice, and would be
easiest to explain in a publication. Unfortunately, we ran
into singularities when we tried it for \CLe\ in Siminos
thesis\rf{SiminosThesis} and
\HREF{http://ChaosBook.org/projects}
     {Rebecca's summer project}.
But Ruslan, please do give it a try.

\medskip\noindent{\bf Ruslan:}
Actually, not really on strike, just reluctant to participate
in your, what I believe to be, futile efforts to apply all
kinds of fixes and patches to the approach which, I believe,
is fundamentally incurable when it comes to systems like KS
in a fully developed chaotic regime. [\dots]

\medskip\noindent{\bf Predrag} Fair enough - way too many fixes and
patches. We need to quotient the symmetry in order to figure
out the symbolic dynamics for KS and for plane Couette and
pipe flows. If something like \refeq{RLDfix} works it would
be nicer. The reason why we think it will not is that when we
use the polar coordinates-inspired \slice\ fixing point
$\ssp^{*}=(0+i,0, \cdots)$, $\Re\ssp_1=0,\;\\Im\ssp_1>0$
(which I believe for $U(1)$ version corresponds to fixing the
phase of the first Fourier mode) the \reducedsp\ equations
are given by
\beq
\dot{\ssp} = v - \frac{{\Im} v_1}{{\Re}\ssp_1} t(\ssp)
\,.
\ee{EqMotionMovFrame}
Trajectories shown in %\reffig{fig:PCunrot1}
Siminos thesis\rf{SiminosThesis} and
\HREF{http://ChaosBook.org/projects}
     {Rebecca's summer project}
exhibit jumps by $\pi$. With $\ssp^{*}$ on \reqv\ orbit we
were luckier, and got a strange attractor which encountered
no $\dot{\theta}$ singularity. But unhappy, as we did not
understand why we were lucky.

\medskip\noindent{\bf Predrag} to Vaggelis: when you tried my initial
proposal to project velocity locally (not on a global slice),
the method that when fixed will probably be called ``method
of connections'' - are you sure that the extra geometrical
phases gained by relative periodic orbits were not rational
fractions of $\pi$?

\subsection{2009-08-27 Don't worry. Be happy}

\begin{description}
\item[Ruslan]
    Let me start by answering Vaggelis's question about rotation
by $\pi$.  Forget about multiple modes for the moment.  Just
consider evolution of the first mode: $a_1(t) =
r_1(t)\mathrm{e}^{i\phi_1(t)} \in \mathbb{C}$.  If $a_1(t)$
goes through zero, then $r_1(t)$ bounces off of zero, while
$\phi_1(t)$ changes by $\pi$ or $-\pi$, (the sign is
immaterial).   This is just the nature of the polar
coordinates and has nothing to do with the implementation of
$arg$.

\item[Vaggelis]
But still I don't understand why would you need to add or subtract
$\pi$? Doesn't the function you use to calculate the angle in this
plane take care of that?

\item[Ruslan]
That's it.  All I was saying
was that $\phi_1$ would change by $\pi$ or $-\pi$ as $a_1$
crosses zero.  And that's what the angle function would give
you.  Nothing else needs to be added.

\item[Ruslan]
Now about the nature of what you call the 'singularity'.  If
I understand it correctly, what worries you is that, if we
take $\phi_1$ as our $\theta$, then each time it changes by
$\pi$, the $k$-th mode needs to be changed by $k\pi$, $k >
1$.  You perceive this as a jump, i.e. a discontinuity of the
reduced orbit, which you don't like.  Is this what you mean
by the 'singularity'? If `yes', then do you really need to
worry about it?

\item[Predrag]
If $1/x$ is considered a
`singularity' for $x=0$ on small islands off Continent, than
we will be so bold to call it singularity. As you say, there
should be simple analytic fixes for going through zero; we
have to make sure that we teach our numerical routines how to
implement this. Your pretty and clean epicycle model just
gave me a new set of ulcers, as for $r_2$ not very larger
than 1/2 the integrators seem not to know that there was any
singularity at all, and relative periodic orbits became
periodic only mod~$\pi$. And $\pi$ or $-\pi$  sign is not
immaterial - we have to get \rpo s shifts right.

\item[Vaggelis]
I want to add that the singularities in expressions such as
in Table 3 in Chapter 8 of my
thesis cannot be essential, in the sense that, since the transformations
are generated by rotations, the expressions
cannot really blow up. In practice they pose numerical issues,
related to the small denominators in these expressions. So I agree there
should be simple analytic fixes.

\item[Ruslan]
If you think about it, what you perceive as a jump is
actually a very fast rotation (infinitely fast if you go
exactly through zero, but otherwise finitely fast).  If you
don't like that it rotates so fast, then why don't you just
rescale the time?  Just slow it down near $r_1 = 0$.  I think
something like $d\tau = dt/r_1(t)$ might work, since it will
remove $\Re\ssp_1$ from the denominator in
\refeq{EqMotionMovFrame}.

\item[Predrag]
Agreed. I have been also thinking about redefining time.
That's also in ChaosBook
\HREF{http://chaosbook.org/chapters/conjug.pdf} {Chapter 6 - Get straight},
called there the
Kustaanheimo-Stiefel (also known as KS!) transformation, and
applied in Example 6.2: what to do with Keplerian ellipses of
arbitrarily large velocity. That we might need for close
passages to the $U(1)$ invariant subspace, with $r^2 = \sum
\ssp_k^*\ssp_k$ going small. Fortunately, in KS that never
happens - strange attractor is safely away from the $u(x)=1$
\eqv. Our problem with method of slices is more naive, as you
explain above.

Of course, the real challenge is: who will be the first to read the
\HREF{ChaosBook.org/projects/siminos/thesis.pdf}
      {Thesis that Nobody Reads} first?

\item[Ruslan]
On the other hand, if our goal is to eventually reduce the
dynamics to a Poincar\'e map, then why do we need to worry
about time at all?

\item[Predrag]
Agreed - anything that gets us the sensible return (or
forward) maps is good. Just have to make numerics is correct,
and yields correct \rpo s shifts and periods - at the moment
we do not know how to do that right even for the 2-mode
epicycles model.

\item[Vaggelis]
I also agree, that was my thesis final conclusion on what one should do: just
make sure that the Poincar\'e section is away from problematic regions. It is
just that finding a good section is not always easy, so one needs some visualization
that works well in order to get intuition on where to place it.
\end{description}

\subsection{2009-08-28 On ulcers, singularities, and `tender beasts'}

\noindent{\bf Ruslan:} I thought the dot product
\refeq{RLDfix} would cure your ulcers for the epicycle model,
and I'm pretty sure for the CL and KS as well, provided you
stop worrying about the fast rotations in the reduced space.
I don't mind that you call them `singularities', but, hearing
your complaints about numerics, I'm pretty sure somebody
does...  Let me tell you something about computers: they are
tender beasts and if you say, or even think, this word in
their presence, they will get spooked and throw all kinds of
fits.  But seriously, as I already said above, any symmetry
reduction or other transformations that you want to carry out
should be done as post-processing, i.e. after you obtain your
numerical trajectory by integrating the original flow.  That
way the singularities are much more tractable numerically.

\subsection{2009-08-27 Maslov trick}

\noindent{\bf Predrag}  The Maslov trick is described in
\HREF{http://chaosbook.org/version12/chapters/WKB.pdf}
{ChaosBook vers. 12, Chapter 31 - WKB quantization}.
Now I realize I give no references to Maslov paper or other
relevant sources - if you find them, please let me know so we can
write up the remark on the Maslov approach. Citing from this
chapter:

A simple physical picture, due to Maslov, is illustrated by quantization
of the harmonic oscillator. Semi-classical propagator has
a factor $1/$(velocity$)^{1/2}$, reminiscent of our $\dot{\theta}$, and
that blows up at the turning points, points where the particle as viewed
from the $q$ coordinate frame reverses velocity.

In the $q$ coordinate, the turning points are defined by the
zero kinetic energy condition,
and the motion appears singular.
This is not so in the full \statesp: the trajectory in a smooth confining
1-dimensional potential is always a smooth loop,
with the ``special'' role of the turning points $q_L, q_R$ seen to be an
artifact of a particular choice of the $(q,p)$ coordinate frame
(in our application, choice of a slice). Maslov's
idea was to proceed from the initial point
$(q',p')$ to a point $(q_{A},p_A)$ preceding the turning
point in the $\psi(q)$
representation, then switch
to the momentum representation (in our application, use a slice turned by
$90^o$)
continue from $(q_A, p_A)$ to $(q_B, p_B)$, switch back to the coordinate
representation (in our application, use a slice turned by
$90^o$), and so on.

In other word, as slices are local, switch to the next one whenever
convenient, and for recurrent orbits, make sure you are in the original
slice when you come back, in order to get a return map and hopefully sensible
symbolic dynamics.

\subsection{2009-08-28 Any 2-mode epicycle reduces to a periodic orbit}

\begin{description}
\item[Predrag]
A generic 2-Fourier modes epicycle trajectory runs on a 2-torus in the
full \statesp. Hence any trajectory is a \po\ in the \reducedsp, not
only the hand-crafted \rpo\ such as \refeq{RLDrec}.
(I actually verified this statement by running random trajectories
in \texttt{Mathematica}, so it is true.)
For testing methods of symmetry reduction
it might be a good idea to increase the number of Fourier modes.
\item[Ruslan]
That's right.  And thinking about it
as a 2-torus might hint at why the singularity develops as
$r_2$ grows. As $r_2$ grows, at some point the torus becomes
self-intersecting. I'm sure this happens when $r_2 = r_1/2$.
The SO(2) group orbit on this torus is a line that winds once
around $r_1$ and twice around $r_2$.  As the torus becomes
self-intersecting this orbit first self-intersects at $r_2 =
r_1/2$ and then forms a knot.  And that's why an extra $\pi$
shift appears when we try to project an orbit onto the slice
normal to this knot.  This is just a speculation, but maybe
there is something in it.
\end{description}

\section{2009-08-27 Symmetry reduction by method of connections?}

\noindent{\bf Predrag}
Here we take $t(\ssp)$ to
be the tangent for the trajectory state space point \ssp\ is
used \emph{locally}, within the `horizontal' hyperplane
normal to $t(\ssp)$.

The 2-epicycle model is a linear sum of Fourier modes
$(\ssp_{-2},\ssp_{-1},1,\ssp_1,\ssp_2)$.
Predrag's first guess for the reconstruction equation for
phase shift for slice fixed \emph{locally} by $t(\ssp) =
T\cdot\ssp(t)$ was
\beq
\frac{d\theta}{dt}=
     \frac{t(\ssp)^{*} \cdot \dot{\ssp}(t) + \dot{\ssp}(t)^* \cdot t(\ssp)}
                        {2 \,t(\ssp) \cdot t(\ssp)^{*}}
\,,
\ee{MethConnWrong}
which evaluates to
\[
\frac{\omega_1\, \ssp_1(t) \ssp_1(t)^{*}
  + 2\,\omega_2\, \ssp_2(t) \ssp_2(t)^{*}
     }{
               \ssp_1(t) \ssp_1(t)^{*}
  +         4\,  \ssp_2(t) \ssp_2(t)^{*}}
\,.
\]
This integrates to a convoluted, initial point dependent
but \po\ in the reduced
space, because in the 2-epicycle model all motion is on
2-torus in the full \statesp, 1-torus in the
$U(1)$-\reducedsp. The reconstruction equation
\refeq{MethConnWrong} appears to be wrong; I forgot to
include the `connection,' \ie, the effect of the frame itself
changing in the next time step. Presumably \stabmat\
needs to be included, as in the Lie algebra equivariance
condition
(see Siminos thesis\rf{SiminosThesis} and
\HREF{http://ChaosBook.org/projects}
     {Rebecca's summer project}).
For linear models \stabmat\ is constant, would be easy to
include into integrations.

If you want to be a path-breaking American type entrepreneur, rather
than a Eurosceptic kvetch, derive the right reconstruction equation
for this case. The glove is thrown.

\begin{description}
\item[Ruslan to Predrag]
I've never aspired to be an American entrepreneur.
I'm a lazy Ukrainian, so I don't do things unless I'm
convinced they are relatively easy to do and will result in
something useful.  Besides, I don't like algebra, not even of
Lie kind, not even of any kind.  I like geometry.  And the
vague picture I have in my mind tells me that the method of
connections, being in some sense more local than the method
of slices, has even less chance of succeeding where the
problems we are experiencing are clearly of a global nature.
So, I don't understand why I should be trying to work through
this quite complicated calculation, while I have a much
simpler approach based on the 1st Fourier mode, which works
for epicycles and I'm quite sure will work for KS.  It might
not give me a nice and smooth orbit.  The orbit might even
look ugly, but at least it will be a correct symmetry reduced
orbit.

\item[ to Ruslan]
OK, we'll suffer through
convoluted algebraic thinking as it applies to the geometry
of equivariant flows, and you industrious subject of Queen
Elizabeth go test \refeq{RLDfix}, the simple approach based
on the first Fourier mode, on KS.

\item[ to Predrag]
OK, this glove fits me better, especially since I've done most of it 20 months ago.
\end{description}

\section{2009-08-29 Kuramoto-Sivashinsky O(2) quotienting}

\subsection{Back to the future:\\ pasting from siminos/blog/davidchack/071231fundamental.html}

\medskip\noindent{\bf Ruslan:}
To fix the notations, let me recap the Kuramoto-Sivashinsky equation (KSE) and its representation in Fourier space:
\[ u_t = -uu_x - u_{xx} - u_{xxxx},  \quad x \in [-L/2, L/2] \]
with periodic boundary conditions:  $u(x+L,t) = u(x,t)$.  In the Fourier representation
\[ u(x,t)=\sum_{k=-\infty}^{+\infty} a_k (t) e^{ i q_k x }\,,\]
where $ q_k = 2\pi k/L$, the KSE takes the form
\[ \dot{a}_k = v_k(a) = (q_k^2 - q_k^4)a_k - \frac{iq}{2}\sum_{m=-\infty}^{\infty}
    a_m a_{k-m}\,,\quad a_k \in {\cal C}\]
It is convenient to represent complex modes as pairs of real variables, either in cartesian or polar coordinates:
\[ a_k = (b_k, c_k) = b_k + ic_k = (r_k, \theta_k) = r_k e^{i\theta_k}\,. \]

\noindent{\bf Symmetries:} If $u(x,t)$ is a solution of the KSE, then so are
\[ \tau_{\ell/L} u(x,t) = u(x+\ell,t) \quad\mbox{and}\quad R\,u(x,t) = -u(-x,t).\]
The action of symmetry transformations on Fourier modes is as follows:
\[ \tau_{\ell/L} a_k = e^{iq_k \ell} a_k = (r_k, \theta_k + q_k \ell) \,,\qquad R\,a_k = -a_k^\ast = (-b_k, c_k)\,.\]

\subsubsection{Defining KSE quotient space $\pS/$O(2)} % (a.k.a. fundamental domain)}
\label{sect:RLDslice}

The KSE quotient space $\pS/$O(2) is defined in Fourier space as follows:
\begin{enumerate}
\item If $r_1 > 0$ then $\theta_1 = \pi/2$ and $\dot{\theta}_1 \leq 0$;
\item if $r_1 = 0$ then $\arg \dot{a}_1 = \pi/2$;
\item if $a_1 = \dot{a}_1 = 0$ then the KSE solution lives in
        the $L/2$-periodic invariant subspace, where $\pS/$O(2) is
        defined as above but for the 2nd mode;
\item and so on...
\end{enumerate}
So the quotient space $\pS/$O(2) consists of a hierarchy of
slices fixed by the $k$-th Fourier mode for the
$L/k$-periodic invariant subspace.

\begin{description}
\item[Predrag's comment] I do not find fixing a single
Fourier coefficient natural, and you pay for it by having to
deal with discrete shift symmetry subcases separately (fixing
higher $k$ Fourier coefficients).
\item[Ruslan's reply] I think it is completely natural that
for solutions symmetric by $L/k$ shift, we use the $k$-th
Fourier mode. Since functions in $L/k$-symmetric subspace are
invariant under KS dynamics, using $k$-th Fourier Mode in
$[0, L]$ is the same as using the 1st Fourier mode in $[0,
L/k]$.  Also note that, again because of the invariance of
these subspaces, the choice of the mode which defines
$\pS/$O(2) is fixed once and for all times by the initial
condition, so there is no need to jump between slices when
following the KS flow.
\end{description}

\subsubsection{Mapping KSE solutions to $\pS/$SO(2)}

Following Predrag's suggestion, I'll split the mapping of KSE solution to $\pS/$O(2) into two parts: 1) map to $\pS/$SO(2) by translation $\tau_{\ell/L}$; 2) map from $\pS/$SO(2) to $\pS/$O(2) by reflection $R$.

\medskip\noindent{\em ...to be continued...}

%\vspace{2ex}\noindent\includegraphics[width=\textwidth]{ks22rpo016.31_02.863.pdf}


%
% *Note 1:* The invariant subspace of antisymmetric solutions
%
% $$ u(x,t) = R\,u(x,t) = -u(-x,t) \quad\Rightarrow\quad Re\,a_k = 0 $$
%
% belongs to the FD.
%
% *Note 2:* If both
%
% $$ a_1 = 0 \mbox{~~and~~} \dot{a}_1 = 0 $$
%
% then the solution lives in the L/2-periodic invariant subspace, and the FD
% is defined in terms of
%
% $$ r_2 \mbox{~~and~~} \theta_2\,. \mbox{~~And so on...} $$
%
% A KSE solution
%
% $$ (r_1, \theta_1, r_2, \theta_2, \ldots) $$
%
% is mapped into the FD by translation
%
% $$ \ell/L = \frac{\pi/2 - \theta_1}{2\pi} $$
%
% and reflection if
%
% $$ \dot{\theta}_1 > 0\,, \mbox{~~or~~} b_1\dot{c}_1 - c_1\dot{b}_1 > 0\,, \mbox{~~since} $$
%
% $$ \dot{\theta_k} = \frac{b_k\dot{c}_k - c_k\dot{b}_k}{b_k^2 + c_k^2} $$
%
% The map into FD is characterized by two parameters: the translation
% parameter
%
% $$ \ell = \frac{\pi/2 - \theta_1}{2\pi}L $$
%
% and the reflection parameter
%
% $$ \rho = 1 \mbox{~~when~~} \dot{\theta}_1 \leq 0 \mbox{~~and~~} -1
% \mbox{~~when~~} \dot{\theta}_1 > 0 $$
%

\subsection{2009-08-31 A closer look at the singularity}

\noindent{\bf Vaggelis:}
Taking \CLe\ as an example I will examine how the invariants
\beq
\begin{split}
        \overline{x}_2 &= \sqrt{x_1^2+x_2^2} \continue
        \overline{y}_1 &= \frac{x_2 y_1-x_1 y_2}{\sqrt{x_1^2+x_2^2}}\continue
        \overline{y}_2 &=\frac{x_1 y_1+x_2 y_2}{\sqrt{x_1^2+x_2^2}}\,.
        \label{eq:invLaser}
\end{split}
\eeq
generated by straightforward application of the moving frame method behave as $x=x_1+i x_2$ approaches
zero. For the limit to exist we must get a direction independent result as $x\rightarrow 0$.
Using $x=r_x\, e^{i\theta_x}\,,\, y=r_y\, e^{i\theta_y}$ we can write, for instance, $x_2 y_1-x_1 y_2 = r_x r_y \sin(\theta_x-\theta_y)$
and therefore,
\beq
        \overline{y}_1= r_y\sin(\theta_x-\theta_y)\,.
\eeq
Therefore, for any given $y$, the limit of $\overline{y}_1$ for $x \rightarrow 0$ does not exist, as the above expression depends on the direction
on the complex $x$-plane along which we approach zero. In terms of projecting dynamics on variables \refeq{eq:invLaser} (or applying the equivalent
procedure of rotating points back to the \slice) this means that
we need to take into account the direction along which
we approach zero and use the `angle
of descent' as the angle with which we rotate points back to the \slice, if such points have exactly $x=0$ (in CLE this does not happen). Since the subspace $x=0$ is not flow invariant we do not need to worry about dynamics that stay within the subspace. We need to worry about the equilibria that exist in this subspace though, here the equilibrium at the origin, as there is no way to transform them to the new variables. For KS $\EQV{2}$ and $\EQV{3}$ belong to such a subspace.

\begin{description}
\item[Ruslan's more optimistic view of the singularity] Of
course, when $x = 0$, the angle $\theta_x$ is undefined.  But
if you look dynamically at $x(t)$ as it crosses zero at $t =
t_0$, then you realize that if $\theta_x(t < t_0) = \theta$
then $\theta_x(t > t_0) = \theta + \pi$ and all you need to
decide is how to define $\theta_x(t_0)$.  The most natural
definition is based on the direction of $\dot{x}(t = t_0)$,
i.e. $\theta_x(t_0) = \arg \dot{x}(t_0) = \theta + \pi$.
That's the reason I have item 2 in my definition of $M/$O(2)
quotient space.  So, as you can see, $\theta_x(t)$ remains
well defined at all times.

Regarding E2 and E3 in KS, they live in $L/2$- and
$L/3$-periodic subspaces, respectively.  So, to map them onto
$M/$O(2), we'll use the 2nd and 3rd Fourier modes,
respectively, as stated in items 3 and 4.

\item[Vaggelis]
I agree about trajectories that cross zero, but not about
\EQV{2} and \EQV{3}. As soon as you use 2nd and 3rd Fourier
modes you effectively use different transformations, so it is
not obvious to me how you can piece everything together in
the same space. Fortunately we are not interested on dynamics
in $L/2$- and $L/3$-periodic subspaces, as they are not
invariant, but rather on how unstable manifolds of \EQV{2}
and \EQV{3} organize the $L$-periodic space. As those
unstable manifolds do not have vanishing first Fourier mode
we can visualize them and forget the equilibria.

\item[Ruslan]
  Actually, it's somewhat the other way
around for me: what worries you, does not worry me and vice
versa.  The proposed hierarchy of transformations is
completely consistent, since the higher-mode transformations
only act on those {\statesp} points which are not influenced
by the 1st mode transform.  By the way, the $L/k$-periodic
subspaces \underline{are} invariant: e.g. if $a_1(0) = 0$ and
$\dot{a}_1(0) = 0$ then $a_1(t>0) = 0$, so the solution is
$L/2$-periodic (provided $a_2(0) \neq 0$).  And finally, we
do have to keep track of the special symmetries of the
unstable manifolds of the equilibria.  For example, once
$\EQV{2}$ is placed within $M/$O(2) using the 2-nd Fourier
mode, its unstable manifold completely lives in the
anti-symmetric subspace (i.e. $Re\,a_k = 0$). {\color{blue}
The 1st mode transformation doesn't do anything to it, since
for any point on the manifold $\theta_1 = \pi/2$ already.}
So, the manifold we show in our SIADS Fig.~5.6 is already in
$M/$O(2).  What worries me is that $\EQV{2}$ is represented
here by two different points ($\EQV{2}$ and
$\tau_{1/4}\EQV{2}$).  This is the result of the additional
symmetry of $\EQV{2}$.  Whether or not these two points can
be merged into one $\EQV{2}$ without loss of dynamical
information about the KS flow, remains to be investigated.

What I wrote above, highlighted in blue, is wrong.  SO(2)
still acts in the antisymmetric subspace, since the points
there can also have $\theta_1 = -\pi/2$.  So they are rotated
by $\pi$.  Whether or not this will also rotate
$\tau_{1/4}\EQV{2}$ into $\EQV{2}$ I'm not yet sure.

\item[Vaggelis]
SO(2) does not act in the antisymmetric subspace (in the
sense that it does not leave it invariant as a set) but
merely on the intersection of antisymmetric subspace and its
$\tau_{1/4}$ translated copy. The group orbits of points on
antisymmetric subspace produce a family of copies of it. One
can identify a single representative by choosing for instance
$\theta_1=\pi/2$, so I don't see any problem with points on
the unstable manifolds of $\EQV{2}$. $L/k$-periodic subspaces
\underline{are not} invariant: if $a_1(0)=0$ then in general
$\dot{a}_1(0)\neq 0$, due to the nonlinear terms. Right?

\item[Ruslan]
  I didn't quite understand your
comments about the translated copies of the antisymmetric
subspace, but if you think it's not causing any problems, I'm
happy.  Regarding the $L/k$-periodic subspaces, I guess I
wasn't defining them correctly.  So, if $\dot{a}_1 \neq 0$
then we can still map this point to $M/$O(2) using the 1st
mode.  I was speaking then about the parts of $L/k$-periodic
subspaces which remain invariant under the KS flow.  They
need to be mapped to $M/$O(2) using higher FMs.

\end{description}


\subsection{2009-08-29 - Implementation of Ruslan's $a_1$-fixed slice}

\noindent{\bf Vaggelis:}
I thought its time to shut up and compute something, so I've
implemented first part of (my interpretation of) Ruslan's
prescription for KS that should take care of $SO(2)$. Namely,
I have fixed the \slice\ following
the rules of \refsect{sect:RLDslice}:
\bea
\Im \bar{a}_1  &=& 0
\continue
\mbox{If } r_1 &>& 0\,,\quad \mbox{\ESedit{rotate to the \slice\ by}}
        -\arctan( \Im a_1/\Re a_1 )
\continue
\mbox{If } r_1 &=& 0\,,\quad \mbox{\ESedit{rotate to the \slice\ by}}
        -\arctan( \Im \dot{a}_1/\Re \dot{a}_1 )
\,.
\label{ES-RLDslice}
\eea


\begin{figure}
 (a)~\includegraphics[width=0.45\textwidth]{../figs/ksRotatedTW1um.eps}\,
 (b)~\includegraphics[width=0.45\textwidth]{../figs/ksRotatedE2um.eps}
\caption{
 $(\Re \bar{a}_1,\,\Re \bar{a}_2,\Im \bar{a}_2)$ projections of KS
 $\SOn{2}$-reduced dynamics by slice \refeq{ES-RLDslice}.
 Several \rpo s are shown along with the
 unstable manifolds of (a) \REQV{+1}, (b) \EQV{2}. $L=22$.
}
\end{figure}

I've posted such a figure long ago in Jonathan's blog, but
nobody got interested. I am also not happy with this figure
but one might argue that there are implementation problems,
and this is probably right, so I will have a second look.
Then, if I am to follow Ruslan's next step in the
prescription and set $\Im \bar{a}_2 =0$ in order to include
$\EQV{2}$ in this figure, the equilibrium will not ``meet''
it's unstable manifold, correct?

\begin{description}
\item[Ruslan]
 I agree that the unstable manifold of $\EQV{2}$ reduced by
 the 1st mode to $\pS/\SOn{2}$ will not meet $\EQV{2}$ reduced by
 the 2nd mode. \par I have a question: do we expect that an
 antisymmetric solution of KS will remain antisymmetric in
 $\pS/\SOn{2}$?  If yes, then we are in trouble, which has
 nothing to do with using the 1st mode: As I stated above,
 the only thing SO(2) can do to leave an antisymmetric
 solution antisymmetric is rotate it by $\pi$.  However, in
 order for the unstable manifold of $\EQV{2}$ (which is
 antisymmetric) to return to back to $\EQV{2}$ we need the
 rotations along the orbit to add up to $-\pi/2$ (to
 compensate for the $\tau_{1/4}$ shift).  So, somewhere along
 the way the unstable manifold has to leave the antisymmetric
 subspace.  This doesn't seem natural, since antisymmetric
 subspace is invariant under KS flow.

\item[Vaggelis]
If we all agree that rotating points back to the slice $\Im\
a_1=0$ is equivalent to transformations of Table 3 in Chapter
8 in my thesis (that I copy here for convenience), up to
treatment of points with $a_1=0$, then my answer would be as
follows: applying reflections generated by $b_i\mapsto - b_i$
to the $u_i$ of \reftab{tab:SO2n6} we have $u_3\mapsto -u_3$,
$u_4\mapsto u_4$, $u_5\mapsto u_5$, $u_6\mapsto -u_6$ and so
on, so that the reflection matrix $\mathbf{R}$ in new
coordinates is diagonal, with entries $\pm 1$ and therefore
satisfies $\mathbf{R}^2=1$, as it should. Points in the
antisymmetric subspace $b_i=0$ for every $i$, are mapped to
points in the new coordinates that only depend on the $c_i$'s
and are therefore invariant under reflections, \ie,
antisymmetric. I don't see why the unstable manifold in
reduced space should leave the antisymmetric subspace, since
we don't even know were the equilibria live in such a reduced
space.

\item[Ruslan]
Well, we do know that the unstable manifold of $\EQV{2}$
converges to $\tau_{1/4}\EQV{2}$.  And I also assume that we
want $\EQV{2}$ and $\tau_{1/4}\EQV{2}$ to be represented by
the same point in the reduced space.  So, in order to bring
these two points together in the reduced space, we need to
rotate $\tau_{1/4}\EQV{2}$ by $-\pi/2$.  This rotation needs
to occur somewhere along an orbit (a closed loop in the
reduced space) within the unstable manifold of $\EQV{2}$.
But since the antisymmetric subspace is \underline{not}
invariant wrt rotation by $-\pi/2$, the orbit must leave the
antisymmetric subspace.  Another way to put this, is that, if
we only look at KS solutions within the antisymmetric
subspace, then $\EQV{2}$ and $\tau_{1/4}\EQV{2}$ are two
\underline{distinct} equilibria, since the first one is
unstable, while the second one is stable.  So, there is no
way to map these two equilibria into the same point while
staying within the antisymmetric subspace.

I don't know how much we should care about this, but this
tells me that, no matter how we construct the reduced space,
we will not be able to retain the simple structure of the KS
invariant subspaces within the reduced space.

\RLDedit{
\item[Ruslan: 2009-10-14]
{\em Preamble: I was hoping that Predrag would engage in the
above discussion, but since he's currently switched his
attention to 'separating the women from girls', I'm going to
continue talking to myself (and maybe to Vaggelis) here.}

I still don't see how to reconcile this: On the one hand, it
would appear natural that the antisymmetric subspace should
be a part of $\pS/\SOn{2}$, but, on the other hand, the
antisymmetric subspace contains two distinct images of
$\EQV{2}$, while, in the reduced representation of the full
KS flow, we would hope that there would be only one point for
$\EQV{2}$ in $\pS/\SOn{2}$.  I can see only two possibilities
here: either we introduce discontinuities of the flow in
$\pS/\SOn{2}$ (like it is happening with my Fourier modes
representation), or we collapse the dynamics to a subspace
that completely ignores the phase information (e.g., like in
the energy transfer representation).
    }

\PCedit{
\item[Predrag 2009-10-15]
Yes, please quotient the full $\pS/\On{2}$. It's good to do so,
quotienting discrete symmetries helps a lot, see
\wwwcb{/chapter/discrete.pdf}.
    }

\RLDedit{
\item[Ruslan: 2009-10-14]
Since I don't like collapsing things (I would like to have a
representation where I can keep track of the phase shift
$\theta$, which will allow me to reconstruct the original
dynamics if I wish to do so), and since I don't know how to
get rid of the discontinuities otherwise, I'm going to go
crazy and embrace them.  So, my plan here is as follows:
I'll try to construct a reduced representation (and hopefully
a Poincar\'e map) for the dynamics in the vicinity of the
unstable manifold of $\EQV{2}$. I think that if I manage to
do it here, then there is hope that it can be done for the KS
flow elsewhere as well.  I may continue using the Fourier
modes, or I might try something else, like using the
real-space representation (remember those $u$-$u_x$-$u_{xx}$
plots?).
}

\PCedit{
\item[Predrag 2009-10-15]
You keep track of discrete $\LieEl$ for a given full
\statesp\ trajectory by noting every time you need to apply
it (upon exit from the fundamental domain, see Lorentz flow
$\pS/\Ztwo$ Van Gogh attractor in
\wwwcb{/chapter/discrete.pdf}.
    }

\ESedit{
\item[Vaggelis to Ruslan]
Sorry that it took so long to get back to you, I am
just not sure I follow your arguments and I am afraid there
is a danger that the discussion becomes cyclic. The
antisymmetric subspace is indeed not invariant under $\pi/2$
rotations. On the other hand the image of the unstable
manifold of \EQV{2} under symmetry reduction by
\refeq{ES-RLDslice} is unique. It belongs to a
``antisymmetric set" in the reduced space, in which space the
action of reflections has been properly re-defined as in my
previous post. Of course the latter set is not invariant
under the action of reflection as defined in the original
space. In other words, if we would like to see the reduced
space as embedded in the original space we would have to say
that the linear antisymmetric subspace has been deformed in a
nonlinear manner through the transformations in
\reftab{tab:SO2n6} and reflections do not longer leave it
invariant. Then I do not see any problems with identifying
all copies of \EQV{2} with a single point. I just see greater
problems with a discontinuous $\pS/\SOn{2}$.

Regarding ignoring phase information. I think we both agree
that it is safer to integrate only in the original space. So
no matter how we construct the reduced space we do not need
to through away any information.
}

\end{description}


\begin{table}[t]
\caption
{First $11$ fundamental invariants for the standard action
  of SO(2)}
\scriptsize
\[
\begin{array}{ll}
  u_1=r_1=\sqrt{b_1^2+c_1^2}&  \\ u_3=\frac{b_2 \left(b_1^2-c_1^2\right)+2 b_1 c_1 c_2}{r_1^2}&u_4=\frac{-2
b_1 b_2 c_1+\left(b_1^2-c_1^2\right) c_2}{r_1^2}\\ u_5=\frac{b_1 b_3 \left(b_1^2-3 c_1^2\right)-c_1 \left(-3
b_1^2+c_1^2\right) c_3}{r_1^3}&u_6=\frac{-3 b_1^2 b_3 c_1+b_3 c_1^3+b_1^3 c_3-3 b_1 c_1^2 c_3}{r_1^3}\\ u_7=\frac{b_4
\left(b_1^4-6 b_1^2 c_1^2+c_1^4\right)+4 b_1 c_1 \left(b_1^2-c_1^2\right) c_4}{r_1^4}&u_8=\frac{4 b_1
b_4 c_1 \left(-b_1^2+c_1^2\right)+\left(b_1^4-6 b_1^2 c_1^2+c_1^4\right) c_4}{r_1^4}\\ u_9=\frac{b_1
b_5 \left(b_1^4-10 b_1^2 c_1^2+5 c_1^4\right)+c_1 \left(5 b_1^4-10 b_1^2 c_1^2+c_1^4\right) c_5}{r_1^5}&u_{10}=\frac{-b_5
c_1 \left(5 b_1^4-10 b_1^2 c_1^2+c_1^4\right)+b_1 \left(b_1^4-10 b_1^2 c_1^2+5 c_1^4\right) c_5}{r_1^5}\\ u_{11}=\frac{b_6
\left(b_1^6-15 b_1^4 c_1^2+15 b_1^2 c_1^4-c_1^6\right)+2 b_1 c_1 \left(3 b_1^4-10 b_1^2 c_1^2+3 c_1^4\right) c_6}{r_1^6}&u_{12}=\frac{-2
b_1 b_6 c_1 \left(3 b_1^4-10 b_1^2 c_1^2+3 c_1^4\right)+\left(b_1^6-15 b_1^4 c_1^2+15 b_1^2 c_1^4-c_1^6\right) c_6}{r_1^6}\\
\end{array}
\]
\label{tab:SO2n6}
\end{table}

\section{2009-09-04 Talked to Hugues Chat\'{e}}

\noindent{\bf Predrag} Talked to Chat\'{e}, got very
enthused about their results about splitting the KS
eigenvectors into the ``physical," inertial manifold ones,
and the ``hyperbolically isolated," transient ones. We should
reexamine eigenvectors of our solutions, might find the split
already on our very small system sizes (they work at $L=96$).

Here is my email to Chat\'{e}:

Iggy (as Anglos spell it),
%
now that I have become your fan - I just love all those
`unphysical,' lonely, isolated, alienated eigenvectors
banished into their neurotic transitory world off the
inertial manifold -  I'm entering following references to
your papers:
\refrefs{PoGiYaMa06,ginelli-2007-99,YaTaGiChRa08,TaGiCh09}
in the final final edits of to be published in SIADS (part I;
part II is still being penned - 60,000 relative periodic
orbits and no place to go). Let me know if these are the
right ones, add/correct what needs to be edited, corrected.

I have no clue why you guys call linearized stability
eigenvectors ``Lyapunov" (in My Book Lyapunov has to do with
the $J^T J$ symmetric matrix, with only real parts of Floquet
multipliers captured, and eigenvectors that do not seem to
mean anything), but you must have your reasons. We have 10-30
leading eigenvectors for our `narrow cell' with $L=22$, for
each one of the 60,000 invariant solutions that clever but
mindless computation has handed us, and nothing interesting
we had found to do with them, but they should presumably fit
into your bigger L eigenvectors as into a glove. We see
`unphysical' eigenvalues drop like a ton of rocks - only 4
leading eigenvalues really matter, 2 of them marginal at
$L=22$.

PS - for something more "physical," check out
\HREF{http://www.warwick.ac.uk/~masax/}{Dwight Barkley}'s long pipe calculations with
\HREF{http://adsabs.harvard.edu/abs/2008APS..DFD.BD008B}{David Moxley}.
You'll find them inspirational. Probably the same story as KS, but explaining the
phase transition between frozen puffs and turbulent puffs would make plumbers happy.

\noindent{\bf Predrag, to us:} Here are the relevant articles.

\begin{description}
\item[Hyperbolicity and the effective dimension of
           spatially-extended dissipative systems]\rf{YaTaGiChRa08}:
  This paper is a fairly rigorous demonstration of what we
  have observed numerically, that a finite number of Fourier modes
  captures most of the dynamics of KS, and most relevant to our research.
 They show, using covariant Lyapunov vectors, that
  the chaotic solutions of two spatially extended dissipative
  systems, \KS\ and complex Landau-Ginzburg,
  evolve within a manifold spanned by a finite number
  of physical modes hyperbolically isolated from a set of
  residual degrees of freedom, themselves individually
  isolated from each other. The number grows linearly with
  $L$ and is twice as large as the Kaplan-Yorke dimension estimates.
  The results imply that a
  faithful numerical integration of dissipative
  partial differential equations needs to incorporate at
  least all physical modes and that increasing the resolution
  beyond that
  merely increases the number of isolated modes.

\item[Lyapunov analysis and the collective dynamics of
           large chaotic systems]\rf{TaGiCh09}:
  Not as directly relevant to us as \refref{YaTaGiChRa08}.
  They show, using globally-coupled
  (a) limit-cycle oscillators
  (b) noisy logistic maps,
  that the collective dynamics of large chaotic
  systems is encoded in their Lyapunov spectra: most modes
  are typically localized on a few degrees of freedom, but
  some are delocalized, acting collectively on the
  trajectory. For globally-coupled maps, they show moreover a
  quantitative correspondence between the collective modes
  and some of the so-called Perron-Frobenius dynamics.

\item[Characterizing dynamics with covariant Lyapunov
              vectors]\rf{ginelli-2007-99}:
This is the main reference on the
`covariant Lyapunov vectors.' They describe the QR algorithm for
computing Gram-Schmidt vectors (GSV) and recovering
the covariant Lyapunov vectors (CLV) from them.
They show, using covariant Lyapunov vectors, that
  the chaotic solutions of four spatially-coupled maps,
(a) chaotic tent maps,
(b) chaotic symplectic maps,
(c) continuous time rotator model, and
(d) Fermi-Pasta-Ulam chain
  evolve within a manifold spanned by a finite number
  of physical modes.

\item[An efficient method for recovering Lyapunov vectors from
singular vectors]\rf{WoSa07}:
{\bf Predrag 2009-10-15} Wolfe and Samelson seem to be the most important article
to understand. If Ginelli \etal\rf{ginelli-2007-99} say the key thing
(how to compute covariant eigenvectors, actually) they hide it well.
Wolfe and Samelson say: ``
  Standard techniques for computing Lyapunov vectors produce
  results which are norm-dependent and lack invariance under
  the linearized flow. An efficient, norm-independent method
  for constructing the n most rapidly growing Lyapunov
  vectors from n-1 leading forward and n leading backward
  asymptotic singular vectors is proposed. The Lyapunov
  vectors so constructed are invariant under the linearized
  flow in the sense that, once computed at one time, they are
  defined, in principle, for all time through the tangent
  linear propagator. An analogous method allows the
  construction of the n most rapidly decaying Lyapunov
  vectors from n decaying forward and n-1 decaying backward
  singular vectors.
  ''

\item[From synchronization to Lyapunov exponents and
              back]\rf{PoGiYaMa06}:
The early paper in this series.
In the first part they discuss a general approach to determine
Lyapunov exponents from ensemble- rather than time-averages.
The approach passes through the identification of locally
stable and unstable manifolds (the Lyapunov vectors), thereby
revealing an analogy with generalized synchronization. The
method is then applied to a periodically forced chaotic
oscillator. The
analytical calculations are carried out for a model, the
generalized special flow, that they construct as a simplified
version of the periodically forced R\"ossler oscillator.

\item[Structure of characteristic {L}yapunov vectors in
      spatiotemporal chaos]\rf{PaSzLoRo09}:
They study Lyapunov vectors (LVs) corresponding to
the largest Lyapunov exponents in systems with spatiotemporal
chaos. They focus on characteristic LVs and compare the results
with backward LVs obtained via successive Gram-Schmidt
orthonormalizations. Systems of a very different nature such
as coupled-map lattices and the (continuous-time) Lorenz `96
model exhibit the same features in quantitative and
qualitative terms. They propose a minimal
stochastic model that reproduces the results for chaotic
systems.

\item[Periodic orbits, Lyapunov vectors, and singular vectors
      in the Lorenz system]\rf{TrePan98}:
{\bf Predrag 2009-10-15} Trevisan and Pancotti apparently
need to be cited for the observation that covariant vectors
reduce to Floquet eigenvectors in the particular case of a
periodic orbit (seems so obvious it is in ChaosBook without
attribution, and Ruelle and Eckmann\rf{eckerg} surely say
that... Wolfe and Samelson\rf{WoSa07} are inspired by this
article, but seem to go beyond it. Trevisan and Pancotti say
``
A periodic orbit analysis in the Lorenz system
and the study of the properties of the associated tangent
linear equations are performed. A set of vectors are found
that satisfy the Oseledec (1968) theorem and reduce to
Floquet eigenvectors in the particular case of a periodic
orbit. These vectors, called Lyapunov vectors, can be
considered the generalization to aperiodic orbits of the
normal modes of the instability problem and are not
necessarily mutually orthogonal. The relation between
singular vectors and Lyapunov vectors is clarified. The
mechanism responsible for super-Lyapunov growth is shown to
be related to the nonorthogonality of Lyapunov vectors. The
leading Lyapunov vectors, as defined here, as well as the
asymptotic final singular vectors, are tangent to the
attractor, while the leading initial singular vectors, in
general, point away from it. Perturbations that are on the
attractor can be found in the subspace of the leading
Lyapunov vectors.
    ''


\item[Are bred vectors the same as
    {L}yapunov vectors?]\rf{KaCoCa02}:
{\bf Predrag 2009-10-13} Bred vectors might have inspirational
value for us, as the atmospheric people visualize them as
spatiotemporal structures. They say:
    ``Bred vectors (BVs) are, by construction, closely
related to Lyapunov vectors (LVs). In fact, after an
infinitely long breeding time, and with the use of
infinitesimal amplitudes, bred vectors are identical to
leading Lyapunov vectors. In practical applications,
however, bred vectors are different from Lyapunov
vectors in two important ways: a) bred vectors are
never globally orthogonalized and are intrinsically
local in space and time, and b) they are finite
amplitude, finite time vectors. These two differences
are very significant in a very large dynamical system.''

\end{description}

\subsection{2009-09-13 Lyapunov exponents for KS}

% PC 2009-09-12: (b) generated by siminos/figSrc/gnu/lyapSpec.gnu
\begin{figure}
 (c)~\includegraphics[width=0.40\textwidth]{eigenvalues.ps}
 (b)~\includegraphics[width=0.50\textwidth]{lyapSpec}
\caption{
(a)
Lyapunov exponents $\lambda_k$ versus $k$ for the periodic
orbit $\overline{1}$ compared with  the stability eigenvalues
of the $u(x,t)=0$ stationary solution $k^2- \nu k^4$ (from
\refref{Christiansen97}). $\lambda_k$ for $k \geq 8$ fall
below the numerical accuracy of integration and are not
meaningful. Antisymmetric subspace, $N=16$ real Fourier
modes, $L=36.31$. One needs to rescale the time to compare
this to figure (b); -60 in the Lyapunov scale of figure (a)
corresponds to approx. -1.8 in figure (b).
(b)
First 14 Lyapunov exponents $\lambda_j$ for the full
\statesp, periodic b.c. KS for $L=22$, from a 62 real Fourier
modes long-time simulation (from \refref{SCD07}).
}
\label{fig:lyapSpec}
\end{figure}


\begin{description}
\item[Ruslan]
 Here's the expanded list of Lyapunov exponents for KS with $L = 22$:
 0.048,    0.0,    0.0,   -0.003,   -0.189,   -0.256,   -0.290,   -0.310,
-1.963,   -1.967,   -5.605,   -5.605,  -11.923,  -11.923...
So, there appears to be 8 `physically relevant' exponents and
the rest are pairs of hyperbolically separated ones.  I read
the papers about these covariant Lyapunov vectors and I'm not
sure I understand how they are related to eigenvectors at
periodic orbits: are they aligned?  I suspect that not quite,
apart from the most expanding and the most contracting
direction, the rest are sitting somewhere within the
subspaces spanned by the $k$ most expanding, or $m$ most
contracting eigendirections, but not quite aligned with the
eigendirections themselves.  That's probably why they are
more appropriately called `Lyapunov vectors'.

\item[Predrag]
Sure looks persuasive, see \reffig{fig:lyapSpec}. I have also
added stability of a periodic orbit from
\refref{Christiansen97}, for KS on the periodic b.c.,
antisymmetric subspace, system size $\tilde{L} = 5.8$ close
to the onset of chaos, 16 real Fourier modes. As (perhaps?)
discussed in \refref{lanCvit07}, one has to be careful about
defining the effective system size $\tilde{L}$ for the
antisymmetric subspace, so these computations are done on
$L=36.31$ (or $L = 18.155$ if one considers the fundamental
$[0,L/2]$ domain only). Going from $(L,\nu) =
(2\pi,0.029924)$ of \refref{Christiansen97} to $(L,\nu) =
(L,1)$ convention used here requires that the time be
rescaled as $t \to \nu t$, and the Lyapunov exponents as
$\lambda_i \to \lambda_i/ \nu = \lambda_i/ 0.029924 $, so -60
in the Lyapunov scale of \reffig{fig:lyapSpec}\,(a)
corresponds to approx. -1.8 on the scale of
\reffig{fig:lyapSpec}\,(b), which would mean that then we
computed only the first pair of isolated eigenvalues. The
reason is that for periodic orbits we are computing {\em
Floquet multipliers} which underflow numerically very
quickly, so we cannot compute many {\em Floquet exponents}.
These covariant Lyapunov eigenvector methods are apparently
much smarter.

In \refref{lanCvit07} computations are done at $L = 38.5$,
but we listed only 4 eigenvalues per periodic orbit, and
considering hopeless organizational skills on the Lan astral
plane, I doubt that the full spectra can be rescued from
Lan's calculations. And, as explained above, probably we cannot
compute them for periodic orbits.

 \item[Ruslan] I use 32 complex Fourier modes, so the
truncated system has 62 degrees of freedom.  The Lyapunov
exponent calculation is standard (using Gram-Schmidt).  The
exponents are the same, independent of the method of
calculation.  I have not attempted to calculate the covariant
Lyapunov vectors. I'm pretty sure that
orthogonality between physical and isolated eigenvectors
applies, but have not checked.

 \item[Ruslan]
It's OK to I email Hong-liu
Yang\rf{YaTaGiChRa08}, ask him to rerun his covariant
Lyapunov vector spectrum for $L=22$, to see whether he agrees
with us.

 \item[Ruslan]
I have the 62 eigenvalues/eigenvectors for all the RPO/PPO
I've detected, but the highly contracting eigenvalues are
 numerical noise. As Predrag mentioned above,
the straightforward calculation of highly contracting
eigenvalues may suffer from the numerical noise. We need a
better method to compute Floquet multipliers. I have now
checked:   Looks like {Kurt Lust} has proposed one, but I
cannot get his paper on ``Improved Numerical Floquet
Multipliers''\rf{Lust01}. If you could
get it for me, it would be great.

\item[2009-09-13 Ruslan]
``Structure of characteristic Lyapunov
vectors in spatiotemporal chaos''\rf{PaSzLoRo09} states
that characteristic Lyapunov vectors ``reduce to the Floquet
eigenvectors for a periodic orbit'' and references Trevisan
and Pancotti\rf{TrePan98}, which I cannot get electronically.

\item[2009-09-14 Predrag]
I added abstracts of these papers to the reading list above.
The `covariant Lyapunov vectors' are indeed the (right,
non-orthogonal) eigenvectors of the \jacobianM\ \jMps, as
defined in ChaosBook, and coincide with Floquet eigenvectors
for a periodic orbit. For a flow they are defined at a given
\statesp\ point by going back and forward a finite, but
`sufficiently long' time $t$, and coincide with the Floquet
eigenvectors if the point is (relative) periodic. I believed
that for a PDE we cannot go backward, but was wrong; the do
it by using the segment of forward trajectory stored in
memory. The reason why one can get the Floquet {\em exponent}
for arbitrarily long orbit is that the multiplier for eigenvector
evolved in time is just a number, so taking its logarithm over
short time segments and
storing it is trivial, no underflow problems one would get if
one worked with the Floquet {\em multiplier}. So we should be able
to keep track of all 62 eigenvectors, redo it for 126 eigenvectors,
and compare with plots in \refref{YaTaGiChRa08}.
Ginelli \etal\rf{ginelli-2007-99} seems to be the main
reference on the `covariant Lyapunov vectors.' They describe
the QR algorithm for computing Gram-Schmidt vectors (GSV) and
recovering the covariant Lyapunov vectors (CLV) from them.
What confuses me is that so far all papers refer to $\Lyap_j
= \eigRe_j + i \, \eigIm_j$ as purely real, and list only
$\eigRe_j$, but I guess that will be explained somewhere.

\item[2009-09-14 Predrag] You probably do not need to read
\HREF{https://perswww.kuleuven.be/~u0006235/}
     {Kurt Lust}'s paper on ``Improved Numerical Floquet
Multipliers''\rf{Lust01}. But - for future reference - his
     code for these calculations is
\HREF{https://perswww.kuleuven.be/~u0006235/ACADEMIC/r_psSchur.html}
     {available here}, and
\HREF{https://perswww.kuleuven.be/~u0006235/ACADEMIC/r_PDEcont.html}
     {PDEcont library}  for continuation and bifurcation analysis
      of large scale systems might be of interest.
Their ``A numerical and analytical study of Floquet
        multipliers of periodic solutions near
        a homoclinic orbit''\rf{ZhLuRo01}
might also come in handy.
He used to be a part of the Keller, Doedel, Wulff, Krauskopf,
Hinke crowd.

\item[2009-09-14 Predrag]
My initial \reffig{fig:lyapSpec} was not optimal; I have now
reploted it as in Fig.~4 of
Yang \etal\rf{YaTaGiChRa08},
agrees with their 'extensivity' plot for $L=96$ and $192$.

I prefer $x$-axis to be $j/\tildeL = 2 \pi j/L$, as in
\reffig{fig:lyapSpecRscld}.

I do not like the way they count eigenvalues:  due to the $\On{2}$
2-dimensional irreducible representations
one should group their $j,j+1$ pairs,
plot them as a single, two-valued $j$, as in
\reffig{fig:lyapSpecRscld}. What one chooses to pair for low
$j$ might be ambiguous, as the nonlinear interactions mix up
the $\On{2}$ 2-dimensional linearly irreducible representations.
Reploted as in our much ignored 1997 paper\rf{Christiansen97},
eigenvalues fall onto $ (2 \pi j/L)^2 - (2 \pi j/L)^4 $
\eqv\ $\EQV{0}$  stability curve.
The isolated `covariant Lyapunov vectors'
are damped by $-(2 \pi j/L)^4$. I worry that we will not have such
amicable divorce for plane Couette and pipe flows...

% PC 2009-09-14: (b) generated by siminos/figSrc/gnu/lyapSpec.gnu
\begin{figure}
 (a)~\includegraphics[width=0.50\textwidth]{YaTaGiChRa08fig4}
 (b)~\includegraphics[width=0.40\textwidth]{lyapSpecRscld}
\caption{
(a)
Fig.~4 of
Yang \etal\rf{YaTaGiChRa08}:
Extensivity of the Lyapunov spectrum for the KS equation with
periodic b.c.. Inset: number of non-negative exponents (circles),
Kaplan-Yorke dimension (squares), metric entropy (diamonds,
multiplied by $50$), and number of physical modes (triangles).
In perfect agreement with
\reffig{fig:lyapSpec}\,(b) here, plotted the same
(incorrect) way.
(b)
First 14 Lyapunov exponents $\lambda_j$ for the full
\statesp, periodic b.c. KS for $L=22$, from a 62 real Fourier
modes long-time simulation (from \refref{SCD07}).
The same as in part (a) and in \reffig{fig:lyapSpec}\,(b), but
abscisa is $j/\tildeL = 2 \pi j/L$, and each $j$ labels
the $\On{2}$ 2-dimensional irreducible representation
Lyapunov exponents pair.  Full line corresponds to
the stability eigenvalues
of the $u(x,t)=0$ stationary solution
$(j/\tildeL)^2- (j/\tildeL)^4$, for arbitrary system size $L$.
}
\label{fig:lyapSpecRscld}
\end{figure}



We also see
that the `physical' modes are split into a less contracting 1/2
and more contracting 1/2 (different slopes in the Fig 4). We
used to think the first 1/2 is the physically important one,
and say so in the current arXiv version of the article.
We stand corrected.

The Floquet eigenvectors of our (relative) periodic orbits are
what they call 'covariant,' so they should fit into their finite back/forth
time vectors like into a glove, whenever the respective state space
points are sufficiently close.

The main point of exploring the ergodic states space hierarchically
by periodic orbits is to tessellate it systematically, in the most
uniform way, by neighborhoods (linearized stable/unstable manifolds)
of periodic points. A periodic solution computed on a small system
size $L$, periodic b.c. is a solution on any multiple of $L$, and it comes
together with a smooth family of corresponding solutions for nearby
$L$'s.

I see prospects of a long and happy marriage here.

\end{description}

\section{2009-09-16 Lyapunov vectors, illustrated}

\begin{description}
\item[Predrag]
In order to spare you from reading the source literature, I'm including here
those figures from Yang \etal\rf{YaTaGiChRa08} that I find most striking.
The main advance of using Lyapunov vectors instead of eigenvalues alone
is that the approximate orthogonality of the 'isolated' ones provides a clear
threshold between the 'physical' and the rest.

You might be unimpressed by the KS example, as the $-k^4$ hyper-diffusion
term kills all higher Fourier modes very effectively. For that reason the
complex Ginzburg-Landau equation, \reffig{fig:lyapSpecCLG}
is very persuasive; the nonlinearity is
of $u(x)^3$ variety (instead of $u \partial u$ of Navier-Stokes and
KS), but there is only a $-k^2$ diffusive term, and nevertheless there
is a clear threshold for the 'isolated' Lyapunov vectors. Furthermore,
the system exhibits left and right traveling waves, which is more like
fluids than the rigid wave patterns of KS.

\begin{figure}
 (a)~\includegraphics[width=0.45\textwidth]{YaTaGiChRa08fig2}
 (b)~\includegraphics[width=0.45\textwidth]{YaTaGiChRa08fig3}
\caption{
(left panel)
Fig.~2 of Yang \etal\rf{YaTaGiChRa08}:
Properties of covariant Lyapunov vectors for the KS system
($L = 96$, $k_{\rm cut} = 42 \cdot 2\pi/L$, and PBC).
(a) Spatiotemporal plot of a typical vector in the isolated region
($j=46$, total time $100$).
(b) Spatial power spectra of vectors of indices
$j = 1$, $16$, $32$, $38$, $44$, $52$, $60$, $68$, $76$, $84$
(from left to right in peak position).
(c) Top panel: peak wavenumber in the power spectra (red circles)
and $k = [j/2] \cdot 2\pi/L$ (black line).
Bottom panel: DOS violation fraction $\nu^{(j,j+1)}_\tau$ for pairs of
neighboring vectors (pairs within the same step are omitted).
(d) Angle distributions between pairs of vectors.
(e) Same as (d) but with a different abscissa.
(right panel)
Fig.~3 of Yang \etal\rf{YaTaGiChRa08}:
Complex Ginzburg-Landau equation in the amplitude turbulence regime
($L = 64$, $k_{\rm cut} = 31 \cdot 2\pi/L$, and PBC).
(a,b) Spatiotemporal plots of the phase component of a typical vector $j = 91$ in the isolated region
(same trajectory, but at two distant periods of time, during a total time of $20$ for each plot).
(c) Lyapunov spectrum; inset: close-up around threshold.
(d) Time fraction $\nu^{(j,j+1)}_\tau$ of DOS violation, as a function of $\tau$
($j=78$, 82, 86, 90, 94, from top to bottom).
}
\label{fig:lyapSpecCLG}
\end{figure}

\end{description}


\section{2009-09-17  Covariant `transport of vector fields,' a trace formula}

\begin{description}
\item[Predrag] Extracted from \refref{ginelli-2007-99} arXiv source file (might want to check
\refref{PoGiYaMa06} as well):



\item[2009-09-20 Predrag - what's to be done] We presumably have Ruslan's
Floquet multipliers in places like
\\
svn repository vaggelis/production/KS22.0/rpo/ks22rpo016.31\_02.863/Jdiag.dat
\\
(it's hell fishing through the programs, because if there is any description
of what they do and where the data sets are, I have not found them)
but unfortunately they are useless for separating out ``physical'' and
``hyperbolically isolated'' Lyapunov vectors, as they suffer from underflows,
and only a few Floquet multipliers can be computed by methods we have used
so far. We really need to start from scratch and
compute leading 10-20 Floquet {\em exponents}.

Why? If we clear it up for KS,
we have a fighting chance of clearing up the issue for \pCf, where
separating out ``physical'' from
``hyperbolically isolated'' Lyapunov vectors would be a big deal.
Currently Gibson has some 30 eigenvectors for \eqva, but only a handful for
\po s.


\end{description}

\paragraph{
An extract from  ChaosBook.org {appendApplic.tex}, Appendix
{\em Transport of vector fields}
          }
% PC Lyapunov vectors setting up?      	20sep2009
% \section{\EvOper\ for Lyapunov exponents}
% \label{c-vatt-det}
\renewcommand{\ssp}{x}

For higher-dimensional flows only the \jacobianMs\ are
multiplicative, not individual eigenvalues, and the
 evaluation of Lyapunov spectra
requires the extension of evolution equations
to the flow in the tangent space.
The text that follows is clipped from \refref{Vattay}
and the ChaosBook Appendix
\HREF{http://chaosbook.org/chapters/appendApplic.pdf}
     {Transport of vector fields}.
(Probably should also
credit also Lazutkin, I vaguely remember a similar
construction in his work, except that at the moment
I cannot find a reference).
First one determines a periodic orbit, by any scheme.
Once one has it, determine the Lyapunov {\em unit}
vectors by the tangent space eigenvalue
condition, with flow forced by
the (already known) periodic solution. To make it a workable scheme we
need to reformulate this, and instead of computing Floquet
multiplier, evaluate the Floquet exponent along each such
tangent eigen-direction. Shouls be doable, as it is a 1-dimensional
integration. Failing this, we need to learn how other people compute
Lyapunov vectors...

The key idea is to extend the dynamical system by
the tangent space of the flow,
suggested by the standard numerical methods for evaluation of
Lyapunov exponents:      %\rf{BGS80}:
start at $\xInit$ with
an initial infinitesimal tangent space vector in the
$d$-dimensional tangent space
$\eta(0) \in T\pS_{\ssp}$,
and let the flow transport it  along the
trajectory $\ssp(t)= f^t(\xInit)$.

The dynamics in the tangent bundle
$(\ssp,\deltaX) \in  {\bf T}\pS$
is governed by the system of equations of variations:
% \refeq{die}:
%\rf{arnold92}:
	\PC{fix notation: ${\bf(T)}\pS$?}
\[
\dot{\ssp}=\vel(\ssp) \,, \quad
\dot{\eta}=\Mvar(\ssp) \, \eta\, .
%\label{die}
\]
Here $\Mvar(\ssp)$ is % \refeq{DerMatrix},
the {\stabmat} (\velgradmat) of the flow.
We write the solution as
\begin{equation}
x(t)=f^t(\xInit) \,, \quad
 % {\bf \eta}(\xInit,\eta_0,t)
\eta(t)=\jMps^t(\xInit) \, \eta_0 \, ,
\label{xit}
\end{equation}
with the tangent space vector $\eta$ transported by
the {\jacobianM} $\jMps^t(\xInit) = \partial \ssp(t)/ \partial \xInit$.
% \refeq{hOdes}.

% As explained in \refsect{s_lin_stab},
The growth rate of this vector is multiplicative along the trajectory
and can be represented as $\eta(t)=|{\eta}(t)|/|{\eta}(0)| \, u(t)$
where $ u(t)$ is a ``unit" vector in some norm $||.||$.
For asymptotic times and for almost every initial $(\xInit,\eta(0))$,
this factor converges to the leading eigenvalue of the
linearized stability matrix of the flow.

We implement this multiplicative evaluation
of Floquet multipliers by adjoining
the $d$-dimensional transverse tangent space
$\eta \in {\bf T} \pS_x$; $\eta(\ssp) \cdot \vel(\ssp)=0$
to the ($d$+1)-dimensional dynamical evolution
space $x\in \pS \subset \reals^{d+1}$.
	\PC{this looks wrong: $ \vel(\ssp)$ is not normal
            to other eigenvectors. Keep the marginal direction?}
In order to determine the length of the vector $\eta$
we introduce a homogeneous
differentiable scalar function $g(\eta)=||\eta||$.
It has  the property $g(\Lambda \eta)=|\Lambda | \, g(\eta)$
for any $\Lambda $.
An example is the projection of a vector to its $d$th component
\[  %\begin{equation}
 g \left( \begin{array}{c}
	\eta_1 \\
	\eta_2 \\
	\cdots \\
	\eta_d
\end{array} \right)= |\eta_d| \,.
%\label{proj_norm}
\] %\end{equation}

Any vector $\eta(0) \in T\pS_{\ssp}$ can now be represented by the product
$
\eta=\Lambda {u}
$,
where $ {u}$ is a ``unit" vector in the sense that its
norm is $||{u}||=1$, and the factor
\begin{equation}
\Lambda^t(x_0,{u}_0)=g(\eta(t)) = g(\jMps^t(x_0) {u}_0)
\label{lamb_def}
\end{equation}
is the multiplicative ``stretching'' factor.

Unlike the leading eigenvalue of the \jacobianM, the stretching factor is
multiplicative along the trajectory:
\[
\ExpaEig^{t'+t}(x_0,{u}_0)=\ExpaEig^{t'}(x(t),{u}(t))
				\, \ExpaEig^t(x_0,{u}_0).
\]
The ${u}$ evolution constrained to $E {\bf T}_{g,x}$, the space of unit transverse
tangent vectors,
is given by rescaling of (\ref{xit}):
\begin{equation}
%{u}(x_0, {u}_0, t)
{u}'
=R^t(x,{u})=
\frac{1}{\ExpaEig^t(\ssp,{u})} \jMps^t(\ssp) {u} \, .
%{u}(x(t))\in E {\bf T}_{g,x }
\label{R}
\end{equation}

We note  next that if the trajectory $f^t(x)$ is periodic with
period $T$, the tangent space contains $d$ periodic solutions
\[
\jEigvec[i](x(T+t))=\jEigvec[i](x(t)) \,, \quad i=1,...,d ,
\]
corresponding to the $d$ unit eigenvectors $\{\jEigvec[1],
\jEigvec[2], \cdots, \jEigvec[d]\}$ of the transverse stability
matrix, with ``stretching" factors (\ref{lamb_def})  given by
its eigenvalues
\[
\monodromy_p(x) \jEigvec[i](x) = \ExpaEig_{p,i} \, \jEigvec[i](x)
\,, \quad i=1,...,d.
\qquad \mbox{(no summation on $i$)}
\]

\noindent{\bf Predrag} what follows was developed for evaluation
of spectral determinants on the extended, tangent bundle space. Not
sure we need it, but we might... (need stability eigenvalues
to solve Newton equations for eigendirections?)

In order to
compute the stability of the $i$th eigen-direction solution, it
is convenient to expand the variation around the eigenvector
$\jEigvec[i]$ in the stability matrix\ eigenbasis
$
\delta {u} = \sum \delta u_\ell \, \jEigvec[\ell]
\, .
$
The variation of the map (\ref{R}) at a complete period $t=T$
is then given by
\begin{eqnarray}
\delta R^T (\jEigvec[i])  &=&
\frac{\monodromy \delta{u}}{g(\monodromy \jEigvec[i])}
-\frac{\monodromy \jEigvec[i] }{g(\monodromy \jEigvec[i])^2}
\left(
 \frac{\partial g(\jEigvec[i])}{\partial {u}}
	 \monodromy  \delta{u}
\right)
			\nonumber \\
	&=&
\sum_{k \neq i} \frac{\ExpaEig_{p,k} }{ \ExpaEig_{p,i }}
 \left( \jEigvec[k] -\jEigvec[i]
       \frac{\partial g(\jEigvec[i])}{\partial u_k}\right)\delta u_k
\, .
\label{var_R}
\end{eqnarray}
The $\delta u_i$ component does not contribute to this sum
since $g(\jEigvec[i]+du_i\jEigvec[i])=1+du_i$ implies $\partial
g(\jEigvec[i]) / \partial u_i = 1$. Indeed, infinitesimal
variations $\delta{u}$ must satisfy
\[
g({u}+\delta{u})=g({u})=1 \quad \Longrightarrow \quad
\sum_{\ell=1}^d \delta u_\ell
	  \frac{ \partial g({u})}{\partial u_\ell} = 0
\,,
%\label{constra}
\]
so the allowed variations are of form
\[
\delta {u} = \sum_{k\neq i}
		\left( \jEigvec[k] -\jEigvec[i]
              \frac{\partial g(\jEigvec[i])}{\partial u_k}\right)c_k
    \,, \quad |c_k| \ll 1
\, ,
\]
and in the neighborhood of the $\jEigvec[i]$ eigenvector
the $\int d{u}$ integral can be expressed as
\[
\int_g d{u} = \int \prod_{k\neq i} dc_k
\, .
\]
%
%The integral over ${u}$ is restricted to
%$ET_{g,x}$, the space of unit tangent vectors
%transverse to the ($d$+1)-dimensional flow,
%is ($d$-1)-dimensional. For example, if the norm is such that
%the Euclidean length of $ {u}$ is 1,
%this space is a ($d$-1)-dimensional unit hemisphere,
%and if the norm is given by (\ref{proj_norm}), $ET_{g,x}$
%is the ($d$-1)-dimensional hyperplane fixed by condition $\eta_d=1$.
Inserting these variations into the $\int d{u}$ integral we obtain
\begin{eqnarray}
\int_g d{u}\, &&
\prpgtr{ \jEigvec[i]+\delta{u} -R^{T}(\jEigvec[i])
				-\delta R^T(\jEigvec[i]) + \dots}
	\nonumber \\
= \, && \int \prod_{k\neq i} dc_k \,
	\prpgtr{(1-\ExpaEig_{k}/\ExpaEig_{i}) c_k + \dots }
	\nonumber \\
	&&= \prod_{k\neq i} \frac{ 1}{ \left| 1-
			{\ExpaEig_{k} / \ExpaEig_{i } }\right|}
\, .
\nonumber
\end{eqnarray}

\section{2009-10-10  Covariant Lyapunov vectors, an algorithm}

\begin{description}
\item[2009-10-10 Sara and Predrag] Understood
\refref{ginelli-2007-99} (might want to
check \refref{PoGiYaMa06} as well).
\end{description}

\paragraph{
A general method to
determine covariant Lyapunov vectors in both discrete- and
continuous-time dynamical systems.
            }

The widely used procedure to calculate the Lyapunov
exponents\rf{bene80a} relies on construction of orthogonal
sets of Gram-Schmidt vectors. They  are not covariant,
\ie, the Gram-Schmidt vectors at a given \statesp\ point are not mapped by the
linearized dynamics into the Gram-Schmidt vectors of the forward images of
this point. They are orthogonal by construction, in contrast
to the \jacobianM\ $\jMps_n$ eigenvectors \jEigvec[i] which
generically span the $d$-dimensional tangent space, but are
generically not normal. In numerical work, frequent
Gram-Schmidt re-orthogonalization of the tangent coordinate
frame is necessitated by the exponential growth-rate
separation along $\jMps_n$ eigendirections as $n$ increases;
for a long periodic orbit $p$ direct computation of
eigenvalues of $\jMps_p$ yields only a set of exponentially
separated leading Floquet multipliers
$\{\ExpaEig_1,\ExpaEig_2,\cdots\}$ that can resolved within a
given machine precision.

The method of \refref{ginelli-2007-99} enables computation of
{\em all} eigenvectors of $\jMps_n$. It is based on two key ides,
previously often underappreciated:
\begin{enumerate}
  \item
Due to the upper-triangular structure a Gram-Schmidt
  re-orthogonalization matrix ${\bf R}_n$, a vector that starts
  within the subspace $\pS^{[j]}$ spanned by the first $j$ Gram-Schmidt
  basis vectors, {\em stays} in the subspace.
  \item
Once a set of $\{{\bf R}_1,{\bf R}_2,\cdots,{\bf R}_n\}$ along
trajectory $\{\ssp_1,\ssp_2,\cdots,\ssp_n\}$ has been computed and
stored in the memory,
it can be used to describe the linearized flow,
\ie, the action of the \jacobianM\ $\jMps_n$ both
\emph{forward} and \emph{backward} in time.

So far, this is true of any QR decomposition. Now to
hyperbolic dynamics; order the \jacobianM\ $\jMps_n$
eigenvectors \jEigvec[\ell] by the real parts of their
eigen-exponents, and let $\pS^{[j]}$ be the subspace spanned
by the leading $j$ eigenvectors.

Strongly contracting $\ExpaEig^{(j)}$ multiplier forward in time,
becomes the leading $1/\ExpaEig^{(j)}$ multiplier backward in time.
Matrix power method then pulls out this eigenvalue as the leading one
within the subspace $\pS^{[j]}$. \emph{Presto:} Increase the dimension of the
subspace by one, and you get the next $\ExpaEig^{(j+1)}$. Repeat, and
you get all eigenvalues and eigenvectors, even those insanely contacting ones,
like $\ExpaEig^{(j)} \approx 10^{-137}$.

\end{enumerate}
\emph{Bonus points:}
\begin{enumerate}
  \item
For periodic orbits we need to evaluate ${\bf R}_n$
for only one traversal of the cycle: the method should then yield
{\em all} eigenvectors and Floquet exponents (no need to mess with
convergence of `Lyapunov' eigenvectors). As unstable \po s fill
the ergodic component of the \statesp\ hierarchically, their linearized
stable / unstable manifolds tessellate the asymptotic, `Lyapunov'
stable / unstable foliation with exponentially increasing accuracy.
No need to do mindless numerics, starting with a random initial point.

  \item
\emph{Unexpected bonus:}
Near-orthogonality of these eigenvectors appears to separate the women from
girls. Gives us apparently a sharp way to count the number of degrees of
freedom for a PDE solved on a compact domain.
\end{enumerate}

A coordinate-independent, local decomposition of \statesp\
into covariant Lyapunov directions is called Oseledec
splitting\rf{ruelle79,eckerg}.
Its numerical implementation in \refref{ginelli-2007-99}
is based on forward/backward
iterations of the tangent dynamics and determination
a set of dynamics covariant directions,
called `covariant Lyapunov vectors (CLV).'

\begin{description}
\RLDedit{
  \item[Ruslan 2009-10-14] I'm sorry to ruin your
optimism here, but I don't think this procedure is
numerically stable: The round-off errors will spill outside
$\pS^{[j]}$ and then $1/\ExpaEig^{(k)}$ multipliers for $k >
j$ will stretch these errors.
}

\PCedit{
\item[Predrag 2009-10-15]
You are right that as it stands, we have not explained the
algorithm. The answer seems to be in Wolfe and
Samelson\rf{WoSa07} (above and in ChaosBook.org/library, link
below), perhaps the most important article to understand.
If Ginelli \etal\rf{ginelli-2007-99} say the key thing (how
to compute covariant eigenvectors, actually) they hide it
well.
    }

\RLDedit{
  \item[Ruslan 2009-10-14:]
Another thing that you have noticed is that they don't say
anything about complex eigenvalues.  This is because in the
construction of the algorithm they assume that all Lyapunov
exponents are distinct, which implies that all Floquet
multipliers of periodic orbits are real and distinct as well
(Even though they write $\lambda_1 \geq \lambda_2 \cdots$
instead of $>$, this is mentioned in one of the papers you
discussed above, but I don't remember which one).  I just
hope that, if we do have a pair of complex conjugate
eigenvalues, the corresponding two Lyapunov vectors will span
the same subspace as the real and imaginary parts of the
corresponding eigenvector.  But this needs to be verified.
}

\PCedit{
\item[Predrag 2009-10-15]
You are right that Wolfe and Samelson\rf{WoSa07} cannot prove
that their method converges for degnerate sets of eigenvalues
(complex pairs being a particular case - real parts are
equal, and the method maps orhtogonal frame to orthogonal
frame, codes stretching/shrinking into $R_{ij}$ matrix,
ignores frame rotations. Ginelli \etal\rf{ginelli-2007-99}
deal with complex eigenvalues (see the plots I copied to the
blog above), so I guess it works. I might ask Takeuchi how
they really do it, Chat\'e says he is smart.
    }


\RLDedit{
  \item[Ruslan 2009-10-14]
 This may be the best
algorithm we have for determining Lyapunov vectors along a
typical chaotic orbit, but if we want to determine them for a
PO or a RPO, then I think that the best way to do this is to
find out how to solve the eigen-problem for a very badly
conditioned matrix $J$ which can be represented as a product
$J = J_{n}J_{n-1}\cdots J_1$ of not-so-badly conditioned
matrices $J_j$.  You seem to have something along these lines
in the extract 'Transport of vector fields' above, but I
would prefer to see this construction done for maps, since
\emph{computers don't do flows}.
}

\PCedit{
\item[Predrag 2009-10-15]
It's usually easier for maps than flows, and as this is all formulated for finite times,
I think it works for discrete and continuous finite times equally well. Computers
do flows pretty well, that's how we compute \FloquetM\ \monodromy.
Wolfe and Samelson\rf{WoSa07} did not do it for Floquet eigenvectors.
Trevisan and Pancotti (see above, and \wwwcb{/library} apparently
need to be cited for the observation that covariant vectors
reduce to Floquet eigenvectors in the particular case of a
periodic orbit (seems so obvious it is in ChaosBook without
attribution, and Ruelle and Eckmann\rf{eckerg} surely say
that.
    }
\end{description}


\subsection{QR decomposition}

\begin{description}
\item[2009-10-10 Sara and Predrag] Understood
\refref{ginelli-2007-99} (might want to
check \refref{PoGiYaMa06} as well).
\end{description}

Consider a $d$-dimensional discrete-time dynamical system.
Let $\ssp_{n}\in \pS \subset \reals^d$ denote the \statesp\
point at time $t_{n}$ and let $\{{\bf g}_{n}^{(\ell)}\}$, $\ell=1,
\ldots d$, be a set of $d$ orthogonal vectors,
${\bf g}_{n}^{(\ell)} \cdot {\bf g}_{n}^{(k)} = \delta_{\ell k}$; these vectors
span the tangent space of the flow at time $t_{n}$.
Collect these into the $n$th \emph{GS basis} written as the
orthogonal matrix
\[
{\bf G}_n = ({\bf g}_n^{(1)} | \ldots |  {\bf g}_n^{(d)})
\]
whose columns are the basis vectors $\{{\bf
g}_{n}^{(\ell)}\}$. Start now at $t_{n-1}$. Iterating the
evolution equations once maps the ${\bf G}_{n-1}$ orthogonal
basis into a set of non-orthogonal, non-unit basis vectors
$\overline{\bf G}_n = (\overline {\bf g}_n^{(1)} | \ldots |
\overline {\bf g}_n^{(d)})$,
\[
\overline {\bf g}_n^{(\ell)} =\jMps_{n-1}
            \, {\bf g}_{n-1}^{(\ell)}
\,,
\]
where $\jMps_{n-1}$ is the \jacobianM\ of the
transformation evaluated at time $t_{n-1}$.
The $n$th GS basis is now obtained by applying the
Gram-Schmidt orthogonalization to the vectors
$\overline {\bf g}_n^{(\ell)}$. Pick
$\overline{\bf g}_n^{(1)}$ as the direction of the first
Gram-Schmidt vector,
${\bf g}_n^{(1)} = \overline{\bf g}_n^{(1)}/|\overline{\bf g}_n^{(1)}|$.
Pick the component of
$\overline{\bf g}_n^{(2)}$ normal to ${\bf g}_n^{(1)}$
as the direction of the second
Gram-Schmidt vector, normalize to the unit eigenvector,
\bea
{\bf g}_n^{(2)} &=& \frac{1}{|\cdots|}
            \left(\overline{\bf g}_n^{(2)}
                  - (\overline{\bf g}_n^{(2)} \cdot {\bf g}_n^{(1)})
                      \, {\bf g}_n^{(1)}
            \right) \,,
\continue
{\bf g}_n^{(3)} &=& \frac{1}{|\cdots|}
            \left(\overline{\bf g}_n^{(3)}
                  - (\overline{\bf g}_n^{(3)} \cdot {\bf g}_n^{(2)})
                      \, {\bf g}_n^{(2)}
                  - (\overline{\bf g}_n^{(3)} \cdot {\bf g}_n^{(1)})
                      \, {\bf g}_n^{(1)}
            \right) \,,
\nnu
\eea
and so on.
Thus the Gram-Schmidt orthogonalization  amounts to the $\overline{\bf G} =
{\bf Q}{\bf R}$ decomposition, where
\[
\overline{\bf G}_n =
 {\bf G}_n ({\bf G}_n^T \overline{\bf G}_n) =
 {\bf Q}_n {\bf R}_n \,.
\]
 The columns of the orthogonal matrix ${\bf Q}_n = {\bf G}_n$ are
 the elements of the $n$th GS basis,
while ${\bf
R}_n= {\bf G}_n^T \overline{\bf G}_n$ is an upper-triangular matrix
whose
nonzero elements are obtained by projecting each vector
$\overline {\bf g}_n^{(j)}$ onto the subspace spanned by
$\{ {\bf g}_n^{(k)}\}$ with $k \leq j$.
        \PC{the reminder of this entry is clippings from \refref{PoGiYaMa06},
        still needs to be edited; there is more QR algebra to fill in. Apologies.
        }

Ershov and Potapov
have shown\rf{ErshPot98} that, by repeating the above
procedure up to a time $t_m$ for $m$ much larger than $n$,
the GS basis converges to an orthogonal set of vectors
$\{{\bf e}_m^{(k)}\}$, $k=1,\ldots, d$ - the $m$th Gram-Schmidt
vectors - which solely depend on the \statesp\ point
$\ssp_{m}$.
    \PC{For \po s, we need no Ershov and
    Potapov\rf{ErshPot98}. Oseledec arguments are only
    needed for a random long-time ergodic trajectory}

The Lyapunov exponents $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_d$
are then nothing but the time-averaged values of the
logarithms of the diagonal elements of ${\bf R}_n$.
        \PC{are they implying that diagonal elements of ${\bf R}_n$
            are the eigenvalues of the \jacobianM\ $\jMps_n$?}
The method of \refref{PoGiYaMa06} also utilizes the
information contained in the off-diagonal elements of ${\bf R}_n$ .

Assume that a set of Gram-Schmidt vectors at $\ssp_m$ has
been generated by iterating the generic initial condition
$\ssp_0$. Let ${\bf u}_m^{[j]}$ be a generic vector inside
the subspace $\pS^{[j]}$ spanned by $\{{\bf e}_m^{(k)}\}$,
$k=1,\ldots, j$, \ie, by the first $j$ Gram-Schmidt vectors
at time $t_m$. This vector can be iterated backward in time
by inverting the upper-triangular matrix ${\bf R}_m$: if the
$c_m^{i}=({\bf e}_m^{(i)} \cdot {\bf u}_m^{[j]} )$ are the
coefficients expressing ${\bf u}_m^{[j]}$ in terms of the
Gram-Schmidt vectors at $\ssp_m$, then $c_{m-1}^{i}= \sum_k
[{\bf R}_m^{-1}]_{ik} c_m^{k}$. Since ${\bf R}_m$ is
upper-triangular, it is easy to verify that ${\bf u}_n^{[j]}
\in \pS^{[j]}_n$ at all times $t_n$.


Iterating ${\bf u}_m^{[j]}$ backward for a sufficiently large
number $(m-n)$ of times, it eventually aligns with the
(backward) most expanding direction within $\pS^{[j]}_n$.
This defines  ${\bf v}_n^{(j)}$, our intrinsic $j$-th
(forward) expanding direction at the \statesp\ point
$\ssp_n$.

In order to verify that ${\bf v}_n^{(j)}$ is covariant,
define the matrix $[{\bf C}_m]_{ij} = c_m^{ij}$,
\[
{\bf C}_m = {\bf R}_m {\bf C}_{m-1}
\,.
\]
By multiplying both sides by ${\bf Q}_m$ and substituting
$\overline {\bf G}_m$ for its QR decomposition on the
resulting right hand side, one is left with ${\bf
v}_{m}^{(j)}=\jMps_{m-1} {\bf v}_{m-1}^{(j)}$ for
$j=1,\ldots, d$.

The covariant Lyapunov vectors are independent of where the
backward evolution is started along a given trajectory,
provided that it is sufficiently far in the future.
    \PC{For \po s, we need no Ershov and
    Potapov\rf{ErshPot98} ergodic trajectory arguments
    }

Covariant Lyapunov vectors $\{{\bf v}_{m}^k\}$ constitute an
intrinsic, covariant basis defining expanding/contracting
directions in \statesp.
In the presence of degeneracies covariant Lyapunov vectors
have to be grouped according to the multiplicity of the
corresponding Lyapunov exponent.

The Lyapunov exponents are simply obtained from the covariant
Lyapunov vectors: the $i$th exponent is the average of the
growth rate of the $i$th vector. For 2\dmn\ maps and 3\dmn\
flows this has been shown by
B. Eckhardt, D. Yao, Physica D {\bf 65}, 100 (1993);
G. Froyland, K. Judd and A. I. Mees, Phys. Rev. E {\bf 51}, 2844 (1995);
and
A. Politi \etal\rf{PoGiYaMa06}.

Evidence of the validity of the approach was given in
\refref{PoToLe98}, where covariant Lyapunov vectors were
introduced to characterize time periodic orbits in a 1D
lattice of coupled maps. There, it was found that the number
of nodes (changes of sign) in a covariant Lyapunov vectors is
directly connected to the position of the corresponding
Lyapunov exponent within the Lyapunov spectrum.


Note that the covariant Lyapunov vectors are also well
defined for non-invertible dynamics, since it is necessary
and sufficient to follow backward a trajectory previously
generated forward in time. The determination of the covariant
Lyapunov vectors is very efficient. The main computational
bottleneck is the memory required to store the matrices ${\bf
R_n}$ and the $n$-time Gram-Schmidt vectors during the
forward integration. The memory required can be substantially
reduced by occasionally storing the instantaneous
configuration in real and tangent space and re-generating the
rest when needed.

\subsection{Separate the women from girls: Hyperbolicity}
\begin{description}
\item[2009-10-10 Predrag]
\end{description}

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\SFIG{gin99angle}
{}{
(a) Probability distribution of the angle between stable and
unstable manifold for the H\'enon map $x_{n+1} = 1 -1.4\,
x_n^2 + 0.3 x_{n-1}$, and the Lozi map $x_{n+1} = 1
-1.4\,|x_n| + 0.3 x_{n-1}$ (black line, rescaled by a factor
10). (b) Ignore this frame.
}{fig:gin99angle} %{Hyp}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
A dynamical system is said to be \emph{hyperbolic} if its
\statesp\ has no homoclinic tangencies, \ie, the stable and
unstable manifolds are everywhere transversal to each other.
Since covariant Lyapunov vectors correspond to the local
expanding/contracting directions, one can compute their
relative transversality and quantify the degree of hyperbolicity.
The knowledge of the covariant Lyapunov vectors allows testing hyperbolicity by
determining the angle between each pair $(j,k)$ of expanding
($j$) and contracting ($k$) directions
\[
\phi_n^{j,k} = \cos^{-1}(|{\bf v}_n^{(j)} \cdot{\bf v}_n^k|) \in [0,\pi/2]
\,.
\]
As a test, Ginelli \etal\rf{ginelli-2007-99} compute the
probability distribution $P(\phi)$ of $\phi_n^{1,2}$ for the
H\'enon and Lozi two-dimensional maps. Arbitrarily small
angles are found for the H\'enon map, while the distribution
is bounded away from zero in the Lozi map \reffig{fig:gin99angle},
consistent with the common belief that
only the Lozi map is hyperbolic \rf{CoLe84}.

\begin{description}
\item[2009-10-15 Predrag]
Added references \refref{WoSa07,KaCoCa02,TrePan98,PaSzLoRo09}
(discussed above) to
\\
\wwwcb{/library}. As far as I can tell, the important one is
\refref{WoSa07}.

\item[2009-10-15 Predrag to Ruslan]
I have lost interest in quotienting $\SOn{2}$? Nothing could
be further from the actual state of affairs. Ashley Willis
has made a slice work for a down-stream traveling wave in
pipe (downstream we have pure $\SOn{2}$, and for
angular-traveling waves we have $\On{2}$), and we might soon have a
neighborhood spiral-out. His plots of $\dot(\theta)$ already
show hiccups with velocity going large every so often.

But computing Lyapunov vectors seems important as well. I've
put all the papers you did not yet read onto the
\wwwcb{/library}.

\item[2009-10-15 Ruslan]
I believe slices will work for traveling waves and other
types of relative steady (or nearly steady) states, but not
for generic \rpo s and other 'moving' states.  There we will
probably have to learn how to live with a discontinuous
$\theta(t)$.
\end{description}




\renewcommand{\ssp}{a}
