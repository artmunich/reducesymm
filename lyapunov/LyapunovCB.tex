        \ifdasbuch
        \else
        \fi

        \ifdasbuch
\Chapter{Lyapunov}{20mar2013}{Lyapunov exponents}
\listofsections{0}
% $Author$ $Date$
        \else
\chapter{Lyapunov exponents}
\label{c-Lyapunov}
% merge into ChaosBook \section{Appendix: Implementing evolution}
        \fi


% Predrag created Lyapunov.tex          18mar2013
% Predrag added stretches               11mar2013
% Predrag singular value decomposition  27jun2011
% Predrag                               30jan2008
% Predrag making Grigo happy            30aug2007
%       Predrag          14/3-95

\index{Lyapunov!exponent|(}

        \ifdasbuch
        \else
\begin{description}

\item[2013-03-21 PC]
I have copied this chapter from ChaosBook.org to here in order to get advice
from Kazamusa on how Qi Ge and I should incorporate essential write up
of the Lyapunov vector computations; it is already formatted so it
can be simply moved from here to the ChaosBook.

\end{description}
        \fi


\noindent
        \ifdasbuch
\lettrine[lines=3,lraise=0.1,lhang=0.2]{L}{et us apply}
        \else
Let us apply
        \fi
the newly acquired tools to the fundamental diagnostics in
this subject: Is a given system `chaotic'? And if so, how chaotic?
                                        \toExam{exmp:FlowStrange}
If all points in a neighborhood of a trajectory converge toward the same
trajectory, the attractor is a fixed point or a limit cycle.
                                        \toSect{s-What-is-chaos}
However, if the attractor is strange, any two trajectories
\( %beq
\ssp(t)=\flow{t}{\xInit} \; {\rm and } \;
\ssp(t)+\deltaX(t) =\flow{t}{\xInit+\deltaX_0}
\) %ee{trajs}
                                        \toRem{r:LyapExps}
that start out very close to each other separate exponentially with time,
and in a finite time their separation attains the size of the accessible
{\statesp}.

This {\em sensitivity to initial conditions} can be quantified as
\beq
|\deltaX(t)| \approx e^{\Lyap t} |\deltaX_0|
\ee{traj-sep}
\noindent
where $\Lyap$, the mean rate of separation of trajectories of the
system, is called the leading {\em Lyapunov exponent}. As it so often
goes with easy ideas, it turns out that Lyapunov exponent are not
natural for study of dynamics, and we would have passed them over in
silence, were it not for so much literature that talks about them. So
in a textbook we are duty bound to explain what all the excitement is
about.
\index{attractor!strange}\index{strange attractor}
\index{limit cycle}\index{cycle!limit}
\index{sensitivity to initial conditions}
    \PC{2013-0320 remember to correct
    \HREF{http://en.wikipedia.org/wiki/Lyapunov_exponent}
    {en.wikipedia.org/wiki/Lyapunov\_exponent}
    }

\section{Stretch and twirl}
%\subsection{Singular value decomposition}
\begin{bartlett}
\bauthor{
\HREF{http://prairiehome.publicradio.org/programs/20031129/scripts/guy_noir.shtml}
    {Governor Arnold Schwarzenegger}
        }
Diagonalizing the matrix: that's the key to the whole thing.
\end{bartlett}

\noindent
\cyclist
In general the \jacobianM\ $\jMps^\zeit$ is neither diagonal, nor
diagonalizable, nor constant along the trajectory. What is a
geometrical meaning of the mapping of a neighborhood by $\jMps$? Here
the continuum mechanics insights are helpful, in particular the polar
decomposition which affords a visualization of the linearization of a
flow as a mapping of the initial ball into an
ellipsoid (\reffig{f:jacobMat}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\SFIG{jacobian}{}{
The linearized flow maps a swarm of initial points in an
infinitesimal spherical neighborhood of squared radius $\delta\ssp^2$
at $\xInit$ into an ellipsoid
$\delta\transp{\ssp}(\transp{\jMps}\!\jMps)\,\delta\ssp$ at
$\ssp(\zeit)$ finite time $\zeit$ later,  rotated and
stretched/compressed along the principal axes by \PCedit{stretches}
$\{\sigma_{j}\}$ .
}{f:jacobMat}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

First, a few definitions: A symmetric $[d\!\times\!d]$ matrix
$\covMat$ is \emph{positive definite}, $\covMat > 0$, if
$\transp{\ssp}\covMat \ssp > 0$ for any nonzero vector $\ssp \in
\reals^d$. $\covMat$ is \emph{negative definite},  $\covMat < 0$, if
$\transp{\ssp}\covMat \ssp < 0$ for any nonzero vector $\ssp$.
Alternatively, $\covMat$ is a positive (negative) definite matrix if
all its eigenvalues are positive (negative). A matrix ${R}$ is
orthogonal if $\transp{{R}} {R} = \matId$, and proper orthogonal if
$\det {R} = +1$. Here the superscript $\transp{{}}$ denotes the
transpose. For example, $(\ssp_1,\cdots,\ssp_d)$ is a {row} vector,
$\transp{(\ssp_1,\cdots,\ssp_d)}$ is a {column} vector.
\index{matrix!positive definite}\index{positive definite matrix}
\index{matrix!negative definite}\index{negative definite matrix}

                                                    \toRem{r:SVD}
By the {polar decomposition theorem}, a deformation $\jMps$ can be
factored into a {rotation} ${R}$ and a right~/~left {stretch}
tensor ${U}$~/~${V}$,
\beq
\jMps={R}{U}= {V}{R}
\,,
\ee{PolarFact}
where ${R}$ is a proper-orthogonal matrix and ${U}$, ${V}$ are
symmetric positive definite matrices with strictly positive real
eigenvalues $\{\sigma_1,\sigma_2,\cdots,\sigma_d\}$ called
\emph{principal stretches} (singular values), and with orthonormal
eigenvector bases,
\index{singular values}
\bea
{U}{\bf u}^{(i)} &=& \sigma_i{\bf u}^{(i)}
    \,,\qquad \{{\bf u}^{(1)},{\bf u}^{(2)},\cdots,{\bf u}^{(d)}\}
\continue
{V}{\bf v}^{(i)} &=& \sigma_i{\bf v}^{(i)}
    \,,\qquad \{{\bf v}^{(1)},{\bf v}^{(2)},\cdots,{\bf v}^{(d)}\}
\,.
\label{e:stretches}
\eea
$\sigma_i > 1$ for stretching and $0<\sigma_i<1$ for compression
along the direction ${\bf u}^{(i)}$ or ${\bf v}^{(i)}$. $\{{\bf
u}^{(j)}\}$ are the \emph{principal axes of strain} at the initial
point $\xInit$; $\{{\bf v}^{(j)}\}$  are the principal axes of strain
at the present placement $\ssp$. From a geometric point of view,
$\jMps$ maps the unit sphere into an ellipsoid, \reffig{f:jacobMat},
the singular values are then the lengths of the semiaxes of this
ellipsoid. The rotation matrix ${R}$ carries the initial axes of
strain into the present ones,
\(
{V}= {R}{U}\transp{R}
\,.
\)
The eigenvalues of the
\PC{
In spirit of \refsect{p:spaghetti}: Lagrangian=co-moving,
Eulerian=external `reference' coordinate frame:
   ``the columns of the matrix  ${V}$ are the principal axes
${e}_i$ of stretching in the Lagrangian coordinate frame, and the
orthogonal matrix ${R}$ gives the orientation of the ellipse in the
Eulerian coordinates.
%\index{Lagrangian!coordinates}
%\index{Eulerian coordinates}
   }
\bea
\mbox{right Cauchy-Green strain tensor:} && \transp{\jMps}\!\jMps=U^2
    \continue
\mbox{ left Cauchy-Green strain tensor:} && \jMps\,\transp{\jMps}=V^2
\label{Cauchy-Green}
\eea
are $\{\sigma_j^2\}$, the squares of principal stretches.
    \index{polar decomposition}\index{stretches}
    \index{principal!stretches}\index{principal!directions}
    \index{Cauchy-Green strain tensor}

Stretches $\{\sigma_{j}\}$ are \emph{not related} to the the
\jacobianM\ $\jMps^\zeit$ eigenvalues $\{\ExpaEig_{j}\}$ in any
simple way: the eigenvectors $\{{\bf u}^{(j)}\}$  of strain tensor
$\transp{\jMps}\!\jMps$ that determine the orientation of the
principal axes, are distinct from the \jacobianM\ $\jMps$
eigenvectors $\{\jEigvec[j]\}$, and strain tensor
$\transp{\jMps}\!\jMps$ satisfies no semigroup property such as
\refeq{Jmultiplic}. To emphasize this distinction, the \jacobianM\
eigenvectors $\{\jEigvec[j]\}$ are sometimes called `covariant' or
`covariant Lyapunov vectors'. Under time evolution the covariant
vectors map forward as $\jEigvec[j] \to \jMps\jEigvec[j]$ (transport
of the velocity vector \refeq{JacobVeloc} is an example), In
contrast, the principal axes have to be recomputed from the scratch
for each time $\zeit$.
    \index{covariant Lyapunov vector}\index{Lyapunov!covariant vector}
    \index{semigroup!dynamical}\index{singular values}
    \PC{fix this: \reffig{f:covariant1} is for $\jMps$,
        \reffig{f:jacobMat} is for $\transp{\jMps}\!\jMps$}
    \PC{Make Trevisan\rf{TrePan98} 2D example an exercise (with
        given answers). Discuss transient growth.}
    \PC{replace the tensors by the invariants - then the
    relations between $\{\ExpaEig_{j}\}$ and $\{\sigma_{j}\}$ should
    be immediate?}

Eigenvectors / eigenvalues are suited to study of iterated forms of a
matrix, such as \jacobianM\ $\jMps^\zeit$ or exponential
$\exp(\zeit\Mvar)$, and are thus a natural tool for study of
dynamics. Principal vectors are not, they are suited to study of the
matrix $\jMps^\zeit$ itself. The polar (singular value)
decomposition is convenient for numerical work (any matrix, square or
rectangular, can be brought to such form), as a way of estimating the
effective rank of matrix $\jMps$ by separating the large, significant
singular values from the small, negligible singular values.

\fastTrackExam{exmp:ns-sing}

\section{Lyapunov exponents}
\label{s-LyapExps}
% must correct http://en.wikipedia.org/wiki/Lyapunov_exponent
%   Predrag created \Chapter{Lyapunov}      18mar2013
%   Predrag moved from \Chapter{stability}  21sep2001
% Dorte                                     24sep2001
% Predrag                                   18sep2001
% Predrag:  moved from getused.tex          27sep2001
% PC moved from \Chapter{applic}{            9aug2000
% GV                                         7jul2000
% PC                                        23jan2000
% mal\_conv   Non-linearity version          7feb1990
% Joachim Mathiesen                         26jan2000
%   excerpted from 'A Study of The Rössler Attractor'
%   Term paper for Fysik711B fall semester 1999

\authorJMPC

The mean growth rate of the distance $|\deltaX(t)|/|\deltaX_0|$
between neighboring trajectories \refeq{traj-sep} is given by the The
leading {\em Lyapunov exponent} can be estimated for long (but not
too long) time $t$ as
\beq
\Lyap \simeq %\lim_{t\rightarrow\infty}
    \frac{1}{t} %\lim_{\delta x\rightarrow\infty}
    \ln{|\deltaX(t)| / |\deltaX_0|}
%\,.
\ee{lyax}
(For notational brevity we shall often suppress the dependence of
quantities such as $\Lyap =\Lyap(\xInit)$, $\deltaX(t) =
\deltaX(\xInit,t)$ on the initial point $\xInit$). One can take
\refeq{lyax} as is, take a small initial separation $\deltaX_0$,
track the distance between two nearby trajectories until
$|\deltaX(t_1)|$ gets significantly bigger, then record $ t_1 \Lyap_1
=\ln (|\deltaX(t_1)|/|\deltaX_0|)$, rescale $\deltaX(t_1)$ by factor
$|\deltaX_0|/|\deltaX(t_1)|$, and continue add infinitum, as in
\reffig{f:lyapCalc}, with the leading Lyapunov exponent given by
\beq
\Lyap =\lim_{t\rightarrow\infty}\frac{1}{t}
    \sum_i t_i \Lyap_i
\,.
\ee{LyapSepar}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\SFIG{lyapCalc}
{}{
A long-time numerical calculation of the leading Lyapunov exponent
requires rescaling the distance in order to keep the
nearby trajectory separation within the linearized flow range.
}{f:lyapCalc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Deciding what is a safe 'linear range' before the separation
vector $\deltaX(\zeit)$ should be rescaled is a dark art.

We can start out with a small $\deltaX$ and try to estimate the
leading Lyapunov exponent $\Lyap$ from \refeq{LyapSepar}, but now
that we have quantified the notion of linear stability in
\refchap{c-stability}, we can do better. The problem with measuring
the growth rate of the distance between two points is that as the
points separate, the measurement is less and less a local
measurement. In the study of experimental time series this might be
the only option, but if we have equations of motion, a better way is
to measure the growth rate of vectors transverse to a given orbit.

Given the equations of motion, for
infinitesimal $\deltaX$ we know the $\deltaX_i(t)/\deltaX_j(0)$ ratio
exactly,
as this is by definition the \jacobianM\
\[
\lim_{\deltaX(0) \to 0} {\deltaX_i(t) \over \deltaX_j(0)}
        = {\pde \ssp_i(t) \over \pde \ssp_j(0)}
        = \jMps^\zeit_{ij}(\xInit)
\,,
\]
so the leading {Lyapunov exponent} can be computed from the
linearization \refeq{xit_1}
\beq
{\Lyap(\xInit)}
 = \lim_{t\rightarrow\infty}\frac{1}{t}\ln
        { \left| \jMps^\zeit(\xInit)\deltaX_0\right| \over |\deltaX_0|}
 = \lim_{t\rightarrow\infty}\frac{1}{2t}\ln\left(
        \transp{\unitVec}
        \transp{\jMps^\zeit}\!\jMps^\zeit {\unitVec}\right)
\,.
\ee{ddd}
In this formula the scale of the initial separation drops out, only
its orientation given by the initial orientation unit vector
${\unitVec} = {\deltaX_0}/|\deltaX_0|$ matters.
As we do not care about the orientation of the separation vector between a
trajectory and its perturbation, but only its magnitude, we can interpret
\( %beq
\left| \jMps^\zeit(\xInit)\deltaX_0\right|^2
 =
        \transp{\deltaX_0}(\transp{\jMps^\zeit}\!\jMps^\zeit){\deltaX_0}
\,,
\) %ee{appStab-ddd}
as the  {\em error correlation matrix}.
    \index{error correlation matrix}
In the continuum mechanics language, \refeq{Cauchy-Green}, or the
right Cauchy-Green strain tensor $\transp{\jMps}\!\jMps$ is the
natural object to describe how linearized neighborhoods deform.
\emph{Stretches} of continuum mechanics are called the
\emph{finite-time} \emph{Lyapunov} or \emph{characteristic} exponents
in the theory of dynamical systems,
\beq
\Lyap(\xInit,\unitVec;\zeit) = \frac{1}{\zeit} \ln \| {\jMps}^\zeit \unitVec \|
= \frac{1}{2\zeit}\ln\left(
        \transp{\unitVec} \transp{\jMps^\zeit}\!\jMps^\zeit {\unitVec}\right)
\,.
\ee{e:finTimeLyapExp}
They depend on the initial point $\xInit$ and on the direction of the
unit vector $\unitVec$, $|\unitVec| =1$ at the initial time. If
this vector is aligned along the $i$th {principal stretch},
\(
\unitVec = {\bf u}^{(i)}
\,,
\)
then $\Lyap_i = \ln \sigma_i/t$. If ${\bf u}^{(1)}$ is the direction of
the \emph{largest principal stretch}, the corresponding finite-time
Lyapunov exponent is given by the largest stretch:
    \PC{make Trevisan\rf{TrePan98}, original Lorenz 3D Lorenz model
    singular vectors exerBox here}
\beq
\Lyap_1(\xInit;\zeit) =
%\max_{\|\unitVec\|=1}
    \Lyap(\xInit,{\bf u}^{(1)};\zeit)
    = \frac{1}{\zeit}\ln\sigma_1(\xInit;\zeit).
\ee{e:ftLyapStretch}

The leading \emph{Lyapunov exponent} is given by
        \PC{Rates of stretching?}
\beq
\Lyap(\xInit,\unitVec)
= \lim_{\zeit\to\infty} \frac{1}{\zeit} \ln \|{\jMps}^\zeit \unitVec \|
= \lim_{\zeit\to\infty} \frac{1}{2\zeit}\ln\left(
        \transp{\unitVec} \transp{\jMps^\zeit}
        \jMps^\zeit {\unitVec}\right)
\,.
\ee{e:leadLyapExp}
\index{Lyapunov!exponent, cycle}
\index{cycle!Lyapunov exponent}
    \PC{why do they say: ``Extending this relation to the case of a
    position-dependent deformation gradient requires the notion of
    simultaneous diagonalization?'' Read \refref{HoJo90}}
Expanding the initial orientation in the strain tensor eigenbasis
\refeq{e:stretches},
$
{\unitVec}=\sum ({\unitVec} \cdot {\bf u}^{(i)}) {\bf u}^{(i)}
\,,
$
we have
    \PC{wrong, need to prove the $\ExpaEig_1$ dominance}
\[ %beq
\transp{\unitVec}\transp{\jMps^\zeit}\!\jMps^\zeit{\unitVec}
        = \sum_{i=1}^d({\unitVec} \cdot {\bf u}^{(i)})^2\sigma_i^2
        = ({\unitVec} \cdot {\bf u}^{(1)})^2 \sigma_1^2
      \left( 1
          + O(\sigma_2^2/\sigma_1^2)
      \right)
\,,
\] %ee{apl}
with stretches
ordered by decreasing magnitude,
$\sigma_1 > \sigma_2 \geq \sigma_3 \cdots $.
For long times
the largest stretch dominates exponentially  \refeq{ddd},
provided the orientation ${\unitVec}$
of the initial separation
was not chosen perpendicular to the dominant
expanding eigen-direction ${\bf u}^{(1)}$.
The
%(initial point $\xInit$ dependent)
Lyapunov exponent is
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \ifdasbuch
\SFIG{leadinglyapunov.ps}
{}{
A numerical computation of the logarithm of the stretch
$\transp{\unitVec}(\transp{\jMps^\zeit}\!\jMps^\zeit){\unitVec}$ in
formula \refeq{e:leadLyapExp} for the R\"ossler flow
\refeq{Rossl_eq}, plotted as a function of the R\"ossler time units.
The slope is the leading Lyapunov exponent $\Lyap \approx 0.09$. The
exponent is positive, so numerics lends credence to the hypothesis
that the R\"ossler attractor is chaotic. The big unexplained jump
illustrates perils of Lyapunov exponents numerics.
~~(J.~Mathiesen)
}{f-RossLyap}
        \else
\SFIG{leadinglyapunov}
{}{
A numerical computation of the logarithm of the stretch
$\transp{\unitVec}(\transp{\jMps^\zeit}\!\jMps^\zeit){\unitVec}$ in
formula \refeq{e:leadLyapExp} for the R\"ossler flow
\refeq{Rossl_eq}, plotted as a function of the R\"ossler time units.
The slope is the leading Lyapunov exponent $\Lyap \approx 0.09$. The
exponent is positive, so numerics lends credence to the hypothesis
that the R\"ossler attractor is chaotic. The big unexplained jump
illustrates perils of Lyapunov exponents numerics.
~~(J.~Mathiesen)
}{f-RossLyap}
        \fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
    \PC{prove this}
    \JG{Notation for eigenvalue on LHS should match that on LHS of \refeq{ddd}.}
\bea
{\Lyap(\xInit)}
    &=& \lim_{t\to\infty}
        {1 \over t}\left\{
          \ln |{\unitVec} \cdot {\bf u}^{(1)}|
        + \ln |\ExpaEig_1(\xInit,t)|
            + O(e^{-2(\Lyap_1 - \Lyap_2) t})
               \right\}
    \continue
    &=& {1 \over t} \ln |\ExpaEig_1(\xInit,t)|
\,,
\label{Lyap-t-aver}
\eea
where $\ExpaEig_1(\xInit,t)$ is the leading eigenvalue of
$\jMps^\zeit(\xInit)$. By choosing the initial displacement such that
$\unitVec$ is normal to the first ($i$-1) eigen-directions we can define
not only the leading, but all Lyapunov exponents as well:
    \PC{edit \reffig{f-RossLyap}}
    \PC{make problem set, mention rescaling to avoid overflows}
\beq
\timeAver{\Lyap_i(\xInit)} =
   \lim_{t\rightarrow\infty}\frac{1}{t}\ln{|\ExpaEig_i(\xInit,t)|}
\,,\qquad i= 1,2, \cdots,d
\,.
\ee{lyap-i}
The leading  Lyapunov exponent now follows from the {\jacobianM} by
numerical integration of \refeq{Bew_Miaw}.
The equations can be  integrated accurately for a finite time, hence the
infinite time limit of \refeq{ddd} can be only estimated from plots of
${1\over 2}\ln(\transp{\unitVec} \transp{\jMps^\zeit}\!\jMps^\zeit {\unitVec})$ as function of time,
\index{Lyapunov!exponent!numerical}
such as \reffig{f-RossLyap}
%  of $1/2\ln|\eta_0^T \big( J^t\big)^T  J^t \eta_0|$ versus time.
for the R\"ossler flow \refeq{Rossl_eq}.
    \index{Rossler@R\"ossler!flow}
    \JG{
    \refFig{f-RossLyap} must be for a fixed, finite
$\delta \ssp_0$. If you integrated $M^t_{ij}(\ssp)$ along with $f^t(\ssp)$ as
described in early chapters, you would't get the drop-off, right?
    {\bf PC:} do not know...
    }

As the local expansion and contraction rates vary along the flow, the
temporal dependence exhibits small and large humps. The sudden fall to a
low level is caused by a close passage to a folding point of the
attractor, an illustration of why numerical evaluation of the Lyapunov
exponents, and proving the very existence of a strange attractor is a
difficult problem.
\PC{who knows?  recompute}
The approximately monotone part of the curve can be
used (at your own peril) to estimate the leading Lyapunov exponent by a
straight line fit.

As we can already see, we are courting difficulties
% may run into various problems in practice
if we try to calculate the Lyapunov exponent by using the definition
\refeq{Lyap-t-aver} directly.
First of all, the {\statesp} is dense with atypical
trajectories; for example, if $\xInit$ happens to lie on
a periodic orbit $p$, ${\Lyap}$ would be simply
$\ln|\ExpaEig_p|/\period{p}$, a local property of cycle $p$, not a
global property of the dynamical system.
    \PC{correct this}
Furthermore, even if  $\xInit$ happens to be a `generic' {\statesp}
point, it is still not obvious that $ \ln |\ExpaEig(\xInit,t)|/t$
should be converging to anything in particular. In a Hamiltonian
system with coexisting elliptic islands and chaotic regions,
a chaotic trajectory gets captured in the neighborhood
of an elliptic island every so often
and can stay there for arbitrarily long time;
as there the orbit is nearly stable,
during such episode  $ \ln |\ExpaEig(\xInit,t)|/t$ can dip
arbitrarily close to $0^{+}$. For {\statesp} volume non-preserving
flows the trajectory can traverse locally contracting regions, and
$ \ln |\ExpaEig(\xInit,t)|/t$ can occasionally go negative;
        \PublicPrivate{}{ % switch \PublicPrivate{
\toSect{c-skelet}
        }% end \PublicPrivate{
even worse, one never knows whether the asymptotic attractor is
periodic or `chaotic',
so any finite estimate of  $\timeAver{\Lyap}$ might be dead wrong.
\exerbox{e_robust_Hen}
    \MAP{there is a good Matlab module that computes a Lyapunov exp -
     will provide a reference}




\index{Lyapunov!exponent|)}

\Resume

A neighborhood of a trajectory deforms as it is transported by
a flow. In the linear approximation, the {\stabmat} ${\Mvar}$
describes the shearing / compression / expansion of an
infinitesimal neighborhood in an infinitesimal time step. The
deformation after a finite time $t$ is described
by [...]

Furthermore, although the \jacobianMs\ are multiplicative along the
flow, in dimensions higher than one their eigen\-values in general
are not.
    \PublicPrivate{
    }{% switch \PublicPrivate{
\toAppe{c-vatt-det}
    }% end \PublicPrivate{
This lack of multiplicativity has important
repercussions for both classical and quantum dynamics.
    \PublicPrivate{%\toChap{s-cFD}
    }{
\PC{remember to incorporate JacobianHist.doc}
\toChap{c-quantPinball}

Use Eckmann and Ruelle\rf{eckerg} discussion of characteristic exponents.
    }% end \PublicPrivate

  \ResumeEnd

  \Remarks

\remark{Matrix decompositions of the {\jacobianM}.}{\label{r:SVD}
A `polar decomposition' of a matrix or linear operator is a
generalization of the factorization of complex number into the polar
form, $z=r\,\exp(\phi)$. Matrix polar decomposition is explained in
\refrefs{Botti89,Truesdell91,Gurtin81,HoJo90}. One can go one step
further than the polar decomposition \refeq{PolarFact} into a product
of a rotation and a symmetric matrix, diagonalize the symmetric
matrix by a second rotation, and express any matrix with real
elements in the singular value decomposition (SVD) form
\beq
\jMps = {R_1} {D} \transp{{R_2}}
\,,
\ee{SVD-j}
where ${D}$ is diagonal and real, and ${R_1}$, ${R_2}$ are orthogonal
matrices, unique up to permutations of rows and columns. The diagonal
elements $\{\sigma_{1},\sigma_{2},\dots,\sigma_{d}\}$ of ${D}$ are
the \emph{singular values} of $\jMps$.

Though singular values decomposition provides geometrical insights
into how tangent dynamics acts, many popular algorithms for
asymptotic stability analysis (computing Lyapunov spectrum) employ
another standard matrix decomposition: the QR scheme\rf{Meyer00},
through which a nonsingular matrix $\jMps$ is (uniquely) written as a
product of an orthogonal and an upper triangular matrix $\jMps=QR$.
This can be thought as a Gram-Schmidt decomposition of the column
vectors of $\jMps$. The geometric meaning of $QR$ decomposition is
that the volume of the $d$-dim\-ens\-ion\-al parallelepiped spanned
by the column vectors of $\jMps$ has a volume coinciding with the
product of the diagonal elements of the triangular matrix $R$, whose
role is thus pivotal in algorithms computing Lyapunov
spectra\rf{bene80a,RamSri00,Skokos08}.
\index{singular value decomposition}
\index{polar decomposition}
    \PC{credit Lorenz 1965, 1984; Yoden and Nomura 1993 for introducing
    singular vectors}
    } %end \remark{Matrix decomposition of  {\jacobianM}.}


\remark{Lyapunov exponents.}{\label{r:LyapExps}
\index{multiplicative ergodic theorem}
\index{ergodic!theorem!multiplicative}
\index{Oseledec ergodic theorem}
The Multiplicative Ergodic Theorem of
\PublicPrivate{Oseledec\rf{lyaos}}{Oseledec\rf{lyaos,Pollicott93}}
states that the limits
\refeqs{ddd1}{lyap-i1}
exist for almost all points $\xInit$ and all tangent vectors $\unitVec$.
There are at most $d$
distinct values of $\Lyap$ as we let $\unitVec$ range over the tangent space.
These are the Lyapunov exponents\rf{Lyap1892} $\Lyap_i(\xInit)$.
        \PublicPrivate{}{ % switch \PublicPrivate{
Moreover there is a fibration of the tangent space ${\bf T}_\ssp\pS$,
$L^1(\ssp)\subset L^2(\ssp) \subset \cdots \subset
L^r(\ssp) ={\bf T}_\ssp\pS$,
such that if
$\unitVec \in L^i(\ssp) \setminus L^{i-1}(\ssp)$
the limit \refeq{ddd1} equals $\lambda_i(\ssp)$.
        }% end \PublicPrivate{

\index{Lyapunov!exponent!numerical}
We are doubtful of the utility of Lyapunov exponents as means
of predicting any observables of physical significance, but
that is the minority position - in the literature one
encounters many provocative speculations, especially in the
context of foundations of statistical mechanics
(`hydrodynamic' modes) and the existence of a Lyapunov
spectrum in the thermodynamic limit of spatiotemporal chaotic
systems.

There are volumes of literature on numerical computation of the
Lyapunov exponents, see for example
      \PublicPrivate{
\refrefs{WolfSwift85,EckKamp86,bene80a,bene80b}.
      }{% switch \PublicPrivate{
\refrefs{Thiffeault2001,WolfSwift85,EckKamp86,bene80a,bene80b}.
\HREF{http://www.citeulike.org/search/all?q=Lyapunov}{ {\tt  Citiulike.org}}
search yields a ton of references.
      }% end \PublicPrivate{
For early numerical methods to compute Lyapunov vectors, see
\refrefs{ShiNag79,bene80a,bene80b}. The drawback of the Gram-Schmidt
method is that the vectors so constructed are orthogonal by
fiat, whereas the stable / unstable eigen\-vectors of the
{\jacobianM} are in general not orthogonal. Hence the
Gram-Schmidt vectors are not covariant, \ie, the linearized
dynamics does not transport them into the eigen\-vectors of the
{\jacobianM} computed further downstream. For computation of
covariant Lyapunov vectors, see \refrefs{ginelli-2007-99,PoToLe98}.
\PC{read Skokos\rf{Skokos08} review}
\index{covariant Lyapunov vector}\index{Lyapunov!covariant vector}

      \PublicPrivate{}{
Probably not worth mentioning: a chaotic attractor
characterized by more than one positive Lyapunov exponent:
hyperchaos, with a `thick' chaotic attractor (12,15).
\\
12: H. Haken, Phys. Lett. A 94 (1983) 71.
\\
13: S. Hayes, C. Gerbogi, E. Ott, Phys. Rev. Lett. 70 (1993) 3031.
\\
14: M.P. Kennedy, IEEE Trans. Circuits Syst. I 41 (1994) 771.
\\
15: E. Lindberg, A. Tamasevicius, A. Cenys, G. Mykolaitis, A.
Namajunas, Hyperchaos via X Diode, in: Proceedings of the 6th
International Specialist Workshop on Nonlinear Dynamics of
Electronic Systems, Budapest, 1998, pp. 125 to 128.
    \JG{Can let $\ExpaEig_i > 0$ without loss of generality; no need for
    abs val signs throughout.\\
    PC: no, $\ExpaEig_i$ can be real with either sign, or come in
    complex pairs. Would have to write $\ExpaEig_1^\ast \ExpaEig_1$.
        \JG{
  \refeq{ddd} is misleading, to my ears, since we typically know only
  $v(\ssp)$ exactly and not $\partial f^t_i/\partial \ssp_j =
  \deltaX_i(t) / \deltaX_j(0)$. The latter has to be approximated
  through numerical integration.
  {\bf PC:} no, $\jMps^\zeit(\xInit)$ is as accurate as $\ssp(t)$,
  see \refeq{Bew_Miaw}. You are thinking of your problem in $10^5$
  dimensions - in low dimensions, no problem.
        }
    }
\PC{Trevisan\rf{TrePan98} ``the Floquet eigenvectors have
the desired properties of being independent of the definition of the
norm.'' ``The leading Lyapunov vectors, as well as the asymptotic
final singular vectors, are tangent to the attractor, while the
leading initial singular vectors, in general, point away from it.
Perturbations that are on the attractor and maximize growth should be
considered in applications. These perturbations can be found in the
subspace of the leading Lyapunov vectors.'' While Euclidean norm
might seem `natural', what is a good norm if you are trying to
define Lyapunov exponents for PDEs? General relativity?}

      }% end \PublicPrivate{
} %end \remark{Lyapunov exponents.}{

\RemarksEnd


\section{Examples}
\label{exam:Lyapunov}

The reader is urged to study the examples collected here. To
return back to the main text, click on [click to return] pointer
on the margin.

\example{Lyapunov exponent.}{\label{exam:LyapExp}
Given a 1\dmn\ map, consider observable
$\Lyap(\ssp) = \ln|\flow{'}{\ssp}|$
and integrated observable
\[
\Obser^n(\xInit) = \sum_{k=0}^{n-1} \ln|\flow{'}{\ssp_k}|
= \ln\left|\prod_{k=0}^{n-1} \flow{'}{\ssp_k}\right|
= \ln\left|\frac{\partial f^n}{\partial \ssp}(\xInit)\right|
\,.
\]
The Lyapunov exponent is the average rate of the expansion
\[
\Lyap(\xInit) = \lim_{n\rightarrow \infty} {1\over n}
\sum_{k=0}^{n-1} \ln|\flow{'}{\ssp_k}|
\,.
\]
See \refsect{s-LyapExps} for further details.
}%end \example{Lyapunov exponent}

    \PC{2012-07-17: for some reason the edits of June 29 seem to have vanished.
    Reinstated \refexam{exam:LyapExp}, make sure it is meant to be here?}

\example{Singular values and geometry of deformations:}
        { \label{exmp:ns-sing}
\index{singular values}
Suppose we are in three dimensions, and the \jacobianM\ $\jMps$ is not
singular, so that the diagonal elements of $D$ in
\refeq{SVD-j} satisfy $\sigma_1 \geq \sigma_2 \geq \sigma_3 >
0$. Consider how $\jMps$ maps the unit ball ${\cal S}=\{x
\in \mathrm{R}^3\, | \, x^2=1 \}$. $V$ is orthogonal
(rotation/reflection), so $\transp{V}{\cal S}$ is still the unit
sphere: then $D$ maps ${\cal S}$ onto ellipsoid
$\tilde{\cal S}=\{ y \in \mathrm{R}^3\, | \,
y_1^2/\sigma_1^2+y_2^2/\sigma_2^2+y_3^2/\sigma_3^2=1\}$
whose principal axes directions - $y$
coordinates - are determined by $V$. Finally the ellipsoid
is further rotated by the orthogonal matrix $U$. The local
directions of stretching and their images under $\jMps$ are
called the right-hand and left-hand singular vectors for
$\jMps$ and are given by the columns in $V$ and $U$
respectively: it is easy to check that
$\jMps v_k = \sigma_k u_k$, if $v_k,\, u_k$ are the k-th
columns of $V$ and $U$.                               \jumpBack{exmp:ns-sing}
} % end \example{{Singular values and geometry of deformations:}
    %
    \PC{give $2$\dmn\ example of ellipsoid explicitly}
\PublicPrivate{
        }{ % switch \PublicPrivate{
\toRem{r:SVD}
        }% end \PublicPrivate

         % end \section{Examples}

        \ifdasbuch
  \input{Problems/exerLyapunov}
  \input{chapter/refsLyapunov}
        \else
        \fi

\ChapterEnd
