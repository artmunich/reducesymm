    \iftoCB
\chapter{Energy, dissipation, invariant moments}
\label{c-energy}
    \else
\Chapter{Lyapunov}{21apr2013}{Lyapunov exponents}
\listofsections{0}
% $Author$ $Date$
    \fi
% Predrag created Lyapunov.tex          18mar2013
% Predrag added stretches               11mar2013
% Predrag singular value decomposition  27jun2011
% Predrag                               30jan2008
% Predrag making Grigo happy            30aug2007
%       Predrag          14/3-95

\index{Lyapunov!exponent|(}

    \iftoCB
Let us apply
    \else
\noindent
\lettrine[lines=3,lraise=0.1,lhang=0.2]{L}{et us apply}
    \fi
the newly acquired tools to the fundamental diagnostics in
dynamics: Is a given system `chaotic'? And if so, how chaotic?
                                        \toExam{exmp:FlowStrange}
If all points in a neighborhood of a trajectory converge toward the same
trajectory, the attractor is a fixed point or a limit cycle.
                                        \toSect{s-What-is-chaos}
However, if the attractor is strange, any two trajectories
\( %beq
\ssp(t)=\flow{t}{\xInit} \; {\rm and } \;
\ssp(t)+\deltaX(t) =\flow{t}{\xInit+\deltaX_0}
\) %ee{trajs}
                                        \toRem{r:damdLyaps}
that start out very close to each other separate exponentially with time,
and in a finite time their separation attains the size of the accessible
{\statesp}.

This {\em sensitivity to initial conditions} can be quantified as
\beq
|\deltaX(t)| \approx e^{\Lyap t} |\deltaX_0|
\ee{traj-sep}
\noindent
where $\Lyap$, the mean rate of separation of trajectories of the
system, is called the leading {\em Lyapunov exponent}. In the limit
of infinite time the Lyapunov exponent is a global measure of the
rate at which nearby trajectories diverge, averaged over the strange
attractor. As it so often goes with easy ideas, it turns out that
Lyapunov exponents are not natural for study of dynamics, and we
would have passed them over in silence, were it not for so much
literature that talks about them. So in a textbook we are duty bound
to explain what all the excitement is about. But we do round the
chapter of with a remark longer than the chapter itself.
\index{attractor!strange}\index{strange attractor}
\index{limit cycle}\index{cycle!limit}
\index{sensitivity to initial conditions}
    \PC{2013-0320 remember to correct
    \HREF{http://en.wikipedia.org/wiki/Lyapunov_exponent}
    {en.wikipedia.org/wiki/Lyapunov\_exponent}
    }

\section{Stretch, strain and twirl}
%\subsection{Singular value decomposition}
\begin{bartlett}
\bauthor{
\HREF{http://prairiehome.publicradio.org/programs/20031129/scripts/guy_noir.shtml}
    {Governor Arnold Schwarzenegger}
        }
Diagonalizing the matrix: that's the key to the whole thing.
\end{bartlett}

\noindent
\cyclist
In general the \jacobianM\ $\jMps^\zeit$ is neither diagonal, nor
diagonalizable, nor constant along the trajectory. What is a
geometrical meaning of the mapping of a neighborhood by $\jMps$? Here
the continuum mechanics insights are helpful, in particular the polar
decomposition which affords a visualization of the linearization of a
flow as a mapping of the initial ball into an
ellipsoid (\reffig{f:jacobMat}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\SFIG{jacobian}{}{
The linearized flow maps a swarm of initial points in an
infinitesimal spherical neighborhood of squared radius $\delta\ssp^2$
at $\xInit$ into an ellipsoid
$\delta\transp{\ssp}(\transp{\jMps}\!\jMps)\,\delta\ssp$ at
$\ssp(\zeit)$ finite time $\zeit$ later,  rotated and
stretched/compressed along the principal axes by streches
$\{\sigma_{j}\}$ .
}{f:jacobMat}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

First, a few definitions: A symmetric $[d\!\times\!d]$ matrix
$\covMat$ is \emph{positive definite}, $\covMat > 0$, if
$\transp{\ssp}\covMat \ssp > 0$ for any nonzero vector $\ssp \in
\reals^d$. $\covMat$ is \emph{negative definite},  $\covMat < 0$, if
$\transp{\ssp}\covMat \ssp < 0$ for any nonzero vector $\ssp$.
Alternatively, $\covMat$ is a positive (negative) definite matrix if
all its eigenvalues are positive (negative). A matrix ${R}$ is
orthogonal if $\transp{{R}} {R} = \matId$, and proper orthogonal if
$\det {R} = +1$. Here the superscript $\transp{{}}$ denotes the
transpose. For example, $(\ssp_1,\cdots,\ssp_d)$ is a {row} vector,
$\transp{(\ssp_1,\cdots,\ssp_d)}$ is a {column} vector.
\index{matrix!positive definite}\index{positive definite matrix}
\index{matrix!negative definite}\index{negative definite matrix}

                                                    \toRem{r:SVD}
By the {polar decomposition theorem}, a deformation $\jMps$ can be
factored into a {rotation} ${R}$ and a right~/~left {stretch}
tensor ${U}$~/~${V}$,
\beq
\jMps={R}{U}= {V}{R}
\,,
\ee{PolarFact}
where ${R}$ is a proper-orthogonal matrix and ${U}$, ${V}$ are
symmetric positive definite matrices with strictly positive real
eigenvalues $\{\sigma_1,\sigma_2,\cdots,\sigma_d\}$ called
\emph{principal stretches} (singular values, Hankel singular values),
and with orthonormal eigenvector bases,
\index{singular values}\index{Hankel singular values}
\bea
{U}\,{u}^{(i)} &=& \sigma_i{u}^{(i)}
    \,,\qquad \{{u}^{(1)},{u}^{(2)},\cdots,{u}^{(d)}\}
\continue
{V}\,{v}^{(i)} &=& \sigma_i{v}^{(i)}
    \,,\qquad \{{v}^{(1)},{v}^{(2)},\cdots,{v}^{(d)}\}
\,.
\label{e:stretches}
\eea
$\sigma_i > 1$ for stretching and $0<\sigma_i<1$ for compression
along the direction ${u}^{(i)}$ or ${v}^{(i)}$. $\{{u}^{(j)}\}$ are
the \emph{principal axes of strain} at the initial point $\xInit$;
$\{{v}^{(j)}\}$  are the principal axes of strain at the present
placement $\ssp$. From a geometric point of view, $\jMps$ maps the
unit sphere into an ellipsoid, \reffig{f:jacobMat}; the principal
stretches are then the lengths of the semiaxes of this ellipsoid. The
rotation matrix ${R}$ carries the initial axes of strain into the
present ones,
\(
{V}= {R}{U}\transp{R}
\,.
\)
The eigenvalues of the
\PC{
In spirit of \refsect{p:spaghetti}: Lagrangian=co-moving,
Eulerian=external `reference' coordinate frame:
   ``the columns of the matrix  ${V}$ are the principal axes
${e}_i$ of stretching in the Lagrangian coordinate frame, and the
orthogonal matrix ${R}$ gives the orientation of the ellipse in the
Eulerian coordinates.
%\index{Lagrangian!coordinates}
%\index{Eulerian coordinates}
   }
                            \toRem{r:damdLyaps}
\bea
\mbox{right Cauchy-Green strain tensor:} && \transp{\jMps}\!\jMps=U^2
    \continue
\mbox{ left Cauchy-Green strain tensor:} && \jMps\,\transp{\jMps}=V^2
\label{Cauchy-Green}
\eea
are $\{\sigma_j^2\}$, the squares of principal stretches.
    \index{polar decomposition}\index{stretches}
    \index{principal!stretches}\index{principal!directions}
    \index{Cauchy-Green strain tensor}

\fastTrackExam{exmp:ns-sing}

\section{Lyapunov exponents}
\label{s-LyapExps}
% must correct http://en.wikipedia.org/wiki/Lyapunov_exponent
%   Predrag created \Chapter{Lyapunov}      18mar2013
%   Predrag moved from \Chapter{stability}  21sep2001
% Dorte                                     24sep2001
% Predrag                                   18sep2001
% Predrag:  moved from getused.tex          27sep2001
% PC moved from \Chapter{applic}{            9aug2000
% GV                                         7jul2000
% PC                                        23jan2000
% mal\_conv   Non-linearity version          7feb1990
% Joachim Mathiesen                         26jan2000
%   excerpted from 'A Study of The Rössler Attractor'
%   Term paper for Fysik711B fall semester 1999

\authorJMPC

The mean growth rate of the distance $\norm{\deltaX(t)}/\norm{\deltaX_0}$
between neighboring trajectories \refeq{traj-sep} is given by the
leading {\em Lyapunov exponent} which can be estimated for long (but not
too long) time $\zeit$ as
\beq
\Lyap \simeq %\lim_{t\rightarrow\infty}
    \frac{1}{\zeit} %\lim_{\delta x\rightarrow\infty}
    \ln{\frac{\norm{\deltaX(\zeit)}}{\norm{\deltaX(0)}}}
%\,.
\ee{lyax}
(For notational brevity we shall often suppress the dependence of
quantities such as $\Lyap =\Lyap(\xInit)$, $\deltaX(t) =
\deltaX(\xInit,t)$ on the initial point $\xInit$). One can take
\refeq{lyax} as is, take a small initial separation $\deltaX_0$,
track the distance between two nearby trajectories until
$\norm{\deltaX(t_1)}$ gets significantly bigger, then record $ t_1 \Lyap_1
=\ln (\norm{\deltaX(t_1)}/\norm{\deltaX_0})$, rescale $\deltaX(t_1)$ by factor
${\deltaX_0}/{\deltaX(t_1)}$, and continue add infinitum, as in
\reffig{f:lyapCalc}, with the leading Lyapunov exponent given by
\beq
\Lyap =\lim_{t\rightarrow\infty}\frac{1}{t}
    \sum_i t_i \Lyap_i
\,,\qquad
    t = \sum_i t_i
\,.
\ee{LyapSepar}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\SFIG{lyapCalc}
{}{
A long-time numerical calculation of the leading Lyapunov exponent
requires rescaling the distance in order to keep the
nearby trajectory separation within the linearized flow range.
}{f:lyapCalc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Deciding what is a safe 'linear range', the distance beyond which the
separation vector $\deltaX(\zeit)$ should be rescaled is a dark art.

We can start out with a small $\deltaX$ and try to estimate the
leading Lyapunov exponent $\Lyap$ from \refeq{LyapSepar}, but now
that we have quantified the notion of linear stability in
\refchap{c-stability}, we can do better. The problem with measuring
the growth rate of the distance between two points is that as the
points separate, the measurement is less and less a local
measurement. In the study of experimental time series this might be
the only option, but if we have equations of motion, a better way is
to measure the growth rate of vectors transverse to a given orbit.

Given the equations of motion, for
infinitesimal $\deltaX$ we know the $\deltaX_i(t)/\deltaX_j(0)$ ratio
exactly,
as this is by definition the \jacobianM\
\[
\lim_{\deltaX(0) \to 0} {\deltaX_i(t) \over \deltaX_j(0)}
        = {\pde \ssp_i(t) \over \pde \ssp_j(0)}
        = \jMps^\zeit_{ij}(\xInit)
\,,
\]
so the leading {Lyapunov exponent} can be computed from the
linearization \refeq{xit_1}
\beq
{\Lyap(\xInit)}
 = \lim_{t\rightarrow\infty}\frac{1}{t}\ln
        { \norm{\jMps^\zeit(\xInit)\,\deltaX_0} \over \norm{\deltaX_0}}
 = \lim_{t\rightarrow\infty}\frac{1}{2t}\ln\left(
        \transp{\unitVec}
        \transp{\jMps^\zeit}\!\jMps^\zeit {\unitVec}\right)
\,.
\ee{ddd}
In this formula the scale of the initial separation drops out, only
its orientation given by the initial orientation unit vector
${\unitVec} = {\deltaX_0}/\norm{\deltaX_0}$ matters. If one does not care
about the orientation of the separation vector between a trajectory
and its perturbation, but only its magnitude, one can interpret
\( %beq
\norm{\jMps^\zeit(\xInit)\deltaX_0}^2
 =
        \transp{\deltaX_0}(\transp{\jMps^\zeit}\!\jMps^\zeit)\,{\deltaX_0}
\,,
\) %ee{appStab-ddd}
as the  {\em error correlation matrix}.
    \index{error correlation matrix}
In the continuum mechanics language, the right Cauchy-Green strain
tensor $\transp{\jMps}\!\jMps$ \refeq{Cauchy-Green} is the natural
object to describe how linearized neighborhoods deform. In the theory
of dynamical systems the \emph{stretches} of continuum mechanics are
called the \emph{finite-time} \emph{Lyapunov} or
\emph{characteristic} exponents,
\beq
\Lyap(\xInit,\unitVec;\zeit) = \frac{1}{\zeit} \ln \norm{{\jMps}^\zeit \unitVec}
= \frac{1}{2\zeit}\ln\left(
        \transp{\unitVec} \transp{\jMps^\zeit}\!\jMps^\zeit {\unitVec}\right)
\,.
\ee{e:finTimeLyapExp}
They depend on the initial point $\xInit$ and on the direction of the
unit vector $\unitVec$, $\norm{\unitVec} =1$ at the initial time. If this
vector is aligned along the $i$th {principal stretch},
\(
\unitVec = {u}^{(i)}
\,,
\)
then the corresponding finite-time
Lyapunov exponent (rate of stretching) is given by
    \PC{make Trevisan\rf{Lyap:TrePan98}, original Lorenz 3D Lorenz model
    singular vectors exerBox here}
\beq
\Lyap_j(\xInit;\zeit) =
%\max_{\|\unitVec\|=1}
    \Lyap(\xInit,{u}^{(j)};\zeit)
    = \frac{1}{\zeit}\ln\sigma_j(\xInit;\zeit).
\ee{e:ftLyapStretch}
We do not need to compute the strain tensor eigenbasis to determine
the leading \emph{Lyapunov exponent},
    \index{Lyapunov!exponent!cycle}\index{cycle!Lyapunov exponent}
    \index{characteristic!exponent}\index{characteristic!value}
\beq
\Lyap(\xInit,\unitVec)
= \lim_{\zeit\to\infty} \frac{1}{\zeit} \ln \norm{{\jMps}^\zeit \unitVec}
= \lim_{\zeit\to\infty} \frac{1}{2\zeit}\ln\left(
        \transp{\unitVec} \transp{\jMps^\zeit}
        \jMps^\zeit {\unitVec}\right)
\,,
\ee{e:leadLyapExp}
as expanding the initial orientation in the strain tensor eigenbasis
\refeq{e:stretches},
$
{\unitVec}=\sum ({\unitVec} \cdot {u}^{(i)}) {u}^{(i)}
\,,
$
we have
    \PC{wrong, need to prove the $\ExpaEig_1$ dominance}
\[ %beq
\transp{\unitVec}\transp{\jMps^\zeit}\!\jMps^\zeit{\unitVec}
        = \sum_{i=1}^d({\unitVec} \cdot {u}^{(i)})^2\sigma_i^2
        = ({\unitVec} \cdot {u}^{(1)})^2 \sigma_1^2
      \left( 1
          + O(\sigma_2^2/\sigma_1^2)
      \right)
\,,
\] %ee{apl}
with stretches ordered by decreasing magnitude, $\sigma_1 > \sigma_2
\geq \sigma_3 \cdots $. For long times the largest stretch dominates
exponentially \refeq{e:leadLyapExp}, provided the orientation
${\unitVec}$ of the initial separation was not chosen perpendicular
to the dominant expanding eigen-direction ${u}^{(1)}$. Furthermore,
for long times $\jMps^\zeit{\unitVec}$ is dominated by the largest
stability multiplier $\ExpaEig_1$, so the leading Lyapunov exponent
is
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\SFIG{leadinglyapunov}
{}{
A numerical computation of the logarithm of the stretch
$\transp{\unitVec}(\transp{\jMps^\zeit}\!\jMps^\zeit){\unitVec}$ in
formula \refeq{e:leadLyapExp} for the R\"ossler flow
\refeq{Rossl_eq}, plotted as a function of the R\"ossler time units.
The slope is the leading Lyapunov exponent $\Lyap \approx 0.09$. The
exponent is positive, so numerics lends credence to the hypothesis
that the R\"ossler attractor is chaotic. The big unexplained jump
illustrates perils of Lyapunov exponents numerics.
~~(J.~Mathiesen)
}{f-RossLyap}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
    \PC{prove this}
%    \JG{Notation for eigenvalue on LHS should match that on LHS of
%        \refeq{ddd}.}
\bea
{\Lyap(\xInit)}
    &=& \lim_{t\to\infty}
        {1 \over t}\left\{
          \ln \norm{{\unitVec} \cdot \jEigvec[1]}
        + \ln |\ExpaEig_1(\xInit,t)|
            + O(e^{-2(\Lyap_1 - \Lyap_2) t})
               \right\}
    \continue
    &=& {1 \over t} \ln |\ExpaEig_1(\xInit,t)|
\,,
\label{Lyap-t-aver}
\eea
where $\ExpaEig_1(\xInit,t)$ is the leading eigenvalue of
$\jMps^\zeit(\xInit)$.
    \PublicPrivate{}{
By choosing the initial displacement such that
$\unitVec$ is normal to the first ($i$-1) eigen-directions we can define
not only the leading, but all Lyapunov exponents as well:
    \PC{edit \reffig{f-RossLyap}; fix \refeq{lyap-i}}
    \PC{make problem set, mention rescaling to avoid overflows}
\beq
\timeAver{\Lyap_i(\xInit)} =
   \lim_{t\rightarrow\infty}\frac{1}{t}\ln{|\ExpaEig_i(\xInit,t)|}
\,,\qquad i= 1,2, \cdots,d
\,.
\ee{lyap-i}
    } %end \PublicPrivate
The leading  Lyapunov exponent now follows from the {\jacobianM} by
numerical integration of \refeq{Bew_Miaw}. The equations can be
integrated accurately for a finite time, hence the infinite time
limit of \refeq{ddd} can be only estimated from a finite set of
evaluations of ${1\over 2}\ln(\transp{\unitVec}
\transp{\jMps^\zeit}\!\jMps^\zeit{\unitVec})$ as function of time,
such as \reffig{f-RossLyap} for the R\"ossler flow \refeq{Rossl_eq}.
    \index{Lyapunov!mode}\index{Lyapunov!exponent!numerical}
    \index{Rossler@R\"ossler!flow}
    \JG{
    \refFig{f-RossLyap} must be for a fixed, finite
$\delta \ssp_0$. If you integrated $M^t_{ij}(\ssp)$ along with $f^t(\ssp)$ as
described in early chapters, you would't get the drop-off, right?
    {\bf PC:} do not know...
    }

As the local expansion and contraction rates vary along the flow, the
temporal dependence exhibits small and large humps. The sudden fall
to a low value in \reffig{f-RossLyap} is caused by a close passage to
a folding point of the attractor, an illustration of why numerical
evaluation of the Lyapunov exponents, and proving the very existence
of a strange attractor is a difficult problem.
\PC{who knows?  recompute}
The approximately monotone part of the curve can be used (at your own
peril) to estimate the leading Lyapunov exponent by a straight line
fit.

As we can already see, we are courting difficulties if we try to
calculate the Lyapunov exponent by using the definition
\refeq{Lyap-t-aver} directly. First of all, the {\statesp} is dense
with atypical trajectories; for example, if $\xInit$ happens to lie
on a periodic orbit $p$, ${\Lyap}$ would be simply
$\ln|\ExpaEig_p|/\period{p}$, a local property of cycle $p$, not a
global property of the dynamical system.
    \PC{correct this}
Furthermore, even if  $\xInit$ happens to be a `generic' {\statesp}
point, it is still not obvious that $\ln|\ExpaEig(\xInit,t)|/t$
should be converging to anything in particular. In a Hamiltonian
system with coexisting elliptic islands and chaotic regions, a
chaotic trajectory gets captured in the neighborhood of an elliptic
island every so often and can stay there for arbitrarily long time;
as there the orbit is nearly stable, during such episode
$\ln|\ExpaEig(\xInit,t)|/t$ can dip arbitrarily close to $0^{+}$. For
{\statesp} volume non-preserving flows the trajectory can traverse
locally contracting regions, and $ \ln |\ExpaEig(\xInit,t)|/t$ can
occasionally go negative;
        \PublicPrivate{}{ % switch \PublicPrivate{
\toSect{c-skelet}
        }% end \PublicPrivate{
even worse, one never knows whether the asymptotic attractor is
periodic or `chaotic', so any finite estimate of  $\timeAver{\Lyap}$
might be dead wrong.
\exerbox{e_robust_Hen}
    \MAP{there is a good Matlab module that computes a Lyapunov exp -
     will provide a reference}




\index{Lyapunov!exponent|)}

\Resume
Let us summarize the `stability' \refchaptochap{c-stability}{c-Lyapunov}.
A neighborhood of a trajectory deforms as it is transported by
a flow. In the linear approximation, the {\stabmat} ${\Mvar}$
describes the shearing / compression / expansion of an
infinitesimal neighborhood in an infinitesimal time step. The
deformation after a finite time $t$ is described
by the \jacobianM\ $\jMps^\zeit$.

\emph{Floquet multipliers} and \emph{eigen-vectors} are property of
finite-time, compact invariant solutions, such as \po s and \rpo s;
they are explained in \refchap{c-invariants}. \emph{Stability
exponents}\rf{Lyap:GoSuOr87} are the corresponding long-time limit.

{Finite-time} {Lyapunov} exponents and the associated \emph{principal axes}
are defined in \refeq{e:finTimeLyapExp}.
Oseledec \emph{Lyapunov exponents} are the $\zeit\to\infty$
limit of these.

  \ResumeEnd

  \Remarks

\remark{Lyapunov exponents are unnatural.}{\label{r:damdLyaps}
Eigenvectors / eigenvalues are suited to study of iterated forms of a
matrix, such as \jacobianM\ $\jMps^\zeit$ or exponential
$\exp(\zeit\Mvar)$, and are thus a natural tool for study of
dynamics. Principal vectors are not, they are suited to study of the
matrix $\jMps^\zeit$ itself. The polar (singular value)
decomposition is convenient for numerical work (any matrix, square or
rectangular, can be brought to such form), as a way of estimating the
effective rank of matrix $\jMps$ by separating the large, significant
singular values from the small, negligible singular values.

That is the first problem with Lyapunov exponents: stretches
$\{\sigma_{j}\}$ are \emph{not related} to the \jacobianM\
$\jMps^\zeit$ eigenvalues $\{\ExpaEig_{j}\}$ in any simple way. The
eigenvectors $\{{u}^{(j)}\}$  of strain tensor
$\transp{\jMps}\!\jMps$ that determine the orientation of the
principal axes, are distinct from the \jacobianM\ eigenvectors
$\{\jEigvec[j]\}$. The strain tensor $\transp{\jMps}\!\jMps$
satisfies no multiplicative semigroup property such as
\refeq{Jmultiplic}; unlike the \jacobianM\ \refeq{Jrepeat}, the
strain tensor $\transp{\jMps}{}^r\!\jMps^r$ for the $r$th repeat of a
prime cycle $p$ is not given by a power of $\transp{\jMps}\!\jMps$
for the single traversal of the prime cycle $p$. Under time evolution
the covariant vectors map forward as $\jEigvec[j] \to
\jMps\,\jEigvec[j]$ (transport of the velocity vector
\refeq{JacobVeloc} is an example). In contrast, the principal axes
have to be recomputed from the scratch for each time $\zeit$.
    \index{covariant Lyapunov vector}\index{Lyapunov!covariant vector}
    \index{covariant vector}
    \index{semigroup!dynamical}\index{singular values}
    \PC{fix this: \reffig{f:covariant1} is for $\jMps$,
        \reffig{f:jacobMat} is for $\transp{\jMps}\!\jMps$}
    \PC{Make Trevisan\rf{Lyap:TrePan98} 2D example an exercise (with
        given answers). Discuss transient growth.}
    \PC{replace the tensors by the invariants - then the
    relations between $\{\ExpaEig_{j}\}$ and $\{\sigma_{j}\}$ should
    be immediate?
    }
    \PC{Make an exercise showing that stretch of going twice around
        a cycle is not a square of going around once.}

% [2011-07-01 Predrag]

Lorenz\rf{Lyap:Lorenz65,Lyap:Lorenz84,Lyap:YoNo93} pioneered the use of singular
vectors in chaotic dynamics. We found the Goldhirsch, Sulem and
Orszag\rf{Lyap:GoSuOr87} exposition very clear, \PC{They correctly
distinguish \emph{Lyapunov} eigenvalues and eigenvectors from the
\emph{stability} eigenvalues and eigenvectors.} and we also enjoyed
{Hoovers}\rf{Lyap:HooHoo12} pedagogical introduction to computation of
Lyapunov spectra by the method of Lagrange multipliers. Greene and
Kim\rf{Lyap:GreeKim87} discuss singular values vs. \jacobianM\
eigenvalues. While they conclude that ``singular values, rather than
eigenvalues, are the appropriate quantities to consider when studying
chaotic systems,'' we beg to differ: their Fig.~3, which illustrates
various semiaxes of the ellipsoid in the case of Lorenz attractor, as
well as the figures in \refref{Lyap:TrePan98}, are a persuasive argument
for {\em not} using singular values. The covariant vectors are
tangent to the attractor, while the principal axes of strain point
away from it. It is the perturbations within the attractor that
describe the long-time dynamics within the attractor; these
perturbations lie within the subspace spanned by the leading
covariant vectors.

If Lyapunov exponents are not dynamical, why are they invoked so
frequently? One reason is fear of mathematics: the monumental and
therefore rarely read \PublicPrivate{Oseledec\rf{Lyap:lyaos}}
{Oseledec\rf{Lyap:lyaos,Lyap:Pollicott93}} Multiplicative Ergodic Theorem
states that the limits \refeqs{ddd}{lyap-i} exist for almost all
points $\xInit$ and all vectors $\unitVec$, and that there are at most $d$
distinct Lyapunov exponents $\Lyap_i(\xInit)$ as $\unitVec$ ranges
over the tangent space. To intimidate the reader further we note in
passing that ``moreover there is a fibration of the tangent space
${\bf T}_\ssp\pS$, $L^1(\ssp)\subset L^2(\ssp) \subset \cdots \subset
L^r(\ssp) ={\bf T}_\ssp\pS$, such that if $\unitVec \in L^i(\ssp)
\setminus L^{i-1}(\ssp)$ the limit \refeq{ddd} equals
$\lambda_i(\ssp)$.'' Oseledec proof is important mathematics, but the
method is not helpful in elucidating dynamics. The other reason is
physical and practical: Lorenz\rf{Lyap:Lorenz65,Lyap:Lorenz84,Lyap:YoNo93} asked how
does a cloud of initial points $\ssp(0) + \deltaX(0)$ distributed as
a Gaussian with covariance matrix $\covMat(0)=
\expct{\deltaX(0)\transp{\deltaX(0)}}$ evolves in time. For
linearized flow with initial isotropic distribution
$\covMat(0)= \epsilon \unit$ the answer is given by the
left Cauchy-Green strain tensor,
\beq
\covMat(\zeit)=
\expct{\deltaX(0)\,\jMps\;\transp{\jMps}\transp{\deltaX(0)}}
 = \jMps\!\covMat(\zeit)\transp{\jMps} = \epsilon \jMps\;\transp{\jMps}
\,.
\ee{covEvol}
\index{multiplicative ergodic theorem}
\index{ergodic!theorem!multiplicative}
\index{Oseledec ergodic theorem}

The second problem with Lyapunov exponents is deeper: the intuitive
definition \refeq{lyax} depends on the notion of distance
$\norm{\deltaX(\zeit)}$ between two \statesp\ points. The Euclidean
(or $L^2$) distance is natural in the theory of $3D$ continuous
media, but what the norm should be in for other \statesp s is far from
clear, especially in high dimensions and for PDEs. As we have shown
in \refsect{CyclStabCyclInv}, Floquet multipliers are invariant under
all local smooth nonlinear coordinate transformation, intrinsic to the
flow, the Floquet eigenvectors are independent of the definition of
the norm\rf{Lyap:TrePan98}. In contrast, the stretches $\{\sigma_{j}\}$,
and the right/left principal axes depend on the choice of the norm.

There is probably no name more liberally and more confusingly used in
dynamical systems literature than that of Lyapunov (AKA Liapunov).
Singular values / principal axes of strain tensor
$\transp{\jMps}\!\jMps$ (objects natural to the theory of
deformations) and their long-time limits can indeed be traced back to
the thesis of Lyapunov\rf{Lyap:Lyap1892,Lyap:lyaos}, and justly deserve
sobriquet 'Lyapunov'. Oseledec\rf{Lyap:lyaos} refers to them as `Liapunov
characteristic numbers', and Eckmann and Ruelle\rf{Lyap:eckerg} as
`{characteristic} exponents'.
The natural objects in dynamics are the linearized flow
\jacobianM\ $\jMps^\zeit$, and its eigenvalues and eigenvectors
(stability exponents and covariant vectors).
Why should they also be called 'Lyapunov'? The
{covariant} vectors are misnamed in recent papers as `covariant
Lyapunov vectors' or `Lyapunov vectors', even though \emph{they are
not} the eigenvectors that correspond to the Lyapunov exponents.
That's just confusing, for no good reason - Lyapunov has nothing to
do with linear stability described by the \jacobianM\ $\jMps$, as far
as we understand his paper\rf{Lyap:Lyap1892} is about
$\transp{\jMps}\!\jMps$ and the associated principal axes. To
emphasize the distinction, the \jacobianM\ eigenvectors
$\{\jEigvec[j]\}$ are in recent literature called `covariant' or
`covariant Lyapunov vectors', or `stationary Lyapunov
basis'\rf{Lyap:ErshPot98}. However, Trevisan\rf{Lyap:TrePan98} refers to covariant
vectors as `Lyapunov vectors', and Radons\rf{Lyap:YaRa10} calls them
`Lyapunov modes', motivated by thinking of these eigenvectors as a
generalization of `normal modes' of mechanical systems, whereas by
$i$th `Lyapunov mode' Takeuchi and Chat\'{e}\rf{Lyap:TaCh12} mean
$\{\Lyap_j,\jEigvec[j]\}$, the set of the $i$th stability exponent
and the associated covariant vector. Kunihiro~\etal\rf{Lyap:KMOSTY10} call the
eigenvalues of \stabmat\ \refeq{DerMatrix}, evaluated at a given
instant in time, the `local Lyapunov exponents', and they refer to
the set of stability exponents \refeq{stabExpon} for a finite time
\jacobianM\ as the `intermediate Lyapunov exponent', ``averaged''
over a finite time period. The list goes on: there is `Lyapunov
equation' of control theory, which is the linearization of the
`Lyapunov function', and the entirely unrelated `Lyapunov orbit' of
celestial mechanics.
    \index{Lyapunov!characteristic numbers}
    \index{stationary Lyapunov basis}\index{Lyapunov!mode}
    \index{Lyapunov!equation}\index{Lyapunov!function}
    \index{Lyapunov!orbit}
    \index{covariant Lyapunov vector}\index{Lyapunov!covariant vector}
    \index{Lyapunov!vector}

In short: Lyapunov exponents are an abomination. We are doubtful of
their utility as means of predicting any observables of physical
significance, but that is the minority position - in the literature
one encounters many provocative speculations, especially in the
context of foundations of statistical mechanics (`hydrodynamic'
modes) and the existence of a Lyapunov spectrum in the thermodynamic
limit of spatiotemporal chaotic systems.

    }% end \remark{Lyapunov exponents are unnatural.



\remark{Matrix decompositions of the {\jacobianM}.}{\label{r:SVD}
A `polar decomposition' of a matrix or linear operator is a
generalization of the factorization of complex number into the polar
form, $z=r\,\exp(\phi)$. Matrix polar decomposition is explained in
\refrefs{Lyap:Botti89,Lyap:Truesdell91,Lyap:Gurtin81,Lyap:HoJo90}. One can go one step
further than the polar decomposition \refeq{PolarFact} into a product
of a rotation and a symmetric matrix by diagonalizing the symmetric
matrix by a second rotation, and thus express any matrix with real
elements in the singular value decomposition (SVD) form
\beq
\jMps = {R_1} {D} \transp{{R_2}}
\,,
\ee{SVD-j}
where ${D}$ is diagonal and real, and ${R_1}$, ${R_2}$ are orthogonal
matrices, unique up to permutations of rows and columns. The diagonal
elements $\{\sigma_{1},\sigma_{2},\dots,\sigma_{d}\}$ of ${D}$ are
the \emph{singular values} of $\jMps$.

Though singular values decomposition provides geometrical insights
into how tangent dynamics acts, many popular algorithms for
asymptotic stability analysis (computing Lyapunov spectrum) employ
another standard matrix decomposition: the QR scheme\rf{Lyap:Meyer00},
through which a nonsingular matrix $\jMps$ is (uniquely) written as a
product of an orthogonal and an upper triangular matrix $\jMps=QR$.
This can be thought as a Gram-Schmidt decomposition of the column
vectors of $\jMps$. The geometric meaning of $QR$ decomposition is
that the volume of the $d$-dim\-ens\-ion\-al parallelepiped spanned
by the column vectors of $\jMps$ has a volume coinciding with the
product of the diagonal elements of the triangular matrix $R$, whose
role is thus pivotal in algorithms computing Lyapunov
spectra\rf{Lyap:Skokos08}.
    \PC{hunt up refs BenLy,varLy}
\index{singular value decomposition}
\index{polar decomposition}
    } %end \remark{Matrix decomposition of  {\jacobianM}.}


\remark{Numerical evaluation of Lyapunov exponents.}
       {\label{r:numerLyaps}
\index{Lyapunov!exponent!numerical}
There are volumes of literature on numerical computation of the
Lyapunov exponents, see for example
\refrefs{Lyap:WolfSwift85,Lyap:eckerg,Lyap:EckKamp86,Lyap:Thiffeault2001}.
% \HREF{http://www.citeulike.org/search/all?q=Lyapunov}{ {\tt
% Citiulike.org}} search yields a ton of references.
For early numerical methods to compute Lyapunov vectors, see
\refrefs{Lyap:ShiNag79,Lyap:bene80a}. The drawback of the Gram-Schmidt
method is that the vectors so constructed are orthogonal by
fiat, whereas the stable / unstable eigen\-vectors of the
{\jacobianM} are in general not orthogonal. Hence the
Gram-Schmidt vectors are not covariant, \ie, the linearized
dynamics does not transport them into the eigen\-vectors of the
{\jacobianM} computed further downstream. For computation of
covariant vectors, see \refrefs{Lyap:ginelli-2007-99,Lyap:PoToLe98}.
\PC{read Skokos\rf{Lyap:Skokos08} review}
\index{covariant Lyapunov vector}\index{Lyapunov!covariant vector}
    }% end \remark{Numerical evaluation ...}{\label{r:numerLyaps}

\RemarksEnd

      \PublicPrivate{}{

At that time Goldhirsch \etal\ had no proof that the long-time Lyapunov
exponents converge to the stability exponents (Kazumasa, do you have
some more recent paper that you prefer to Goldhirsch \etal?).

Proposal: First, explain difference between Floquet multipliers and
stretching rates for an explicit 3\dmn example. Second, show the
relation to Lyapunov exponents by looking at the infinite repeat of a
cycle. Prove that the leading Floquet exponent goes into the leading
Lyapunov exponent. Is it true for the rest of the spectrum?

The sum of positive Lyapunov exponents is the
\emph{Kolmogorov entropy} of a dynamical system\rf{Lyap:GraPro83}.
				
      }% end \PublicPrivate{


\section{Examples}
\label{exam:Lyapunov}

The reader is urged to study the examples collected here. To
return back to the main text, click on [click to return] pointer
on the margin.

\example{Lyapunov exponent.}{\label{exam:LyapExp}
Given a 1\dmn\ map, consider observable
$\Lyap(\ssp) = \ln|\flow{'}{\ssp}|$
and integrated observable
\[
\Obser^n(\xInit) = \sum_{k=0}^{n-1} \ln|\flow{'}{\ssp_k}|
= \ln\left|\prod_{k=0}^{n-1} \flow{'}{\ssp_k}\right|
= \ln\left|\frac{\partial f^n}{\partial \ssp}(\xInit)\right|
\,.
\]
The Lyapunov exponent is the average rate of the expansion
\[
\Lyap(\xInit) = \lim_{n\rightarrow \infty} {1\over n}
\sum_{k=0}^{n-1} \ln|\flow{'}{\ssp_k}|
\,.
\]
See \refsect{s-LyapExps} for further details.
}%end \example{Lyapunov exponent}

    \PC{2012-07-17: for some reason the edits of June 29 seem to have vanished.
    Reinstated \refexam{exam:LyapExp}, make sure it is meant to be here?}

\example{Singular values and geometry of deformations:}
        { \label{exmp:ns-sing}
\index{singular values}
Suppose we are in three dimensions, and the \jacobianM\ $\jMps$ is not
singular, so that the diagonal elements of $D$ in
\refeq{SVD-j} satisfy $\sigma_1 \geq \sigma_2 \geq \sigma_3 >
0$. Consider how $\jMps$ maps the unit ball ${\cal S}=\{x
\in \mathrm{R}^3\, | \, x^2=1 \}$. $V$ is orthogonal
(rotation/reflection), so $\transp{V}{\cal S}$ is still the unit
sphere: then $D$ maps ${\cal S}$ onto ellipsoid
$\tilde{\cal S}=\{ y \in \mathrm{R}^3\, | \,
y_1^2/\sigma_1^2+y_2^2/\sigma_2^2+y_3^2/\sigma_3^2=1\}$
whose principal axes directions - $y$
coordinates - are determined by $V$. Finally the ellipsoid
is further rotated by the orthogonal matrix $U$. The local
directions of stretching and their images under $\jMps$ are
called the right-hand and left-hand singular vectors for
$\jMps$ and are given by the columns in $V$ and $U$
respectively: it is easy to check that
$\jMps v_k = \sigma_k u_k$, if $v_k,\, u_k$ are the k-th
columns of $V$ and $U$.                               \jumpBack{exmp:ns-sing}
} % end \example{{Singular values and geometry of deformations:}
    %
    \PC{give $2$\dmn\ example of ellipsoid explicitly}
\PublicPrivate{
        }{ % switch \PublicPrivate{
\toRem{r:SVD}
        }% end \PublicPrivate

         % end \section{Examples}

    \iftoCB
    \else
  \input{Problems/exerLyapunov}
  \input{chapter/refsLyapunov}
     \fi

\ChapterEnd
